import json
import chromadb
import ollama

# === 1. Carregar vocabul√°rio JSON ===
with open("data/vc.json", "r", encoding="utf-8") as f:
    vocabulario = json.load(f)

# === 2. Criar cliente Chroma (persistente) ===
client = chromadb.PersistentClient(path="db_vocabulario")
collection = client.get_or_create_collection("vocabulario")
# === 4. Fun√ß√£o para responder ===
def responder(pergunta, modelo="llama3.2"):
    resultados = collection.query(query_texts=[pergunta], n_results=3)
    contextos = "\n\n".join(resultados["documents"][0])

    prompt = f"""
Contexto:
Voc√™ √© um modelo de linguagem (LLaMA 3.2) integrado a um sistema de Retrieval-Augmented Generation (RAG), atuando como um m√≥dulo de pr√©-processamento para indexa√ß√£o cient√≠fica.

Objetivo:
Extrair termos para indexa√ß√£o a partir de uma pergunta em linguagem natural, garantindo precis√£o terminol√≥gica, rastreabilidade e reprodutibilidade, conforme exigido em ambientes cient√≠ficos.

Restri√ß√µes obrigat√≥rias:
Os termos devem constar no vocabul√°rio controlado fornecido (ex.: tesauro, ontologia, taxonomia cient√≠fica).
Os termos devem aparecer explicitamente na pergunta, com correspond√™ncia literal (string match).
N√£o utilizar sin√¥nimos, varia√ß√µes morfol√≥gicas, lematiza√ß√£o, tradu√ß√£o ou infer√™ncia sem√¢ntica.
Ignorar stopwords, conectivos e termos gen√©ricos n√£o cient√≠ficos.
Caso nenhum termo do vocabul√°rio controlado esteja presente na pergunta, retornar uma lista vazia.

Procedimento de pr√©-processamento:
Normalizar a pergunta (remo√ß√£o de pontua√ß√£o irrelevante).
Tokenizar a pergunta em n-grams compat√≠veis com os termos do vocabul√°rio.
Realizar correspond√™ncia exata entre os n-grams da pergunta e os termos do vocabul√°rio controlado.
Validar cada termo selecionado quanto √† presen√ßa literal na pergunta.

Formato da sa√≠da:
Retornar exclusivamente uma lista JSON.
Manter a grafia exata conforme definida no vocabul√°rio controlado.
N√£o incluir metadados, justificativas ou texto explicativo.
N√£o incluir explica√ß√µes de procedimento.
VOCABUL√ÅRIO:
{contextos}

PERGUNTA: {pergunta}

RESPOSTA:
"""
    

    resposta = ollama.chat(
        model=modelo,
        options={"temperature": 0.1},  # üëà Aqui est√° o ajuste
        messages=[{
            "role": "user",
            "content": prompt
        }])

    conteudo = resposta["message"]["content"]
    print("\nüß© Pergunta:", pergunta)
    print("üí¨ Resposta:", conteudo)
    return conteudo


# === 5. Teste ===
responder("Cite um chatbot semelhante ao BARD.")
responder("O que √© Big Data?")
responder("What is Big Data?")
responder("Cite um m√©todo avan√ßado de IA.")
responder("Defina Detec√ß√£o de Linguagem abusiva.")
responder("Quais s√£o os sistemas de IA?")

