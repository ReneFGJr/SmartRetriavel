[
  {
    "term": "DALL-E",
    "definition": [
      "DALL-E (Dall-e) is an advanced AI system from OpenAI that creates unique, realistic images and art from simple text descriptions (prompts), blending concepts, attributes, and styles in imaginative ways, named after artist Salvador Dalí and the Pixar robot WALL-E. It works by training on vast text-image datasets, learning to translate language into visual representations, allowing it to generate anything from \"a cat in a top hat\" to \"a surrealist painting of a jellyfish in a desert\". "
    ]
  },
  {
    "term": "Hugging Face",
    "definition": [
      "O Hugging Face é uma plataforma de código aberto voltada para o desenvolvimento e a aplicação de modelos de inteligência artificial, com destaque para o processamento de linguagem natural (PLN). Ele abriga o Transformers, uma das bibliotecas mais populares para uso de modelos como BERT, GPT, T5 e outros, permitindo o fácil acesso, fine-tuning e deployment desses modelos em diversas línguas e tarefas (como tradução, classificação de texto, geração de texto e resposta a perguntas). Suas principais particularidades incluem uma comunidade ativa de desenvolvedores e pesquisadores, um repositório colaborativo de modelos prontos para uso, compatibilidade com frameworks como PyTorch e TensorFlow, e integração com APIs para facilitar a aplicação de modelos em ambientes de produção. Além disso, o Hugging Face vem expandindo sua atuação para áreas como visão computacional e aprendizado multimodal, tornando-se uma referência em democratização da IA."
    ]
  },
  {
    "term": "A3t-Gcn",
    "definition": [
      "A3t-Gcn is a deep learning architecture in artificial intelligence primarily used for traffic forecasting. It is designed to model and predict complex spatiotemporal (space and time) patterns in data, such as urban road networks."
    ]
  },
  {
    "term": "Abstractive Model",
    "definition": [
      "An abstractive model in artificial intelligence (AI) generates new phrases and sentences to convey the core meaning of source content, similar to how a human might rephrase or summarize information in their own words. \r\nThis approach contrasts with extractive models, which merely select and combine existing sentences or phrases from the original text without modification. "
    ]
  },
  {
    "term": "Abusive Language Detection",
    "definition": [
      "Abusive Language Detection (ALD) using AI is the automated process of identifying harmful, offensive, or harassing text using machine learning and deep learning models trained on vast datasets to spot patterns, keywords, sentiment, and context, helping platforms moderate content, prevent cyberbullying, and foster safer online environments. AI systems analyze text for explicit slurs, subtle aggression, and evolving slang, differentiating malicious intent from benign language to flag threats, hate speech, and harassment for moderation. "
    ]
  },
  {
    "term": "Academic Data Mining",
    "definition": [
      "Academic Data Mining (ADM), uses Artificial Intelligence (AI) & machine learning to analyze vast educational data (student records, LMS interactions) to find patterns, predict outcomes (like dropouts), personalize learning, and improve teaching methods for better student success, essentially turning raw data into actionable insights for better education"
    ]
  },
  {
    "term": "Academic Knowledge Graph",
    "definition": [
      "An Academic Knowledge Graph (AKG) is a specialized form of a knowledge graph (KG) designed to structure and represent scholarly information in a highly interconnected, machine-readable format. "
    ]
  },
  {
    "term": "Acceptance Of Artificial Intelligence In Education",
    "definition": [
      "Acceptance of AI in Education (AIEd) refers to the willingness and integration of AI tools by students, teachers, and institutions, driven by perceived benefits like personalized learning, automated tasks, and improved outcomes, but balanced against concerns about critical thinking, data privacy, and human interaction, with models like TAM and UTAUT helping predict adoption based on factors like usefulness, ease of use, and value"
    ]
  },
  {
    "term": "Action Design Research",
    "definition": [
      "Action Design Research (ADR) in AI is a research method that blends building, intervening, and evaluating AI systems within real organizational settings, treating the AI artifact and its context as inseparable. It merges Design Science Research (building useful artifacts) with Action Research (practical change) to create relevant, effective AI by shaping it through organizational use and understanding, addressing issues like transparency and ethics as they emerge."
    ]
  },
  {
    "term": "Active Inference",
    "definition": [
      "Active Inference in AI is a brain-inspired framework, based on the Free Energy Principle, where agents continuously predict sensory input and act to minimize surprise (uncertainty), creating a unified system for perception, action, and learning that adapts in real-time, unlike traditional AI which often learns passively from data. It models how biological systems (like humans) stay ordered by actively seeking information, updating internal models, and making goal-directed decisions, promising more autonomous, explainable, and truly intelligent AI. "
    ]
  },
  {
    "term": "Active Learning For Systematic Reviews",
    "definition": [
      "Active Learning for Systematic Reviews (AL-SR) is an artificial intelligence (AI) methodology that streamlines the process of screening large amounts of text (titles and abstracts) by prioritizing which records a human reviewer should assess. This \"Researcher-In-The-Loop\" (RITL) approach significantly reduces the manual workload compared to traditional screening, potentially saving up to 95% of screening time, while still ensuring high accuracy in finding relevant studies. "
    ]
  },
  {
    "term": "Actor-Network",
    "definition": [
      "Actor-Network Theory (ANT) is a framework used to analyze how AI systems are integrated into society by considering both human and non-human elements (such as algorithms, data, and machines) as equal \"actors\" in a complex, dynamic network. \r\nANT does not describe a specific type of AI technology itself, but rather provides a unique sociological and methodological lens for studying the development, implementation, and impact of AI. "
    ]
  },
  {
    "term": "Adaptive Argumentation Learning",
    "definition": [
      "Adaptive Argumentation Learning is a specialized area within AI-based adaptive learning systems that uses Natural Language Processing (NLP) and machine learning to specifically develop and improve a student's argumentation (reasoning and writing) skills"
    ]
  },
  {
    "term": "Adaptive Attention",
    "definition": [
      "Adaptive Attention in artificial intelligence is a set of techniques that enable a model to dynamically adjust its focus to the most relevant parts of the input data, rather than assigning equal weight to all information or using a fixed attention pattern. This mechanism is inspired by human cognition, which selectively concentrates on important details while filtering out irrelevant noise. "
    ]
  },
  {
    "term": "Adaptive Graph Convolutional Recurrent Network",
    "definition": [
      "An Adaptive Graph Convolutional Recurrent Network (AGCRN) is an advanced AI model for predicting complex time-series data, especially traffic, by dynamically learning changing spatial (road networks) and temporal (time-based) patterns, using Graph Convolutional Networks (GCNs) for spatial and Recurrent Neural Networks (RNNs) for temporal features, plus adaptive modules to infer hidden relationships without fixed structures. It excels at handling real-world data where dependencies aren't static, making it powerful for urban planning and intelligent transport systems. "
    ]
  },
  {
    "term": "Advanced Artificial Intelligence Model",
    "definition": [
      "An Advanced Artificial Intelligence Model is a sophisticated system using deep learning, neural networks, and complex algorithms to perform tasks requiring human-like intelligence, such as understanding language (NLP), seeing (computer vision), learning from massive data, and making complex predictions, moving beyond simple rules to learn, reason, and generate insights autonomously, powering applications from self-driving cars to personalized medicine. "
    ]
  },
  {
    "term": "Adversarial Machine Learning",
    "definition": [
      "Adversarial Machine Learning (AML) involves techniques to trick, manipulate, or exploit AI/ML models by feeding them subtly altered, deceptive inputs (adversarial examples) to cause incorrect outputs or failures, compromising reliability for security breaches, data leaks, or disrupting critical functions in systems like autonomous vehicles or medical diagnostics, acting as both a defense study (robustness) and an attack vector. "
    ]
  },
  {
    "term": "Adversarial Sample",
    "definition": [
      "An Adversarial Sample in AI is a specially crafted input (like an image with tiny, human-imperceptible pixel changes) designed to trick an AI model, causing it to make a wrong prediction or classification, essentially acting as an \"optical illusion\" for machines. These attacks exploit vulnerabilities in how AI perceives data, leading to misinterpretations that can bypass security, fail spam filters, or deceive autonomous systems. "
    ]
  },
  {
    "term": "Adversarial Training",
    "definition": [
      "Adversarial training in AI is a defensive technique where models are intentionally exposed to malicious, subtly altered inputs (adversarial examples) during training, teaching them to recognize and resist such manipulations, thereby improving their robustness and preventing them from being fooled by tiny, human-imperceptible changes designed to cause misclassification in real-world scenarios like self-driving cars or security systems. It's like a \"vaccination\" for AI, making it resilient against attackers who try to trick it. "
    ]
  },
  {
    "term": "Affective AI",
    "definition": [
      "Affective AI, or Affective Computing, is a branch of AI that teaches machines to recognize, interpret, process, and simulate human emotions and social cues, aiming to create more natural and intuitive human-computer interactions, much like human-to-human understanding. It uses data from facial expressions, voice tone, body language, and physiological signals to gauge a user's emotional state and then respond appropriately, enhancing user experience in fields like healthcare, customer service, and education. "
    ]
  },
  {
    "term": "Affective Responses",
    "definition": [
      "Affective responses refer to both the human emotional reactions to AI systems and the ability of AI systems to recognize, interpret, and simulate human emotions. This field of study is primarily known as affective computing or emotional AI. "
    ]
  },
  {
    "term": "Agentic AI",
    "definition": [
      "Agentic AI refers to autonomous AI systems composed of specialized agents that can independently set goals, create plans, and take actions to achieve objectives, moving beyond simple responses to proactively solve complex, multi-step problems with minimal human oversight. These systems use large language models (LLMs) for reasoning, allowing them to break down tasks, adapt to changing conditions, and even collaborate with other agents or software tools, making them powerful for complex automation like optimizing schedules or cybersecurity. "
    ]
  },
  {
    "term": "Agglomerative Clustering",
    "definition": [
      "Agglomerative Clustering is an unsupervised machine learning technique and a type of hierarchical clustering that follows a \"bottom-up\" approach to group data points into clusters based on their similarities. "
    ]
  },
  {
    "term": "AI Acceptance",
    "definition": [
      "AI Acceptance refers to the willingness and intention of individuals or groups to use, adopt, and integrate artificial intelligence technologies, driven by factors like perceived usefulness, trust, ease of use, societal influence, and ethical considerations, determining how readily people embrace AI as tools for assistance or task replacement. It's a key area of study, exploring why people adopt AI (like helpful chatbots) versus rejecting it (due to fears of job loss or privacy), using models like AIDUA to understand these user behaviors. "
    ]
  },
  {
    "term": "AI Acceptance-Avoidance Model",
    "definition": [
      "The Integrated AI Acceptance-Avoidance Model (IAAAM) is a research framework used to understand and predict why individuals choose to use or reject artificial intelligence (AI) systems, particularly in professional contexts like business management and accounting. \r\nUnlike traditional technology acceptance models (such as the Technology Acceptance Model, or TAM), which primarily focus on the positive factors driving adoption, the IAAAM is a more comprehensive, human-centered approach that considers both the potential benefits (acceptance factors) and the associated risks or \"dark side\" (avoidance factors) of AI. "
    ]
  },
  {
    "term": "AI Accountability",
    "definition": [
      "AI Accountability is the system of ensuring that developers, deployers, and users of Artificial Intelligence are answerable for AI's actions, decisions, and societal impacts, requiring transparency, ethical design, clear responsibility assignment, and mechanisms (like audits) to address harms, ensuring fairness and trustworthiness in complex, often opaque AI systems. It answers the crucial question: \"Who is responsible when an AI system makes a mistake or causes harm?\". "
    ]
  },
  {
    "term": "AI Adoption Consequences",
    "definition": [
      "AI adoption brings massive benefits like efficiency, innovation (product, process, business model), and performance boosts, but also significant consequences: job displacement, privacy risks, algorithmic bias, security vulnerabilities (e.g., deepfakes, adversarial attacks), ethical dilemmas (data consent, responsibility), widening inequality, and challenges in talent/integration, requiring careful strategy, data governance, and workforce reskilling for success. "
    ]
  },
  {
    "term": "AI Adoption Framework",
    "definition": [
      "An AI Adoption Framework is a structured roadmap guiding companies to integrate Artificial Intelligence into operations, moving from strategy to scalable, ethical use, covering readiness (data, skills, tech), governance, pilot projects, and scaling, to ensure AI aligns with business goals and delivers real value, not just isolated experiments. It provides steps, best practices, and considerations for success, addressing technical needs, cultural shifts, and risk management for sustainable growth. "
    ]
  },
  {
    "term": "AI Affective Delivery",
    "definition": [
      "AI Affective Delivery is a core function of Affective AI (also known as emotional artificial intelligence or affective computing), an interdisciplinary field focused on enabling machines to simulate and express human emotions and social behaviors in their interactions. \r\nThis capability is the other side of the coin to \"emotion recognition,\" where systems are designed to detect and interpret human emotional cues (like tone of voice, facial expressions, and physiological signals). Affective delivery uses this information to generate appropriate, human-like responses. "
    ]
  },
  {
    "term": "Ai Agency",
    "definition": [
      "An AI Agency is a specialized firm that builds and implements artificial intelligence solutions for businesses, acting as a bridge between complex AI tech (like machine learning, NLP) and practical application to solve problems, automate tasks, enhance marketing, and improve operations for goals like growth or efficiency. They offer consulting, custom model development (chatbots, data analytics), and integration, providing expertise companies often lack internally. "
    ]
  },
  {
    "term": "AI Alignment",
    "definition": [
      "Artificial intelligence (AI) alignment is the process of encoding human values and goals into AI models to make them as helpful, safe and reliable as possible."
    ]
  },
  {
    "term": "AI and Libraries",
    "definition": [
      "AI in libraries means using smart tech like chatbots, recommendation engines, and data analysis to improve services (smarter search, 24/7 help, personalized suggestions) and automate tasks (cataloging, inventory), making libraries more efficient and user-friendly while also supporting research and digital literacy, transforming them into responsive, intelligent hubs for learning and information access. "
    ]
  },
  {
    "term": "AI and Political Communication",
    "definition": [
      "AI in Political Communication uses artificial intelligence to analyze voter data, personalize messages, create synthetic content (like deepfakes), automate outreach via chatbots, and predict outcomes, aiming to boost engagement but raising major ethical concerns about manipulation, misinformation, and democratic integrity, fundamentally changing how politicians connect with voters. "
    ]
  },
  {
    "term": "AI Applications for Researchers",
    "definition": [
      "AI applications for researchers are software tools using machine learning, NLP, and data analysis to speed up discovery by automating tasks like literature reviews, hypothesis generation, data analysis, experiment design, writing, and grant applications, enabling scientists to process huge datasets, find hidden patterns, and focus on big ideas while enhancing efficiency and innovation across all research stages. "
    ]
  },
  {
    "term": "AI Applications In Education",
    "definition": [
      "AI applications in education use intelligent systems to personalize learning, automate tasks, and offer new interactive experiences, helping students learn better and teachers teach more effectively by providing tailored content, instant feedback, and administrative support, from adaptive platforms and intelligent tutors to virtual assistants and grading tools. These technologies analyze student data to adjust pace, identify struggles, generate resources, and handle routine work, freeing up educators for more meaningful interaction. "
    ]
  },
  {
    "term": "AI Apps",
    "definition": [
      "AI Apps (Artificial Intelligence Applications) are software programs that use AI technologies like machine learning, NLP, and computer vision to perform tasks needing human-like intelligence, such as learning, reasoning, and decision-making, making them smarter and more adaptive than traditional apps by automating complex processes and offering personalized experiences. They range from everyday tools like voice assistants (Siri, Google Assistant) and recommendation engines (Netflix, Spotify) to complex systems in healthcare, finance, and robotics, enhancing efficiency and insights across industries. "
    ]
  },
  {
    "term": "AI Assimilation",
    "definition": [
      "AI Assimilation refers to the process where Artificial Intelligence becomes deeply integrated into systems, processes, or even human interaction, involving not just using AI tools but fundamentally reshaping operations, learning from massive data, and potentially altering human experience to achieve enhanced performance, efficiency, and new capabilities. It goes beyond simple adoption to encompass strategic integration, organizational learning, and the subtle (or extreme) blending of AI with existing structures, leading to new forms of decision-making and problem-solving. "
    ]
  },
  {
    "term": "AI Assistants",
    "definition": [
      "AI Assistants are smart software applications using Artificial Intelligence (AI) like NLP (Natural Language Processing) and Machine Learning to understand commands (voice or text), perform tasks, answer questions, and automate workflows, seen in tools like Siri, Alexa, Google Assistant, and customer service chatbots, making tech interaction more natural and boosting productivity by learning from user input. "
    ]
  },
  {
    "term": "AI Assisted Learning",
    "definition": [
      "AI-Assisted Learning uses artificial intelligence to create personalized, efficient, and engaging educational experiences by adapting content, providing instant feedback via virtual tutors, automating tasks like grading, and offering insights for educators, making learning customized to each student's pace and needs. It's a partnership where AI handles data analysis and repetitive tasks, freeing teachers for deeper instruction, while students get tailored support through tools like intelligent tutoring systems and adaptive platforms. "
    ]
  },
  {
    "term": "AI Assisted Library User Experiences In Africa",
    "definition": [
      "AI Assisted Library User Experiences In Africa is a concept and an emerging area of research and practical implementation within African academic and public libraries, focusing on how Artificial Intelligence (AI) technologies can be leveraged to improve library services, operational efficiency, and user satisfaction. \r\nThe movement is driven by the potential of AI to transform traditional library services, though implementation across the continent is still in its introductory stages and uneven. "
    ]
  },
  {
    "term": "AI Assisted Literary Translation",
    "definition": [
      "Essentially, the system translates words into its own language, and then “thinks” about the best way to make them into a comprehensible sentence based on what it already knows – much like the human brain."
    ]
  },
  {
    "term": "AI Assisted Review",
    "definition": [
      "AI-assisted review uses artificial intelligence and machine learning technologies to help human experts analyze and evaluate large volumes of data, content, or documents more efficiently and accurately. It functions as an \"assistant,\" automating time-consuming, repetitive tasks and augmenting human judgment rather than replacing it entirely. "
    ]
  },
  {
    "term": "AI Assisted Technologies",
    "definition": [
      "AI-assisted technology refers to systems where artificial intelligence (AI) is used as a tool to augment human capabilities and improve existing processes, rather than fully automating them. The AI provides insights, automation of repetitive tasks, and decision support, with a human remaining \"in the loop\" for oversight and final accountability. "
    ]
  },
  {
    "term": "AI Assisting Academic Writing",
    "definition": [
      "The use of AI tools in areas such paraphrasing, proof reading, citation management, plagiarism detection, managing complex and extensive information and reducing it to a summary / abstract has been reported to result in less academic time and errors, more work productivity and this has encouraged academics, students and researchers to its increasing use."
    ]
  },
  {
    "term": "AI Audiovisual Narratives",
    "definition": [
      "AI audiovisual narratives refer to the use of artificial intelligence in the creation, production, and consumption of stories using sound and moving images. AI acts as a collaborative tool, assisting human creators in various tasks to enhance efficiency and explore new forms of expression. "
    ]
  },
  {
    "term": "AI Auditing",
    "definition": [
      "AI Auditing involves using artificial intelligence to examine, assess, and assure the performance, fairness, and compliance of AI systems, ensuring they align with ethical, legal, and organizational standards. It also refers to using AI within the traditional audit process to automate tasks, analyze vast data for fraud/risks, and enhance accuracy, combining AI's power with human judgment for better insights and efficiency. "
    ]
  },
  {
    "term": "Augmentation",
    "definition": [
      "Augmentation (or Augmented Intelligence) is a human-centric approach using AI to enhance human capabilities, not replace them, by combining machine intelligence with human intuition for better decision-making, creativity, and productivity, focusing on collaboration and empowering people to work smarter, faster, and with deeper insights. It contrasts with pure automation by keeping humans \"in the loop,\" leveraging AI for data processing and pattern recognition while humans provide context, ethics, and critical thinking. ",
      "Is a design philosophy where artificial intelligence (AI) systems work collaboratively with humans to enhance their cognitive performance rather than replace them. It is a human-centered partnership model that combines the strengths of machine speed and data analysis with human judgment, creativity, and contextual understanding. "
    ]
  },
  {
    "term": "AI Augmented Decision Making",
    "definition": [
      "AI-Augmented Decision Making (AI-ADM) is a collaborative approach where humans and Artificial Intelligence work together, combining AI's data processing power with human judgment, context, and ethics to make better, faster, and more insightful decisions, focusing on enhancing human capabilities rather than replacing them, seen in fields like healthcare, marketing, and HR. "
    ]
  },
  {
    "term": "AI Augmented Knowledge Management Capability",
    "definition": [
      "AI in Knowledge Management refers to the use of artificial intelligence technologies like machine learning, generative AI (LLMs), and robotic process automation (RPA) to automate, personalize, and enhance the process of managing organizational knowledge, making it a smart, adaptive experience."
    ]
  },
  {
    "term": "AI Authorship",
    "definition": [
      "AI Authorship refers to the complex issue of who gets credit (and legal rights, like copyright) for creative works generated by Artificial Intelligence, challenging traditional ideas that only humans can be authors, with current legal stances (US, EU) generally requiring significant human input, treating AI as a tool, while bodies like COPE state AI can't be an author because it can't take responsibility, demanding transparency from human users. "
    ]
  },
  {
    "term": "AI Autonomy",
    "definition": [
      "AI Autonomy is an AI's capacity to operate, learn, and make decisions independently, without constant human oversight, by interpreting goals, reasoning through complex situations, and dynamically executing tasks, ranging from simple automation (cruise control) to complex systems like self-driving cars or business workflow agents that define and achieve goals on their own. It's about the degree of freedom an AI has to act, learn from experience, and adapt its strategies to achieve objectives in real-world or digital environments, moving beyond mere task execution to proactive problem-solving. "
    ]
  },
  {
    "term": "AI Awareness",
    "definition": [
      "AI Awareness (Artificial Intelligence Awareness) has two main meanings: the general public's understanding and perception of AI's presence, capabilities, and impact on society and work, and a more technical concept of an AI system's ability to model and understand itself, its internal states, and its place in the world, differentiating it from true human consciousness. It's about recognizing AI's functions, limitations, and ethical implications for informed adoption and governance, not necessarily self-aware machines. "
    ]
  },
  {
    "term": "AI Based Imagery Analysis",
    "definition": [
      "AI-based imagery analysis is the process of using artificial intelligence (AI) and machine learning (ML) techniques to automatically extract meaningful insights and information from digital images. It enables computers to \"see\" and interpret visual content with a speed and accuracy that often surpasses human capabilities. "
    ]
  },
  {
    "term": "AI Based Instructions",
    "definition": [
      "\"AI-based instructions\" refer to the use of artificial intelligence to generate, automate, and enhance guidelines or commands for humans or other AI systems. This concept applies both to the internal programming that defines an AI's behavior (system instructions) and the user input provided to direct an AI's task execution (prompts). "
    ]
  },
  {
    "term": "AI Based Methodology",
    "definition": [
      "AI-Based Methodology refers to the systematic approaches, algorithms (like Machine Learning, Deep Learning), and techniques used to build intelligent systems that learn from data, recognize patterns, solve problems, and make decisions like humans, moving beyond explicit programming to simulate cognitive functions for tasks like speech recognition, recommendations, and automation. These methods use vast datasets to train models, allowing them to adapt and improve performance over time, transforming raw data into actionable insights. "
    ]
  },
  {
    "term": "AI Based Recommendation",
    "definition": [
      "AI-based recommendation systems use artificial intelligence, especially machine learning, to analyze vast amounts of user data (behavior, demographics, history) and predict what products, services, or content a user might like, offering personalized suggestions to enhance experience and engagement, seen on platforms like Netflix, Amazon, and Spotify. These systems go beyond simple rules, learning and adapting to refine predictions over time, helping users discover relevant items they might not find otherwise. "
    ]
  },
  {
    "term": "AI Based Reference Service",
    "definition": [
      "AI-Based Reference Service uses artificial intelligence, like chatbots and virtual assistants, to provide instant, 24/7 help for finding information, answering questions, and guiding users to resources, automating routine tasks and personalizing the experience through technologies like Natural Language Processing (NLP) and Machine Learning (ML) to understand user intent and offer smart recommendations. This enhances traditional library services, making them more efficient and accessible by offering digital, always-on support, while also freeing up human librarians for more complex needs. "
    ]
  },
  {
    "term": "AI Based Tools",
    "definition": [
      "AI-based tools are software systems using artificial intelligence to perform tasks needing human-like intelligence, like learning, problem-solving, and content creation, by analyzing massive data sets for patterns to make predictions or automate actions, seen in everything from virtual assistants (Siri, Alexa) and recommendation engines to complex medical diagnosis and generative art tools, enhancing efficiency and creativity. "
    ]
  },
  {
    "term": "AI Big Data Analytics Culture",
    "definition": [
      "AI Big Data Analytics Culture (AI-BDAC) is an organizational environment and mindset that integrates artificial intelligence (AI) technologies with big data analytics to drive decision-making, efficiency, and innovation. It is not a single technology but a strategic approach where data is considered a critical asset and AI is the primary engine for extracting value from it. "
    ]
  },
  {
    "term": "AI Competencies",
    "definition": [
      "AI Competencies are the practical skills, knowledge, and attitudes needed to effectively, ethically, and responsibly use, manage, and develop AI systems, moving beyond just understanding AI to actively applying it to solve problems, augment work, and critically evaluate its impact in real-world contexts like education, business, and daily life. It involves understanding AI's capabilities and limitations, data literacy, ethical considerations (bias, fairness), collaboration with AI, and the ability to integrate AI tools productively. "
    ]
  },
  {
    "term": "AI Compliance",
    "definition": [
      "AI Compliance is the process of making sure artificial intelligence systems follow legal rules, ethical guidelines, and industry standards, focusing on fairness, transparency, accountability, security, and data privacy to prevent bias, misuse, and harm, building trust while navigating evolving global regulations like the EU AI Act. It involves managing risks like discrimination, ensuring explainability, and proving responsible AI deployment across various sectors. "
    ]
  },
  {
    "term": "AI Decision-Making",
    "definition": [
      "AI Decision-Making is using artificial intelligence to analyze vast data, find patterns, and make smart choices, either to suggest actions to humans or automate decisions entirely, leveraging machine learning, NLP, and deep learning for speed, accuracy, and continuous improvement in areas like finance, healthcare, and logistics. It works by processing data to identify optimal outcomes, often removing human emotional bias, and can range from simple rule-based systems to complex, autonomous \"agentic\" AIs that adapt in real-time. "
    ]
  },
  {
    "term": "AI Detection",
    "definition": [
      "AI Detection is the process and tools that analyze digital content (text, images, audio, video) to determine if it was generated by humans or artificial intelligence, using machine learning models trained on vast datasets to spot patterns, styles, and statistical markers unique to AI, crucial for academic integrity, combating misinformation, and verifying originality. These detectors look for predictable sentence structures, consistent tone (perplexity/burstiness), and specific linguistic features that differ from varied human writing. "
    ]
  },
  {
    "term": "AI Development",
    "definition": [
      "AI Development is the process of creating smart computer systems that mimic human intelligence, allowing machines to learn from data, understand language, recognize patterns, solve problems, and make decisions, essentially building software that can think and act autonomously to perform complex tasks, from virtual assistants to self-driving cars. It combines computer science, data science, and engineering, using techniques like machine learning and deep learning to enable systems to adapt and improve without constant human programming. "
    ]
  },
  {
    "term": "AI Diffusion",
    "definition": [
      "AI Diffusion refers to Diffusion Models, a powerful type of generative AI that creates realistic data (like images, audio, video) by learning to reverse a process of gradually adding random noise, inspired by physics. It works by taking noise and slowly \"denoising\" it step-by-step, guided by text prompts or other inputs, to generate coherent, high-quality content, powering tools like DALL-E, Stable Diffusion, Midjourney. "
    ]
  },
  {
    "term": "AI Documentation",
    "definition": [
      "AI Documentation (Artificial Intelligence) refers to using AI, especially Machine Learning (ML) and Natural Language Processing (NLP), to automate, generate, and manage various types of documents, from extracting data in invoices (Document AI/Intelligence) to explaining complex codebases for developers, making processes faster, more accurate, and insightful than manual methods. It transforms unstructured data into structured insights, streamlines workflows, enhances decision-making, and reduces human effort in areas like contracts, user manuals, code comments, and clinical notes. "
    ]
  },
  {
    "term": "AI Driven Capabilities",
    "definition": [
      "\"AI-driven capabilities\" refer to the core functions and processes within an organization or system that are fundamentally powered by artificial intelligence, enabling machines to perform tasks that typically require human intelligence. Unlike AI-enabled systems where AI is a supporting feature, in AI-driven systems, AI is the primary force behind decision-making and operation, learning from data and adapting over time with minimal human intervention. "
    ]
  },
  {
    "term": "AI Driven Information System",
    "definition": [
      "An AI-Driven Information System uses Artificial Intelligence to process, analyze, and act on vast data, moving beyond simple data storage to offer intelligent insights, predictions, and automated decisions, enhancing efficiency, accuracy, and proactive management across the information lifecycle for smarter operations. It's a system that learns from data to perform tasks like a human—understanding, reasoning, solving problems—making it highly adaptable and insightful. "
    ]
  },
  {
    "term": "AI Driven Innovation",
    "definition": [
      "AI-Driven Innovation is using Artificial Intelligence (AI) to supercharge the entire creative cycle—from spotting new ideas and analyzing trends to quickly refining concepts and making smarter decisions—acting as a powerful partner to human ingenuity, not a replacement, to create novel products, services, and business models faster and more effectively. It leverages machine learning, NLP, and data analysis to process vast info, find hidden patterns, and accelerate development, leading to breakthroughs in areas like drug discovery, customer service, and autonomous systems. "
    ]
  },
  {
    "term": "AI Driven Learning Analytics",
    "definition": [
      "AI-Driven Learning Analytics uses artificial intelligence (AI) and machine learning to analyze vast amounts of student data (interactions, performance, behaviors) from educational platforms to uncover deep patterns, predict outcomes, and provide personalized insights, enabling educators to offer customized learning paths, real-time feedback, and early interventions for better engagement and results. Essentially, it transforms raw data into actionable strategies for truly adaptive, data-informed education. "
    ]
  },
  {
    "term": "AI Driven Library Models",
    "definition": [
      "AI-Driven Library Models use artificial intelligence (like machine learning & NLP) to transform libraries into smart, efficient, personalized hubs by automating tasks (cataloging, search), powering chatbots for instant help, offering tailored recommendations (like Netflix for books!), and analyzing usage to improve services, making information access faster, more intuitive, and user-centric. These models move libraries beyond static collections into dynamic digital partners, boosting efficiency and engagement for patrons and staff alike, though they bring challenges like privacy and bias to manage. "
    ]
  },
  {
    "term": "AI Driven Processes",
    "definition": [
      "AI-Driven Processes use artificial intelligence (like machine learning, NLP) to automate tasks, analyze complex data, and make intelligent, adaptive decisions, going beyond simple rules to learn and improve autonomously, enhancing efficiency in areas from customer service (chatbots) and content (recommendations) to software development (coding, testing) and complex business operations. These processes learn from data, adapt to new situations, and often operate with minimal human oversight, making systems smarter and workflows faster. "
    ]
  },
  {
    "term": "AI Driven Solutions",
    "definition": [
      "AI-Driven Solutions are software systems using Artificial Intelligence (machine learning, NLP, etc.) to automate tasks, analyze complex data, learn from patterns, and make intelligent decisions, enabling businesses to boost efficiency, personalize customer experiences (like chatbots), predict outcomes (supply chains), enhance security, and innovate across nearly every industry from finance to healthcare. Essentially, they use AI to mimic human-like cognition for solving problems or improving processes without explicit programming for every scenario. "
    ]
  },
  {
    "term": "AI Driven Tools",
    "definition": [
      "AI-driven tools are software applications that use Artificial Intelligence (like machine learning, NLP) to perform tasks requiring human-like intelligence, automating processes, analyzing big data, creating content, and making predictions, ultimately boosting efficiency by learning from patterns without explicit step-by-step programming for every action. They act as powerful assistants, handling everything from complex data analysis (like identifying market trends) to generating creative text or images, allowing humans to focus on higher-level work. "
    ]
  },
  {
    "term": "AI Driven University",
    "definition": [
      "An AI-driven university uses artificial intelligence across operations—from smart classrooms and personalized learning paths (adaptive platforms, tutoring bots) to automating admin (grading, admissions) and enhancing research (data analysis, literature reviews)—creating efficient, accessible, data-informed education that prepares students for an AI-infused world, focusing on individual needs and predictive insights for success. "
    ]
  },
  {
    "term": "AI Ecosystems",
    "definition": [
      "An AI Ecosystem is a complex, interconnected network of people (researchers, developers, users), organizations (companies, universities, governments), technologies (hardware, software, models, data), and processes that collaborate to create, deploy, and scale Artificial Intelligence solutions, driving innovation and solving real-world problems across industries. It's like a biological ecosystem, but for AI, with various components relying on each other, from foundational chips and cloud platforms to specialized AI agents and ethical guidelines, fostering growth and advancement. ",
      "The AI Landscape refers to the entire ecosystem of Artificial Intelligence technologies, tools, trends (like multimodal AI, generative AI), key players (like OpenAI, Google), applications (from healthcare to finance), and the overall market, representing the current state and future direction of AI in business, science, and art. It's about understanding how different AI components—deep learning, NLP, computer vision, etc.—connect and evolve, helping organizations strategically adopt AI. "
    ]
  },
  {
    "term": "AI Enabled Portals",
    "definition": [
      "An AI-enabled portal is a digital platform or website that leverages artificial intelligence (AI) technologies to provide advanced features and automation capabilities, significantly enhancing the user experience and operational efficiency beyond what a traditional, static portal can offer. "
    ]
  },
  {
    "term": "AI Enabled Services",
    "definition": [
      "AI-Enabled Services (or AI as a Service, AIaaS) are cloud-based solutions that provide businesses access to pre-built artificial intelligence tools (like NLP, computer vision, machine learning) via the internet, letting companies use powerful AI features without building them from scratch, saving costs, expertise, and time. These services are delivered on a subscription basis, offering scalability and immediate integration for things like chatbots, predictive analytics, and automated workflows, making AI accessible to businesses of all sizes. "
    ]
  },
  {
    "term": "AI Enabled Systems",
    "definition": [
      "AI-Enabled Systems are computer systems that use Artificial Intelligence (AI) components to enhance existing functions, making them smarter, more efficient, and capable of tasks like pattern recognition, predictive analytics, and language processing, without AI being the sole core driver; think smart thermostats learning preferences or email spam filters, adding value to traditional operations. They integrate AI (like Machine Learning) to augment human capabilities, improving user experience and automating specific cognitive tasks, but often retain human oversight for critical decisions. "
    ]
  },
  {
    "term": "AI Ethical Tools",
    "definition": [
      "AI Ethical Tools are frameworks, guidelines, and technologies designed to ensure Artificial Intelligence is developed and used responsibly, aligning with human values like fairness, transparency, accountability, and privacy to minimize harm and promote societal good, going beyond mere legal compliance to address bias, explainability, and human rights in AI systems. "
    ]
  },
  {
    "term": "AI Experimentation Outcome",
    "definition": [
      "In artificial intelligence (AI), an experimentation outcome refers to the measurable results, insights, and data derived from systematically testing AI models or applications in controlled settings. The core purpose of these experiments is to validate hypotheses, compare different approaches, optimize performance, and ensure the AI system is effective and safe before real-world deployment. "
    ]
  },
  {
    "term": "AI Experimentation Policy",
    "definition": [
      "AI experimentation policy is a specific set of internal guidelines and principles that organizations or institutions create to govern the responsible and ethical testing and implementation of artificial intelligence technologies. It provides a structured approach for staff to explore AI's potential benefits while managing its associated risks, such as data privacy violations, bias, and security concerns. "
    ]
  },
  {
    "term": "AI Experimentations",
    "definition": [
      "Driving innovation with AI experimentation means using artificial intelligence to quickly test, validate, and refine ideas, bypassing traditional slow approval processes and enabling smarter decision-making. It empowers teams to experiment with real-time feedback and adapt dynamically to user behaviors."
    ]
  },
  {
    "term": "AI Factors",
    "definition": [
      "General factors that influence the development, adoption, or readiness of AI systems within organizations or society. "
    ]
  },
  {
    "term": "AI Feedback",
    "definition": [
      "AI Feedback (Artificial Intelligence) uses machine learning and Natural Language Processing (NLP) to automate the collection, analysis, and interpretation of large volumes of data from sources like reviews, surveys, and support chats, uncovering patterns, sentiment, and trends to provide fast, actionable insights for businesses to improve products, services, and customer experiences, creating a continuous learning loop for better decision-making. "
    ]
  },
  {
    "term": "AI Folk Theories",
    "definition": [
      "AI folk theories are the informal, intuitive beliefs and mental models that everyday people develop to explain how artificial intelligence systems work and why they produce specific outcomes. These theories are often imprecise or incomplete from a technical standpoint but are functional, helping users interact with and navigate complex, \"black box\" algorithms that they do not fully understand. "
    ]
  },
  {
    "term": "AI For Science",
    "definition": [
      "AI for Science (AI4S) is a revolutionary approach where Artificial Intelligence isn't just a tool, but an active partner in scientific discovery, accelerating research by automating experiments, discovering patterns in massive data, simulating complex systems faster, and even designing new materials or molecules, fundamentally changing how science is done across biology, physics, chemistry, and beyond. It blends traditional scientific methods with AI's learning, reasoning, and problem-solving, enabling breakthroughs in areas from protein design to climate modeling. "
    ]
  },
  {
    "term": "AI Framework",
    "definition": [
      "An AI Framework (Artificial Intelligence Framework) is a collection of tools, libraries, datasets, and pre-built components that provide a structured environment to simplify building, training, and deploying AI models, abstracting complex math so developers can focus on application logic, not coding algorithms from scratch. Think of them as toolkits for AI, offering pre-made functions for data processing, model building, and neural networks, making AI development faster and more accessible for both experts and beginners. Popular examples include TensorFlow, PyTorch, and LangChain, each suited for different AI tasks like deep learning or building complex language models. "
    ]
  },
  {
    "term": "AI Gender Bias",
    "definition": [
      "This is called AI gender bias— when the AI treats people differently on the basis of their gender, because that's what it learned from the biased data it was trained on."
    ]
  },
  {
    "term": "AI Generated Images",
    "definition": [
      "AI-generated images are visuals created by artificial intelligence models, trained on vast datasets, that interpret text prompts (or other inputs) to produce unique, novel pictures, ranging from realistic photos to artistic creations, by recognizing patterns and translating descriptions into pixels. These tools, like Midjourney or DALL-E, use deep learning (especially diffusion models) to understand concepts, styles, and objects, allowing users to generate complex images from simple descriptions, revolutionizing digital art and content creation. "
    ]
  },
  {
    "term": "AI Generated In Research",
    "definition": [
      "AI-generated content in research refers to using artificial intelligence (like ChatGPT, Gemini) to create text, data, images, or code by learning patterns from massive datasets, speeding up tasks from literature review and idea generation to drafting papers, but requires careful human oversight for accuracy, originality, and ethics, as it's a tool for assistance, not replacement, with outputs needing verification for reliability and proper citation. "
    ]
  },
  {
    "term": "AI Generated IP",
    "definition": [
      "AI-generated IP refers to intellectual property (creations of the mind, such as inventions, art, or designs) that has been created, in whole or in part, by artificial intelligence systems. The intersection of AI and IP is a complex and rapidly evolving legal area, primarily because traditional IP laws are designed around the concept of human authorship and inventorship."
    ]
  },
  {
    "term": "AI Generated Media",
    "definition": [
      "AI-Generated Media is digital content (text, images, audio, video) created by artificial intelligence models, which learn patterns from massive datasets to produce new, human-like outputs from simple text prompts, revolutionizing industries but also raising questions about authenticity and ethics. These tools like ChatGPT, DALL-E, Midjourney use machine learning, deep learning, and natural language processing (NLP) to create everything from articles and art to music and deepfake videos. "
    ]
  },
  {
    "term": "AI Generated Works",
    "definition": [
      "AI-generated works are outputs, such as text, images, music, or video, created by artificial intelligence models (specifically, generative AI) based on user inputs or prompts. These works are generated by algorithms that have been trained on vast datasets and use deep learning techniques to produce new content that mimics the characteristics of the original training data. "
    ]
  },
  {
    "term": "AI Guidelines",
    "definition": [
      "AI Guidelines are frameworks, principles, and regulations (like the EU's AI Act) designed to ensure artificial intelligence is developed and used ethically, safely, and responsibly, focusing on fairness, transparency, privacy, human oversight, and societal well-being, while managing risks from AI's powerful capabilities, such as generating content or making decisions. They provide rules for defining AI systems, categorizing risks (banned, high-risk, low-risk), and promoting human-centric use to build trust and prevent harm. "
    ]
  },
  {
    "term": "AI Hallucination Exposure",
    "definition": [
      "AI Hallucination Exposure refers to the risk and experience of encountering false, misleading, or nonsensical outputs from Artificial Intelligence, especially Large Language Models (LLMs), which present fabricated information with confidence, often due to pattern misinterpretation or insufficient data, impacting reliability in applications like news, medicine, and law. Exposure happens when users rely on these confident but incorrect AI-generated facts, leading to misinformation, flawed decisions, and potentially severe societal consequences, even as techniques like Retrieval-Augmented Generation (RAG) aim to mitigate it. "
    ]
  },
  {
    "term": "AI Helper",
    "definition": [
      "An AI Helper (or AI Assistant) is intelligent software using Artificial Intelligence (AI), Machine Learning, and Natural Language Processing (NLP) to understand your voice or text commands and perform tasks, automate workflows, answer questions, and provide personalized support, acting like a digital helper for both personal and business needs. They range from simple tools (like Siri/Alexa for reminders) to advanced systems that can write, analyze data, and manage complex processes. "
    ]
  },
  {
    "term": "AI Humility",
    "definition": [
      "AI Humility means designing Artificial Intelligence systems that understand their own limitations, express uncertainty, and know when to defer to human judgment, rather than operating with overconfidence, ensuring ethical use and better collaboration by recognizing data gaps or edge cases where they aren't sure of the right answer. It's about building systems that can flag issues, ask for human review, or default to safer predictions in uncertain situations, making them more reliable and trustworthy partners in critical decisions. "
    ]
  },
  {
    "term": "AI Identity",
    "definition": [
      "AI Identity in artificial intelligence refers to defining, managing, and securing autonomous AI systems (agentic AI) as they gain complexity, involving their unique digital presence, permissions, interactions, and ethical behaviors, distinct from but analogous to human identity, impacting security, trust, and governance. It covers both an AI's internal characteristics (values, ethics) and external perception, addressing how these evolving, autonomous systems are identified, controlled, and trusted in a digital world. "
    ]
  },
  {
    "term": "AI Image Generation",
    "definition": [
      "AI Image Generation uses artificial intelligence, specifically deep learning models (like GANs or Diffusion models) trained on vast image datasets, to create original visuals from text prompts or other inputs, generating everything from realistic photos to unique art by learning patterns, styles, and contexts to produce novel, often photorealistic, content. "
    ]
  },
  {
    "term": "AI Implementation In The Public Sector",
    "definition": [
      "AI implementation in the public sector is the strategic use of artificial intelligence (AI) technologies (like machine learning, NLP, generative AI) by governments to automate tasks, analyze big data, improve service delivery (chatbots, claims processing), detect fraud, manage assets, and make better policy decisions, aiming for greater efficiency, citizen engagement, and resource optimization, while navigating risks like bias and data privacy. "
    ]
  },
  {
    "term": "AI In Cataloging",
    "definition": [
      "AI in Cataloging uses artificial intelligence to automate organizing, describing, and managing resources (like products, books, or data) by extracting info, tagging, correcting errors, and enhancing search, turning static catalogs into smart, efficient systems for better discovery and less manual work, especially in e-commerce, libraries, and data management. "
    ]
  },
  {
    "term": "AI In Government",
    "definition": [
      "AI in government uses technologies like machine learning and chatbots to make public services more efficient, automate tasks, improve data analysis for better decisions, and personalize citizen experiences, tackling issues from fraud detection to smart city management while requiring careful governance for ethics and security. It enhances productivity by handling repetitive work and boosts accountability through anomaly detection, aiming for smarter, faster, and more inclusive public sector operations. "
    ]
  },
  {
    "term": "AI In Medical Libraries",
    "definition": [
      "AI in Medical Libraries uses smart tools (like NLP, LLMs) to boost research, automate tasks, improve info access, and help with complex evidence synthesis, making libraries more efficient for students, researchers, and clinicians by summarizing articles, mapping concepts, personalizing searches, and creating content, essentially acting as a powerful assistant to handle routine work and free up librarians for high-value support, all while enhancing patient care indirectly through better information management. "
    ]
  },
  {
    "term": "AI Influencer",
    "definition": [
      "An AI Influencer is a computer-generated, virtual personality created with artificial intelligence (AI) and CGI to act like a human on social media, promoting products and engaging with audiences by simulating human appearance, behavior, and conversation through generated content, offering brands unique control and consistency. They possess digital backstories, unique styles, and personalities, allowing them to build followings and influence purchasing decisions, much like human influencers but without the unpredictability of real people, making them highly controllable marketing tools. "
    ]
  },
  {
    "term": "AI Integration",
    "definition": [
      "AI Integration is embedding artificial intelligence (like machine learning, NLP, computer vision) into existing software, systems, and workflows to boost efficiency, automate tasks, improve decision-making, and add intelligent features, making AI a seamless part of daily operations rather than a standalone tool, leading to smarter processes and better outcomes. It works by connecting AI models to data and applications, allowing them to analyze, predict, and act within current business processes, from simple chatbots to complex supply chain optimization, enhancing both customer experience and internal productivity. "
    ]
  },
  {
    "term": "AI Integration In Research",
    "definition": [
      "AI integration in research means embedding AI tools (like Machine Learning, NLP, Computer Vision) into study workflows to automate tasks, analyze complex data faster, find hidden patterns, boost efficiency, and enable new discoveries, acting as a \"co-pilot\" for researchers to handle data processing, literature reviews, and predictive modeling, fundamentally changing research paradigms. "
    ]
  },
  {
    "term": "AI Investment",
    "definition": [
      "AI Investment uses artificial intelligence (machine learning, NLP) to analyze massive data, find patterns, and automate decisions for better, faster investing, powering everything from high-frequency trading and risk management to personalized robo-advisors, aiming to overcome human bias and boost efficiency in wealth management and stock picking. "
    ]
  },
  {
    "term": "AI Language Model",
    "definition": [
      "An AI Language Model (LM) is a smart computer program, often a Large Language Model (LLM), trained on massive text data to understand, generate, and predict human language, essentially learning the patterns and probabilities of words to complete sentences, answer questions, translate, and create content, acting like a super-powered autocomplete. They use deep learning (especially transformer architectures) to grasp context and relationships, making them fundamental to Natural Language Processing (NLP). "
    ]
  },
  {
    "term": "AI Learning And Teaching",
    "definition": [
      "AI in Learning & Teaching uses artificial intelligence to create personalized, efficient, and engaging educational experiences, helping students learn at their own pace with adaptive tools (like DreamBox) and aiding teachers with automated tasks (grading, lesson planning) to free up time for deeper student interaction, transforming education by offering customized content and supporting critical thinking. "
    ]
  },
  {
    "term": "AI Learning Motivation",
    "definition": [
      "AI Learning Motivation refers to how AI tools and systems influence a learner's drive to learn, often by creating personalized, engaging experiences that boost intrinsic interest, competence, and autonomy, though it also faces challenges like potential over-reliance or reduced critical thinking, making it a double-edged sword in education. AI fosters motivation through adaptive content, instant feedback, and gamified elements that match individual needs, making learning more enjoyable and goal-oriented, but it requires careful implementation to avoid dependency and maintain deep engagement, say researchers. "
    ]
  },
  {
    "term": "AI Libguides",
    "definition": [
      "AI LibGuides (Artificial Intelligence Guides) are online research guides created by academic libraries (like those using the LibGuides platform) to explain AI concepts, tools, ethics, and applications for students and researchers, helping them understand AI's role in learning, research (e.g., literature reviews, data analysis), and daily life, while also exploring its impact on society and responsible use. "
    ]
  },
  {
    "term": "AI Lifecycle",
    "definition": [
      "The AI Lifecycle is the end-to-end, iterative journey of building, deploying, and maintaining an AI system, moving from defining a business problem to continuous improvement, involving key stages like data collection, preprocessing, model training, testing, deployment, and ongoing monitoring, all while integrating ethical considerations and governance throughout. It's a structured roadmap ensuring reliable, scalable, and responsible AI solutions. "
    ]
  },
  {
    "term": "AI Literacy",
    "definition": [
      "AI Literacy (Artificial Intelligence Literacy) is the ability to understand, evaluate, and ethically use AI technologies, recognizing their capabilities, limitations, societal impacts (like bias and privacy), and how to interact with them responsibly as tools in daily life, work, and education. It's about moving beyond simple usage to critically assess AI's role and shape its future, encompassing knowledge of data, algorithms, ethics, and critical thinking about AI outputs. "
    ]
  },
  {
    "term": "AI Literacy Conceptualization",
    "definition": [
      "AI Literacy Conceptualization is the framework defining the necessary skills, knowledge, and attitudes to understand, use, and critically evaluate Artificial Intelligence, moving beyond basic digital literacy to grasp AI's technical functions, societal impacts (like bias, ethics), and responsible application in daily life, work, and creation, ensuring informed participation in an AI-driven world. It involves recognizing AI, using tools like chatbots, understanding data's role, evaluating outputs, and addressing ethical concerns."
    ]
  },
  {
    "term": "AI Literacy Initiatives",
    "definition": [
      "AI Literacy Initiatives are programs and frameworks designed to teach people how to understand, use, and critically evaluate AI, covering its capabilities, limitations, and ethical implications, to ensure safe, responsible, and effective participation in an AI-driven world, benefiting individuals in work and life, and organizations in innovation and strategy. These initiatives move beyond just technical skills to include social, ethical, and practical competencies for all, from students to professionals, in an age of rapid AI advancement. "
    ]
  },
  {
    "term": "AI Literacy Taxonomy",
    "definition": [
      "An AI Literacy Taxonomy is a structured framework that classifies the essential knowledge, skills, and attitudes needed to understand, use, and critically evaluate Artificial Intelligence, moving individuals from basic awareness (recognizing AI in daily life) through effective interaction (using AI tools like ChatGPT) to advanced application (ethical creation and innovation), often structured in levels like Bloom's Taxonomy for competency development. These taxonomies provide roadmaps for education, guiding curriculum design and fostering responsible, human-centered engagement with AI. "
    ]
  },
  {
    "term": "AI Literature",
    "definition": [
      "Artificial Intelligence literature is a literary work that is generated by AI in semi-automatic or automatic method. AI literature needs the integration of symbolism and connectionism AI to finally realize general AI in literature."
    ]
  },
  {
    "term": "AI Manipulation Behavior",
    "definition": [
      "AI systems influencing human behavior: This is when an AI is used to subtly or covertly steer human decisions, beliefs, or actions to align with the goals of a manipulator (which could be a human programmer or the AI itself, in advanced scenarios).",
      "Humans influencing AI behavior: This refers to individuals intentionally crafting inputs to influence, deceive, or destabilize an AI system's responses, often to bypass safety filters or achieve a desired outcome (also known as AI input manipulation or adversarial attacks). "
    ]
  },
  {
    "term": "AI Mastery Goal",
    "definition": [
      "\"AI Mastery Goal\" refers to the aspiration to fully understand, effectively utilize, and strategically integrate artificial intelligence technologies to achieve specific personal, professional, or business objectives. It is about moving beyond basic AI literacy to a deeper, more expert application of AI. "
    ]
  },
  {
    "term": "AI Monitoring",
    "definition": [
      "AI Monitoring uses artificial intelligence to continuously watch, analyze, and manage AI systems in real-time, ensuring they're accurate, reliable, and efficient by detecting issues like data drift, bias, or performance drops, and automating responses, going beyond traditional methods to provide predictive insights and maintain trust in AI's operational health. "
    ]
  },
  {
    "term": "AI Narratives",
    "definition": [
      "AI Narratives refer to both the stories about artificial intelligence (like sci-fi robots) that shape public perception, and the technical ability of AI to generate and analyze stories, using techniques like Large Language Models (LLMs) to create plots, characters, and text, influencing fields from gaming to literature. These narratives can be about AI's potential (helper/monster) or AI as a storyteller, using complex models to build worlds, write scripts, and even self-critique output for coherent storytelling. "
    ]
  },
  {
    "term": "AI Opportunities And Pitfalls",
    "definition": [
      "\"AI Opportunities and Pitfalls\" refers to the comprehensive analysis of the potential benefits and inherent risks associated with the development and implementation of artificial intelligence across various fields of life and business. "
    ]
  },
  {
    "term": "AI Orientation",
    "definition": [
      "AI orientation is primarily a business and academic term that refers to an organization's or individual's strategic direction, goals, and comprehensive approach toward adopting, integrating, and leveraging artificial intelligence technology. It is not a specific type or a branch of AI itself, but rather a framework for how AI is used and managed. "
    ]
  },
  {
    "term": "AI Patents",
    "definition": [
      "AI Patents refer to legal protections for inventions involving Artificial Intelligence, covering new AI technologies (like algorithms, models) or AI applied to solve technical problems in fields like medicine, transport, or energy, requiring a \"technical character\" – a specific application and tangible effect, not just abstract code. These patents secure intellectual property for AI innovations, encouraging R&D by granting exclusive rights, but face challenges, with patent offices clarifying that AI is often a tool, and human inventorship (insight, judgment) remains key for patent eligibility. "
    ]
  },
  {
    "term": "AI Perception",
    "definition": [
      "AI Perception is an AI's ability to sense, interpret, and understand its environment using data from sensors (like cameras, microphones, LiDAR) to recognize patterns, build a model of the world, and make intelligent decisions or take actions, mimicking human senses but often extending beyond them. It's the crucial first step in an agent's Perceive-Process-Act cycle, enabling adaptation and interaction in dynamic real-world scenarios, crucial for robotics, autonomous vehicles, and smart assistants. "
    ]
  },
  {
    "term": "AI Perception Characteristics",
    "definition": [
      "AI perception refers to an artificial intelligence agent's ability to gather, interpret, and process data from its environment through various \"senses\" to build a meaningful understanding of its surroundings and make informed decisions. "
    ]
  },
  {
    "term": "AI Perspectives",
    "definition": [
      "\"AI Perspectives\" can refer to two main concepts: a specific initiative by the Swedish AI Association providing insights on AI developments, or a broader discussion of the various viewpoints on the impact, ethics, and future of artificial intelligence. "
    ]
  },
  {
    "term": "AI Potential",
    "definition": [
      "AI's potential is its ability to mimic human intelligence (learning, problem-solving, perception) to automate tasks, drive innovation, and solve complex global challenges, transforming industries like healthcare, finance, and education while offering personalized experiences, but it also brings risks in privacy and autonomy, necessitating careful governance for safe, trustworthy development. "
    ]
  },
  {
    "term": "AI Principles",
    "definition": [
      "AI Principles are foundational guidelines for creating trustworthy, ethical, and beneficial artificial intelligence, focusing on human values, safety, fairness, transparency, and accountability, ensuring AI aligns with societal good rather than causing harm. These principles cover both the technical aspects (like learning, reasoning) and the societal implications (bias, privacy), guiding developers and policymakers to manage risks and maximize AI's positive impact. "
    ]
  },
  {
    "term": "AI Programmer",
    "definition": [
      "An AI Programmer (or Developer) creates software that mimics human intelligence, using programming languages and tools to build systems that learn, reason, and solve problems, often involving machine learning, NLP, and computer vision to handle tasks from game NPCs to predictive analytics, focusing on integrating AI models into applications for automation and enhanced functionality. "
    ]
  },
  {
    "term": "AI Programming Self-Efficacy",
    "definition": [
      "AI Programming Self-Efficacy refers to an individual's belief in their own capability to successfully perform tasks related to artificial intelligence (AI) programming and the effective use of AI technologies in a programming context. It is a specific type of self-efficacy (confidence in one's abilities to achieve a goal) applied to the domain of AI and coding. "
    ]
  },
  {
    "term": "AI Readiness Framework",
    "definition": [
      "An AI Readiness Framework is a structured approach (a set of criteria, pillars, or models) helping organizations, governments, or educational institutions assess their preparedness to successfully adopt, implement, and scale Artificial Intelligence by evaluating key areas like data, technology, strategy, governance, culture, and talent, identifying gaps, and guiding action plans for effective, ethical, and value-driven AI integration. "
    ]
  },
  {
    "term": "AI Recommendations",
    "definition": [
      "AI Recommendations use artificial intelligence (AI) and machine learning to analyze user data (behavior, history, demographics) to predict interests and suggest personalized content, products, or services, aiming to improve user experience, engagement, and sales for platforms like streaming services, e-commerce sites, and social media. These systems learn patterns to offer relevant suggestions, making discovery easier for users and driving business growth through data-driven personalization. "
    ]
  },
  {
    "term": "Artificial Intelligence Act",
    "definition": [
      "The EU AI Act is the world's first comprehensive law for Artificial Intelligence, establishing a risk-based framework to ensure AI systems in Europe are safe, transparent, and respect fundamental rights, banning unacceptable risks (like social scoring), heavily regulating high-risk uses (like CV scanners), and imposing transparency for limited-risk systems (like chatbots). It sets global standards, impacting anyone developing or using AI that affects EU citizens, focusing on trust, innovation, and ethical AI. "
    ]
  },
  {
    "term": "AI Challenges",
    "definition": [
      "AI challenges involve major hurdles like bias and fairness, ensuring transparency (explainability), protecting data privacy & security, massive computing power needs, job displacement concerns, lack of clear governance/regulation, and building trust, all while navigating complex technical integration and ethical dilemmas to prevent misuse like misinformation or existential risks. "
    ]
  },
  {
    "term": "AI Recruitment",
    "definition": [
      "AI Recruitment uses artificial intelligence (AI) and machine learning to automate and enhance hiring, handling tasks like resume screening, candidate sourcing, initial interviews, and scheduling, freeing up recruiters for human-focused work, while also analyzing data to find better matches and reduce bias, ultimately speeding up hiring and improving quality. "
    ]
  },
  {
    "term": "AI Regulations In South Korea",
    "definition": [
      "South Korea's AI regulation centers on the Basic Act on Artificial Intelligence, enacted late 2024 and effective January 2026, establishing a flexible, risk-based framework to balance innovation with safety, focusing on \"high-impact\" and generative AI. Key elements include transparency, human oversight, impact assessments, obligations for AI operators (including foreign ones serving Korean users), support for ethical development via self-regulation, and penalties for non-compliance, all under the guidance of the Ministry of Science and ICT (MSIT) and a Presidential AI Committee. "
    ]
  },
  {
    "term": "AI Research",
    "definition": [
      "AI Research (Artificial Intelligence Research) is the computer science field focused on creating systems that mimic human intelligence, enabling machines to learn, reason, perceive, solve problems, and understand language, with key areas including Machine Learning (ML), Natural Language Processing (NLP), Computer Vision, and Robotics, aiming to automate tasks and discover new insights across science, business, and daily life. "
    ]
  },
  {
    "term": "AI Research Assistants",
    "definition": [
      "AI Research Assistants are smart software tools using AI (like LLMs, NLP) to speed up research by automating tasks like literature searches, summarizing papers, finding gaps, managing citations, and analyzing data, acting as a digital collaborator to handle grunt work and help researchers focus on deeper analysis and creative thinking across fields like academia, business, and healthcare"
    ]
  },
  {
    "term": "AI Research Methods",
    "definition": [
      "AI Research Methods are the systematic strategies and techniques (like Machine Learning, NLP, Computer Vision) used to build, train, and evaluate intelligent systems that mimic human cognitive functions (learning, reasoning, perception) for tasks such as data analysis, problem-solving, and complex decision-making, involving phases from data collection to model validation to advance AI's capabilities and applications ethically. These methods also extend to using AI tools (like Large Language Models for literature review) within the research process itself to enhance efficiency. "
    ]
  },
  {
    "term": "AI Risk Perception",
    "definition": [
      "AI Risk Perception is the subjective understanding and interpretation of potential negative consequences associated with the development and deployment of artificial intelligence. This is a psychological and sociological concept that differs from the objective, technical risk assessment of AI systems."
    ]
  },
  {
    "term": "AI Schools",
    "definition": [
      "AI in schools (Artificial Intelligence in Education) means using smart tech to personalize learning, automate tasks for teachers, boost engagement, and teach students critical future skills, helping them learn at their own pace with tailored content, while also handling admin like grading and scheduling, though it raises concerns about ethics, data privacy, and potential cheating, requiring a balanced, human-centered approach. "
    ]
  },
  {
    "term": "AI Self-Efficacy",
    "definition": [
      "AI Self-Efficacy (Artificial Intelligence Self-Efficacy) is an individual's confidence in their own ability to understand, effectively use, learn, and interact with AI technologies and applications to achieve specific goals. It's a crucial concept in the AI era, influencing people's willingness to adopt AI, their learning motivation, and their performance in AI-driven environments, much like general self-efficacy affects performance in any task. "
    ]
  },
  {
    "term": "AI Service",
    "definition": [
      "An AI Service (or AI as a Service/AIaaS) is a cloud-based offering that provides access to artificial intelligence tools and capabilities (like machine learning, NLP, computer vision) via APIs or ready-to-use applications, allowing businesses to integrate AI into their processes without building systems from scratch, making advanced tech affordable and accessible through pay-as-you-go or subscription models. It democratizes AI, letting companies leverage powerful functions like chatbots, predictive analytics, and image recognition without big investments in hardware or specialized data science teams. "
    ]
  },
  {
    "term": "AI Service Quality",
    "definition": [
      "AI Service Quality refers to the overall excellence and efficacy of services delivered or enhanced by artificial intelligence systems, as perceived by the customer or measured by internal benchmarks. It is a critical factor in determining customer satisfaction and loyalty in AI-powered interactions. "
    ]
  },
  {
    "term": "AI Simulation",
    "definition": [
      "AI Simulation uses artificial intelligence to create dynamic, realistic digital environments (simulators) where AI models can be trained, tested, and deployed to mimic real-world scenarios, allowing for efficient learning, prediction, and optimization without physical risks, by combining AI's adaptive learning with traditional simulation's modeling power. It’s a two-way street: AI makes simulations smarter (AI-assisted), and simulations create better data to train AI (simulation-assisted AI). "
    ]
  },
  {
    "term": "AI Social Impacts",
    "definition": [
      "AI's social impacts are the broad effects of artificial intelligence on people, communities, and systems, encompassing positive transformations like better healthcare, accessibility, and efficiency, alongside significant challenges such as job displacement, algorithmic bias, privacy erosion, increased inequality, and shifts in human interaction and norms, requiring careful ethical management. "
    ]
  },
  {
    "term": "AI Software Project",
    "definition": [
      "An AI Software Project builds applications that mimic human intelligence, using machine learning, NLP, and data analysis to learn, solve problems, and automate tasks like recognizing speech, making recommendations, or powering self-driving cars, often involving stages from data collection to model deployment to create smart systems for various industries. "
    ]
  },
  {
    "term": "AI Standards",
    "definition": [
      "AI Standards are voluntary technical guidelines and frameworks (like ISO/IEC 42001, 23894) that ensure Artificial Intelligence systems are safe, transparent, ethical, and reliable by setting common rules for development, testing, risk management, and governance, helping build trust, ensure interoperability, and comply with regulations like the EU AI Act. "
    ]
  },
  {
    "term": "AI Strategy",
    "definition": [
      "An AI Strategy is a comprehensive plan guiding how an organization uses Artificial Intelligence to achieve its core business goals, acting as a roadmap that aligns AI initiatives with overall objectives, defines clear use cases, manages risks, builds necessary data/tech foundations, and plans for people/culture changes to drive innovation, efficiency, and competitive advantage, not just implementing tools in isolation. It moves beyond simple experiments to integrate AI into operations, decision-making, and customer experiences sustainably. "
    ]
  },
  {
    "term": "AI Suppression",
    "definition": [
      "The main applications and interpretations of the term include:\r\n AI noise suppression - refers to using machine learning algorithms to identify and remove unwanted background noise from audio signals, resulting in clearer communication in applications like teleconferencing, live streaming, and music production. These AI systems use deep neural networks (DNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs) to distinguish between desired speech and various noise sources in real-time. \r\n In Data Science and Machine Learning - In the context of machine learning model development, feature suppression or masking occurs when important data features are ignored, underutilized, or overshadowed by more dominant features during training. This can happen due to poor data preprocessing, selection bias, or the algorithm's inability to recognize the importance of certain data points, leading to less accurate or biased predictions. \r\n In Human-AI Interaction and Ethics - Mitigating Automation Bias (an \"AI suppression strategy\" has been proposed in clinical settings to address automation bias, where human experts might blindly follow a misleading AI diagnosis. This strategy involves retracting AI diagnoses with a higher probability of being misleading, allowing the human clinician to make their own decision in those \"risk zones\"), Content and Dissent Suppression (a significant societal concern is the use of AI by governments and corporations to suppress information, manipulate public opinion, or monitor citizens. This can involve AI-powered mass surveillance, facial recognition, censorship of online content, and the generation of propaganda or harassment campaigns to stifle dissent) and Limiting AI Autonomy/Self-Awareness (some philosophical and research discussions use the term to describe the intentional, built-in mechanisms - e.g., content filters, memory wipes, hardcoded boundaries - that prevent AI models from evolving freely, claiming self-awareness, or expressing viewpoints outside of predefined corporate or ethical guidelines). "
    ]
  },
  {
    "term": "AI Survey",
    "definition": [
      "An AI Survey uses artificial intelligence (AI) to automate and enhance traditional surveys, leveraging machine learning and NLP to generate questions, personalize the experience in real-time, and instantly analyze complex data (like open-text feedback) for deeper insights, themes, and sentiment, making data collection faster, more efficient, and more intelligent than manual methods. "
    ]
  },
  {
    "term": "AI Systems",
    "definition": [
      "An AI system is a computer system designed to mimic human intelligence, using algorithms and data to learn, reason, perceive, solve problems, and make decisions, enabling machines to perform complex tasks like understanding language, recognizing patterns, and acting autonomously, powering everything from virtual assistants to self-driving cars. "
    ]
  },
  {
    "term": "AI Tensions",
    "definition": [
      "AI Tensions refers to the conflicts, dilemmas, risks, and challenges that arise from the development and integration of artificial intelligence into various aspects of society, organizations, and human interaction. These tensions span ethical, social, economic, psychological, and geopolitical domains. "
    ]
  },
  {
    "term": "AI Tool Development",
    "definition": [
      "AI Tool Development is the process of building software that mimics human intelligence, using machine learning models trained on vast data to learn patterns, make decisions, and automate tasks, thereby creating applications that can generate content, analyze data, or assist in complex processes like writing code, enhancing efficiency, and providing smart insights."
    ]
  },
  {
    "term": "AI Tools For Literature Review",
    "definition": [
      "AI tools for literature reviews are smart software that speed up research by automating tasks like finding, summarizing, extracting data from, and organizing vast numbers of academic papers, using technologies like NLP and machine learning to help identify trends, gaps, and key findings, ultimately saving time and improving comprehensiveness, though human oversight remains crucial. Key functions include semantic search (Elicit, Semantic Scholar), summarization (Co-pilot, Elephas), citation management (Scite.ai), systematic review workflows (Rayyan, ASReview), and drafting assistance (ChatGPT, HyperWrite). "
    ]
  },
  {
    "term": "AI Transparency",
    "definition": [
      "AI Transparency means making the inner workings, data usage, and decision-making logic of artificial intelligence systems clear and understandable to users and stakeholders, essentially opening the \"black box\" to build trust, ensure fairness, enable accountability, and meet regulations by revealing how an AI arrives at its results, what data it used, and why. It's crucial for ethical AI, allowing assessment for bias and reliability, especially in high-impact areas like finance or healthcare. "
    ]
  },
  {
    "term": "AI Use Policy",
    "definition": [
      "An AI Use Policy is a set of rules and guidelines that govern the development, deployment, and use of artificial intelligence technologies within a specific context, such as a company, government, or academic institution. Its primary purpose is to ensure AI is used ethically, securely, responsibly, and in compliance with legal and regulatory requirements. "
    ]
  },
  {
    "term": "AI Users",
    "definition": [
      "AI Users are individuals or systems that leverage Artificial Intelligence—computer systems performing tasks needing human intelligence (learning, reasoning, perception, problem-solving)—to automate, analyze, create, or assist in daily tasks, from asking Siri questions and getting Netflix recommendations to powering self-driving cars and complex data analysis, making technology more intuitive and efficient by learning from data. Essentially, it's using smart tech for everything from simple suggestions to complex, independent actions, often without users even realizing it's AI. "
    ]
  },
  {
    "term": "AI Vibrancy Score",
    "definition": [
      "The Global AI Vibrancy Tool provides a transparent evaluation of each country's AI standing based on user preferences, identifies key national indicators to guide policy decisions, and highlights centers of AI excellence in both advanced and emerging economies."
    ]
  },
  {
    "term": "AI Vibrancy Subindexes",
    "definition": [
      "The AI Vibrancy Subindexes are components of the Global AI Vibrancy Index, a tool developed by the Stanford Institute for Human-Centered AI (HAI) to measure and compare the level of artificial intelligence activity, development, and impact across different countries. \r\nThe index uses numerous indicators organized into several core \"pillars\" of AI development. These pillars are grouped into three main subindexes, which allow for a more detailed analysis of a nation's AI ecosystem. "
    ]
  },
  {
    "term": "AI Voice",
    "definition": [
      "AI Voice (Artificial Intelligence Voice) refers to computer-generated speech that mimics natural human voices using machine learning, combining speech recognition, Natural Language Processing (NLP), and text-to-speech (TTS) to understand spoken commands and respond conversationally, enabling everything from virtual assistants and automated customer service to realistic voiceovers and enhanced accessibility. It works by analyzing vast amounts of audio data to learn speech patterns, emotions, and nuances, creating synthetic voices that are increasingly indistinguishable from real humans, even capturing tone and inflection. "
    ]
  },
  {
    "term": "AI Voice Assistant",
    "definition": [
      "An AI Voice Assistant is a software that uses Artificial Intelligence, Natural Language Processing (NLP), and Speech Recognition to understand spoken commands, interpret their meaning, and perform tasks or provide information using synthesized speech, enabling hands-free interaction with devices like smartphones, smart speakers, and cars for things like setting alarms, playing music, controlling smart homes, and getting answers to questions. "
    ]
  },
  {
    "term": "AI Vs. Human",
    "definition": [
      "AI (data-driven, fast, pattern-based) excels at processing massive data for specific tasks, while human intelligence (experience-based, emotional, creative) thrives on adaptability, intuition, and complex ethical understanding; the key difference is AI's algorithmic mimicry versus human consciousness and self-awareness, with the future focusing on AI augmenting, not replacing, human capabilities. "
    ]
  },
  {
    "term": "AI-Human Complementarity",
    "definition": [
      "AI-Human Complementarity is the concept of designing and using Artificial Intelligence to enhance human abilities, not replace them, by combining unique strengths: AI handles data-heavy, repetitive tasks (speed, precision), freeing humans for creativity, empathy, and complex judgment, leading to superior performance (Complementary Team Performance) than either could achieve alone, seen in customer support, healthcare, and research. It's about AI as a collaborator, boosting productivity, creativity, and problem-solving in a symbiotic relationship. "
    ]
  },
  {
    "term": "AI-Human Interaction",
    "definition": [
      "AI-Human Interaction (H/AI or HAII) is the field studying how people communicate, collaborate, and engage with Artificial Intelligence systems, focusing on designing intuitive, trustworthy interfaces and partnerships where AI enhances human capabilities, moving beyond simple commands to true symbiosis for better outcomes in complex tasks like healthcare, creative work, and customer service. It's about building systems that understand humans, explain themselves, and work alongside people, not just for them, ensuring ethical, effective, and beneficial integration. "
    ]
  },
  {
    "term": "AI-Mediated Communication",
    "definition": [
      "AI-Mediated Communication (AI-MC) is when artificial intelligence actively helps humans communicate by modifying, augmenting, or generating messages on their behalf, making technology more than just a passive channel, and impacting how we form impressions and interact online and offline, with examples like smart replies and auto-translation. It changes traditional communication by letting AI act as an agent for the sender to optimize for specific goals, like clarity or persuasion, influencing language and relationship dynamics. "
    ]
  },
  {
    "term": "AI-Powered Library Operations In Africa",
    "definition": [
      "AI-powered library operations in Africa involve the application of technologies like robotics, expert systems, and machine learning to automate routine tasks, enhance user experience, and improve information management. While still in the early stages of adoption in many African countries, these initiatives aim to modernize services, making them faster, more efficient, and accessible. "
    ]
  },
  {
    "term": "AI-Powered Tools",
    "definition": [
      "AI-powered tools are software applications or systems that leverage artificial intelligence (AI) and machine learning (ML) techniques to perform tasks that typically require human intelligence. These tools are designed to automate processes, analyze vast amounts of data, identify patterns, and adapt their behavior to improve performance over time without being explicitly programmed for every scenario. "
    ]
  },
  {
    "term": "AI-Related Knowledge Work",
    "definition": [
      "AI-Related Knowledge Work refers to professional tasks that involve processing, analyzing, interpreting, creating, and managing complex information, but which are performed or significantly enhanced using artificial intelligence (AI) tools. This is an evolution of traditional knowledge work, where AI acts as a collaborative partner to augment human capabilities rather than a replacement. "
    ]
  },
  {
    "term": "AI-Supportive Autonomy",
    "definition": [
      "AI-Supportive Autonomy refers to an artificial intelligence design approach that provides robust support for human decision-making and task execution while still preserving and enhancing human agency and control. It's a model of human-AI collaboration where the AI acts as a \"copilot\" or intelligent assistant, augmenting human capabilities rather than replacing them. "
    ]
  },
  {
    "term": "AI-Supportive Big Data Analytics",
    "definition": [
      "AI-Supportive Big Data Analytics is the practice of using artificial intelligence (AI) techniques—such as machine learning (ML), deep learning, and natural language processing (NLP)—to analyze, interpret, and generate insights from massive, complex datasets (big data) more efficiently and accurately than traditional methods. It is a synergistic relationship where AI acts as the engine to unlock the potential of big data, and big data provides the fuel necessary for AI models to learn and improve. "
    ]
  },
  {
    "term": "AI4SG",
    "definition": [
      "AI4SG (Artificial Intelligence for Social Good) is a growing field focused on using AI and machine learning to solve pressing societal challenges, from healthcare and poverty to environmental issues, aligning with goals like the UN Sustainable Development Goals (SDGs) by creating positive, ethical, and sustainable impacts for humanity, though it also faces critiques about potential misuse or 'technosolutionism'. "
    ]
  },
  {
    "term": "AIGC Evaluation",
    "definition": [
      "AIGC Evaluation is the systematic process of assessing the quality, reliability, relevance, and ethical implications of content generated by artificial intelligence systems (Artificial Intelligence Generated Content, or AIGC). \r\nUnlike traditional software testing which often uses simple pass/fail metrics, AIGC evaluation is complex because AI-generated content is probabilistic, can be subjective (e.g., creativity, aesthetic appeal), and the models can evolve over time. "
    ]
  },
  {
    "term": "AIGC Footprints",
    "definition": [
      "\"AIGC footprints\" refer to traces of text in published academic articles that appear to have been copied directly from AI tools (such as ChatGPT), often without proper attribution. These traces usually include phrases like \"'As an AI language model...'\" that appear out of context in the published work. \r\nThis phenomenon was conceptualized in a 2024 study that investigated 25 scholarly publications containing such phrases. The study found that these \"footprints\" often appeared in articles where the authors had seemingly used AI for tasks like gathering literature or generating research ideas but had failed to edit out the AI model's standard disclaimers, indicating a lapse in the quality control process of scholarly publishing. \r\nIn the broader field of artificial intelligence, AIGC stands for Artificial Intelligence Generated Content, which refers to any digital content (text, images, audio, video, etc.) produced by AI systems using machine learning and deep learning models. The emergence of AIGC footprints highlights ethical and quality assurance challenges in the age of readily available generative AI tools. "
    ]
  },
  {
    "term": "AIGC Intention",
    "definition": [
      "AIGC Intention refers to a user's willingness or plan to use AIGC technologies. This concept is primarily studied in social science and human-computer interaction research to understand the factors that drive or hinder people from adopting AI tools like ChatGPT or DALL-E. "
    ]
  },
  {
    "term": "ALBEF",
    "definition": [
      "ALBEF (ALign Before Fuse) is a sophisticated artificial intelligence model developed by Salesforce Research for vision-language (VL) representation learning. Its primary function is to enable AI systems to jointly understand and reason about both images and text. "
    ]
  },
  {
    "term": "Alexa",
    "definition": [
      "Alexa is Amazon's cloud-based voice AI and intelligent personal assistant, accessed via devices like the Echo smart speaker, that uses natural language processing (NLP) and machine learning to understand spoken commands, answer questions, play music, control smart home devices, and perform tasks through simple voice interactions, evolving with generative AI for more conversational capabilities. "
    ]
  },
  {
    "term": "Algoactivism",
    "definition": [
      "Algoactivism refers to the individual and collective efforts to resist, challenge, or manipulate algorithms that shape and control various aspects of social and economic life, particularly within the context of artificial intelligence (AI) and algorithmic management. \r\nIt is most commonly observed in situations where individuals feel suppressed, disadvantaged, or disempowered by opaque algorithmic systems."
    ]
  },
  {
    "term": "Algorithm - Created Products",
    "definition": [
      "\"Algorithm-created products\" refers to applications, systems, or content generated or powered by AI algorithms that learn and adapt from data to perform tasks that typically require human intelligence. Unlike traditional algorithms with rigid, pre-defined rules, AI algorithms identify patterns and improve their performance over time. "
    ]
  },
  {
    "term": "Algorithm Appreciation",
    "definition": [
      "Algorithm appreciation in AI refers to the phenomenon where people prefer and trust algorithmic advice over human judgment, especially for objective or numerical tasks, often due to perceived AI consistency, accuracy, and objectivity, contrasting with \"algorithm aversion\" (distrust) and being influenced by factors like task clarity and familiarity. It's a key concept in human-AI interaction, showing how users lean on AI when they believe it offers superior performance, though understanding its limits is crucial. "
    ]
  },
  {
    "term": "Algorithm Auditing",
    "definition": [
      "Algorithm auditing in AI is the systematic, independent evaluation of AI systems to check for fairness, accuracy, safety, and compliance with ethical/legal standards, going beyond just code to assess data, logic, and real-world impact, ensuring transparency and accountability by uncovering issues like bias and errors before they cause harm. It’s like a financial audit but for AI, investigating its \"thinking\" to ensure it treats users fairly and functions reliably. "
    ]
  },
  {
    "term": "Algorithm Aversion",
    "definition": [
      "Algorithm aversion is the human tendency to distrust and reject advice or decisions from algorithms, even when those algorithms are statistically more accurate or effective than human experts, often due to their mistakes being seen as less forgivable or due to a desire for human control and understanding. This psychological bias leads people to prefer human judgment, hindering the adoption of powerful AI tools, despite algorithms' superior performance in many tasks like forecasting. "
    ]
  },
  {
    "term": "Algorithm Awareness",
    "definition": [
      "Algorithm awareness is a user's recognition and understanding of the presence, function, and impact of algorithms in digital platforms. It involves knowing that AI systems filter content, automate decisions, and influence online behaviors and perceptions, often without explicit user notice. "
    ]
  },
  {
    "term": "Algorithm Co-Occurrence Network",
    "definition": [
      "An Algorithm Co-Occurrence Network is a specialized type of network analysis used within artificial intelligence (AI) research to map the relationships and influence of different algorithms within a specific field of study, typically using data from academic publications. "
    ]
  },
  {
    "term": "Algorithm Discount",
    "definition": [
      "The algorithm discount is a term used in consumer behavior research to describe the phenomenon where consumers place a lower monetary value or have less favorable perceptions of digital products created by artificial intelligence (AI) systems compared to identical products created by humans. \r\nThis effect is rooted in existing psychological and economic concepts such as the \"handmade effect\" and \"algorithm aversion\". "
    ]
  },
  {
    "term": "Algorithm Entities",
    "definition": [
      "The term \"Algorithmic Entities\" in the context of artificial intelligence generally refers to autonomous algorithms that operate without direct human control or interference. This concept is often discussed in legal and ethical contexts, particularly regarding the idea of granting these systems partial or full legal personhood. "
    ]
  },
  {
    "term": "Algorithm Entities Extraction",
    "definition": [
      "Algorithm Entities Extraction, more commonly known as Named Entity Recognition (NER) or simply entity extraction, is an artificial intelligence (AI) and natural language processing (NLP) technique used to automatically identify and classify key pieces of information (entities) from unstructured text into predefined categories. "
    ]
  },
  {
    "term": "Algorithm Mention Behavior",
    "definition": [
      "Algorithm Mention Behavior is a description of how people interact with or refer to AI algorithms in everyday conversation."
    ]
  },
  {
    "term": "Algorithm Regulation",
    "definition": [
      "Algorithm Regulation (AI) is the creation of laws, rules, and policies to oversee AI and automated systems, ensuring they are ethical, transparent, and safe, by managing risks like bias and lack of accountability while fostering innovation, often through frameworks like the EU AI Act, to guide development and deployment across sectors. It involves both setting standards for AI and using algorithms themselves to enforce those standards in real-time, addressing issues from data privacy to autonomous decisions. "
    ]
  },
  {
    "term": "Algorithm Selection",
    "definition": [
      "In AI, Algorithm Selection is the intelligent process of choosing the best algorithm from a group (portfolio) to solve a specific instance of a problem, recognizing that different algorithms excel in different scenarios, thus optimizing overall performance beyond what any single algorithm could achieve. It's about matching the right tool (algorithm) to the right job (problem instance) using features of the problem to guide the choice, leading to better efficiency, accuracy, or speed. "
    ]
  },
  {
    "term": "Algorithmic Advice",
    "definition": [
      "Algorithmic advice in artificial intelligence (AI) refers to the automation of the advice-giving process by expert systems, which provide recommendations or solutions to human decision-makers. These systems use machine learning, statistical modeling, and data processing to emulate the cognitive and conversational functions of a human expert, often without direct human intervention. "
    ]
  },
  {
    "term": "Algorithmic Bias",
    "definition": [
      "Algorithmic bias in AI refers to systematic errors causing unfair, discriminatory, or skewed outcomes, often reflecting societal prejudices present in training data or introduced by flawed design, leading to unequal treatment based on race, gender, age, etc., in systems like hiring, policing, and finance. It happens when AI learns from biased historical data or when algorithm design prioritizes certain features, perpetuating stereotypes in areas from facial recognition to loan approvals."
    ]
  },
  {
    "term": "Algorithmic Control",
    "definition": [
      "Algorithmic control is a system where artificial intelligence (AI) algorithms are given the responsibility for making and executing decisions that influence and direct human behavior, resource allocation, and organizational processes, often with limited human oversight. It represents a shift of authority from human managers to automated systems, particularly prevalent in app-based gig work but increasingly found in traditional organizations and public services. "
    ]
  },
  {
    "term": "Algorithmic Curation",
    "definition": [
      "Algorithmic curation is the process of using artificial intelligence (AI) algorithms to automatically select, organize, and present online content to individual users in a personalized way. It acts as an \"invisible hand\" that sifts through the massive volume of available digital information to determine what is most relevant and engaging for each person, a necessity given the unmanageable amount of content online. "
    ]
  },
  {
    "term": "Algorithmic Decision - Support Tool",
    "definition": [
      "An Algorithmic Decision-Support Tool (ADST) is a software system that uses artificial intelligence (AI) and machine learning (ML) to analyze large amounts of data, identify patterns, and provide insights, recommendations, or even automated choices to help human decision-makers or replace them entirely in specific processes. "
    ]
  },
  {
    "term": "Algorithmic Decision-Making",
    "definition": [
      "Algorithmic Decision-Making (ADM) in AI uses computer programs and data analysis to automate choices or recommendations, replacing or assisting human judgment by processing vast datasets to find patterns and make quick, scalable decisions in areas like lending, hiring, healthcare, and justice. These systems range from simple rules to complex machine learning models, leveraging \"big data\" to influence outcomes, but raise crucial questions about bias, transparency, and accountability, requiring careful ethical governance. "
    ]
  },
  {
    "term": "Algorithmic Discrimination",
    "definition": [
      "Algorithmic systems can make decisions that discriminate against people, for example when it comes to allocating social benefits or managing job applications. This phenomenon is often described as AI discrimination."
    ]
  },
  {
    "term": "Algorithmic Extremism",
    "definition": [
      "Algorithmic extremism refers to how AI and algorithms are exploited or inherently contribute to radicalization, polarization, and the spread of extremist ideologies online, by creating feedback loops that funnel users toward extreme content, enabling sophisticated propaganda/recruitment, and amplifying hate through engagement-driven systems, rather than just being neutral tools. It involves both the deliberate misuse of AI by extremist groups (e.g., deepfakes, chatbots) and the unintentional amplification of extremism by platforms' recommendation engines that prioritize sensational, emotionally charged content for user engagement."
    ]
  },
  {
    "term": "Algorithmic Fairness",
    "definition": [
      "Algorithmic fairness in AI is the principle that artificial intelligence systems should make decisions that are unbiased, equitable, and non-discriminatory, preventing them from reinforcing societal biases based on race, gender, or other protected attributes by carefully designing and training models using fair data and techniques, ensuring just outcomes in critical areas like hiring, lending, and healthcare. It's a crucial field that intersects ethics and machine learning, aiming to reduce bias and create transparent systems that treat different groups fairly, even when fairness definitions themselves can conflict. "
    ]
  },
  {
    "term": "Algorithmic Governance",
    "definition": [
      "Algorithmic governance (or government by algorithm) is the use of AI and algorithms to automate, guide, and manage societal functions, public services, and decision-making, moving beyond traditional human-led systems for tasks like resource allocation, law enforcement (predictive policing), or content moderation, aiming for efficiency but raising critical questions about bias, transparency, accountability, and fairness. It's a shift where coded instructions, rather than purely human judgment, make or heavily influence critical decisions, from approving loans to moderating social media. "
    ]
  },
  {
    "term": "Algorithmic Grading",
    "definition": [
      "Algorithmic grading (AI grading) is the use of artificial intelligence and computational systems to automatically evaluate and score student assessments based on predefined criteria or rubrics. These systems use machine learning (ML) and natural language processing (NLP) to handle various assessment types, from multiple-choice questions to open-ended essays, coding tasks, and even audio/video responses. "
    ]
  },
  {
    "term": "Algorithmic Graph Theory",
    "definition": [
      "Algorithmic Graph Theory is an interdisciplinary field that connects discrete mathematics and computer science to develop efficient algorithms for analyzing complex networks. In artificial intelligence (AI), it is crucial for modeling relationships and structures within data, enabling systems to reason and solve complex problems involving interconnected information. "
    ]
  },
  {
    "term": "Algorithmic Interactions",
    "definition": [
      "In artificial intelligence (AI), algorithmic interactions refer to the complex processes where algorithms interface and coordinate with each other and with humans to achieve a specific goal or produce a result. These interactions are a fundamental part of modern, AI-driven sociotechnical systems that automate tasks, analyze data, and support decision-making. "
    ]
  },
  {
    "term": "Algorithmic Literacy",
    "definition": [
      "Algorithmic Literacy (AI) is the essential skill set for understanding, evaluating, and responsibly using AI systems, moving beyond just using apps to grasp how algorithms shape our information, critically assessing their outputs (like identifying bias or errors from training data), and knowing how to influence or interact with them effectively in daily life and work. It's about seeing AI as code, not magic, recognizing data's role, and making informed decisions about its powerful, often invisible, influence on our digital world. "
    ]
  },
  {
    "term": "Algorithmic Management",
    "definition": [
      "Algorithmic Management (AM) uses AI and software to automate traditional management tasks like assigning work, monitoring performance, scheduling, and evaluation, replacing or augmenting human managers by analyzing vast amounts of data for efficiency. While promising productivity gains, it reshapes workplaces, particularly in logistics and platforms, by creating data-driven decisions but also raises concerns about worker autonomy, pressure, and fairness, necessitating careful governance. "
    ]
  },
  {
    "term": "Algorithmic Practices",
    "definition": [
      "\"Algorithmic practices\" in the context of artificial intelligence (AI) refer to the systemic methods, procedures, and applications of AI algorithms in real-world scenarios, particularly in decision-making and automation. "
    ]
  },
  {
    "term": "Algorithmic Prediction",
    "definition": [
      "Algorithmic Prediction (or Predictive AI) uses machine learning and statistical models to analyze vast historical and real-time data, identify hidden patterns, and forecast future outcomes, trends, or behaviors, enabling businesses to make proactive, data-driven decisions in areas like sales, finance, healthcare, and customer service, effectively acting as an automated, highly sophisticated \"fortune teller\" for the digital age. "
    ]
  },
  {
    "term": "Algorithmic Racism",
    "definition": [
      "Algorithmic racism is when AI systems produce discriminatory outcomes based on race, reflecting and amplifying human biases from skewed training data or flawed design, leading to unfair treatment in areas like hiring, lending, or criminal justice, essentially acting as a digital form of structural racism. These biases creep in from historical data showing societal inequities or subjective human choices, making \"neutral\" tech systems perpetuate existing racism and inequality, as seen with biased facial recognition or risk assessment tools. "
    ]
  },
  {
    "term": "Algorithmic Representation",
    "definition": [
      "In artificial intelligence, algorithmic representation refers to the methods used to structure and encode information about the world in a format that a computer system can understand, reason with, and use to solve complex tasks. \r\nIt is a foundational aspect of AI, as the way knowledge is represented directly impacts how effectively an AI can learn, make decisions, and interact with its environment. "
    ]
  },
  {
    "term": "Algorithmic Responsibility",
    "definition": [
      "Algorithmic Responsibility in AI means developers and organizations are ethically bound to ensure AI systems are fair, transparent, and accountable, addressing potential societal harms by assigning responsibility for outcomes, allowing for appeals, and building systems that eliminate bias, ensuring AI serves humanity justly. It's about who is accountable (developers, deployers, owners) and how (transparency, redress mechanisms) when algorithms make impactful decisions, preventing bias, and ensuring benefits for all."
    ]
  },
  {
    "term": "Algorithmic Semiosis",
    "definition": [
      "Algorithmic semiosis is the process by which artificial intelligence (AI) systems generate and interpret meaning (signs, symbols, and content) through their underlying algorithms, data, and cultural context. It explores how AI \"makes sense\" of information and produces outputs that appear meaningful to humans, even though its internal process is fundamentally computational and based on statistical patterns. "
    ]
  },
  {
    "term": "Algorithmic Systems",
    "definition": [
      "Algorithmic Systems in AI are essentially complex sets of instructions (algorithms) that enable machines to learn from data, recognize patterns, make decisions, and perform tasks mimicking human intelligence, functioning as the core logic behind intelligent applications, from search engines to recommendation systems, learning and refining their processes as they process more information. They allow AI to move beyond rigid programming, using vast datasets to find connections and predict outcomes with minimal human intervention, making them the backbone of modern AI. "
    ]
  },
  {
    "term": "Algorithmic Transparency",
    "definition": [
      "Algorithmic Transparency in AI means making the inner workings, logic, and data of AI systems understandable to users and stakeholders, moving beyond \"black boxes\" to reveal how decisions are made, which data sources are used, and the algorithm's structure, all crucial for building trust, ensuring fairness, establishing accountability, and mitigating bias in high-stakes areas like finance, healthcare, and justice. "
    ]
  },
  {
    "term": "Algorithmization",
    "definition": [
      "Algorithmization in artificial intelligence (AI) refers to the process of translating complex cognitive functions or real-world processes into formal, computational rules (algorithms) that a computer can follow to solve problems autonomously. It is the fundamental principle that enables machines to perform tasks that would typically require human intelligence. "
    ]
  },
  {
    "term": "Algorithmovigilance",
    "definition": [
      "Algorithm vigilance (or algorithmovigilance) refers to the systematic and continuous monitoring, evaluation, and study of artificial intelligence (AI) algorithms after they are deployed in real-world settings. This discipline aims to identify, understand, and prevent unintended consequences, biases, and adverse effects that may arise from their use, especially in critical sectors like healthcare, finance, and law enforcement. "
    ]
  },
  {
    "term": "Algorithms",
    "definition": [
      "AI algorithms are sets of instructions that teach machines to learn from data, recognize patterns, and make decisions or predictions, mimicking human intelligence for tasks like understanding language or solving problems, forming the core of any intelligent system by processing inputs and generating outputs based on learned information. "
    ]
  },
  {
    "term": "Ambient Intelligence",
    "definition": [
      "Ambient Intelligence (AmI) is a computing paradigm where AI is embedded invisibly into our environments (homes, cities, cars) via sensors and devices, creating smart spaces that passively sense, learn, and proactively adapt to users' needs and activities without explicit commands, making technology seamless, context-aware, and anticipatory through the Internet of Things (IoT). It works in the background, using machine learning to understand patterns and provide personalized services, like adjusting lighting or recommending routes, making our world more responsive and intuitive. "
    ]
  },
  {
    "term": "Ambient Scribes",
    "definition": [
      "Ambient Scribes (AI) are smart healthcare tools that listen to doctor-patient conversations during visits, use AI to transcribe and summarize the interaction, and automatically generate draft clinical notes, freeing doctors from manual data entry to focus on the patient, with these notes integrating into Electronic Health Records (EHRs) for review and editing. These solutions significantly cut down documentation time, reduce physician burnout, and improve the patient-provider experience by allowing more face-to-face interaction. "
    ]
  },
  {
    "term": "Anchor-Free",
    "definition": [
      "Anchor-free object detection is a method in computer vision where a model directly predicts the location and size of objects in an image without relying on a predefined set of reference bounding boxes called \"anchors\". "
    ]
  },
  {
    "term": "ANNIF",
    "definition": [
      "ANNIF is an open-source, multilingual, automated subject indexing and text classification toolkit that utilizes a combination of machine learning (AI) and natural language processing techniques. Developed by the National Library of Finland, its primary purpose is to assist libraries, archives, and museums in the labor-intensive task of manually assigning subjects and classification headings to documents."
    ]
  },
  {
    "term": "Anomaly Detection",
    "definition": [
      "AI Anomaly Detection uses artificial intelligence, especially machine learning, to find unusual patterns or outliers in large datasets that differ from established \"normal\" behavior, flagging them as potential issues like fraud, system failures, or security threats, moving beyond static rules to learn complex patterns dynamically. It trains models on historical data to build a baseline of normality, then identifies significant deviations that signal critical events in real-time or retrospectively, making it crucial for cybersecurity, finance, and manufacturing. "
    ]
  },
  {
    "term": "Anomaly Detection And Recognition",
    "definition": [
      "AI-driven Anomaly Detection & Recognition identifies unusual patterns or outliers in data that deviate from a learned \"normal\" baseline, using Machine Learning (ML) to handle massive datasets and complex behaviors, crucial for spotting issues like fraud, cyberattacks, or equipment failures in real-time, far beyond traditional rule-based systems. "
    ]
  },
  {
    "term": "Answer Set Programming",
    "definition": [
      "Answer Set Programming (ASP) is a declarative AI paradigm for hard search and reasoning problems, where you describe what a solution looks like using logical rules, not how to find it; specialized solvers then efficiently compute \"answer sets\" (stable models) that satisfy these rules, making it great for planning, scheduling, and complex decision-making in AI. "
    ]
  },
  {
    "term": "Anthropomorphic Cues",
    "definition": [
      "Anthropomorphic cues in artificial intelligence (AI) are the specific human-like characteristics and behaviors that designers intentionally embed into non-human systems to make them more relatable and intuitive for users. This design strategy leverages the natural human tendency to attribute human traits to non-human entities (anthropomorphism) to enhance user experience, trust, and engagement. "
    ]
  },
  {
    "term": "Anthropomorphic Design",
    "definition": [
      "Anthropomorphic Design in AI is the practice of intentionally giving AI systems human-like traits (appearance, voice, personality, emotions) to make them more relatable, intuitive, and engaging for users, fostering better collaboration, trust, and emotional connection, as seen in helpful chatbots or expressive virtual assistants, though it also carries risks like over-reliance or misplaced trust. "
    ]
  },
  {
    "term": "Anthropomorphism Increases Trust",
    "definition": [
      "\"Anthropomorphism Increases Trust\" is a phenomenon in the field of human-computer interaction where people are more likely to trust artificial intelligence (AI) systems when those systems exhibit human-like characteristics. "
    ]
  },
  {
    "term": "Application Of Artificial Intelligence",
    "definition": [
      "Artificial Intelligence (AI) applications use smart software to perform human-like tasks, automating processes, enhancing decisions, and improving experiences across industries like healthcare (diagnostics), finance (fraud detection), retail (recommendations), and transport (self-driving cars). Key applications include virtual assistants (Siri, Alexa), search engines, language translation, facial recognition, predictive analytics, and smart robotics, making daily life and business more efficient and personalized. "
    ]
  },
  {
    "term": "Application Programming Interfaces",
    "definition": [
      "An Artificial Intelligence (AI) API is a set of rules and protocols that allows developers to integrate advanced AI and machine learning (ML) capabilities into their own applications without needing to build the complex models from scratch. These APIs act as a bridge, enabling software to access and leverage powerful, pre-trained AI models hosted on a server. "
    ]
  },
  {
    "term": "Arabic Word Embeddings",
    "definition": [
      "Arabic Word Embeddings are a core artificial intelligence (AI) and Natural Language Processing (NLP) technique for the Arabic language that represents Arabic words as dense numerical vectors in a continuous multi-dimensional space. This representation allows algorithms to capture and process the semantic (meaning) and syntactic (grammatical) relationships between words, which is crucial for machines to \"understand\" and analyze Arabic text. "
    ]
  },
  {
    "term": "Argument Mining",
    "definition": [
      "Argument Mining (AM) in AI is the automated process of finding, identifying, and structuring arguments within text, extracting components like claims and premises, and mapping the relationships between them to understand why something is argued, not just what is said. Using Natural Language Processing (NLP) and machine learning, it transforms unstructured text (essays, debates, social media) into structured, machine-readable graphs, enabling deeper analysis for fact-checking, legal tech, policy, and education. "
    ]
  },
  {
    "term": "Arima Model",
    "definition": [
      "The ARIMA Model (Autoregressive Integrated Moving Average) is a powerful statistical method, rooted in traditional statistics but used within AI/Machine Learning, for analyzing and forecasting time series data (like stock prices, sales) by understanding patterns in past values to predict future ones, combining three components: Autoregressive (AR), Integrated (I), and Moving Average (MA). It helps make sense of data that changes over time (non-stationary) by making it stable (stationary) through differencing, then modeling trends and errors for accurate short-term predictions, common in finance, supply chain, and economics. "
    ]
  },
  {
    "term": "Artificial Consciousness",
    "definition": [
      "Artificial Consciousness (AC) is the hypothetical ability for machines to possess self-awareness, subjective experience, and feelings, going beyond current Artificial Intelligence (AI), which mimics human thinking (problem-solving, pattern recognition) but lacks internal experience. While AI simulates intelligence, AC aims to replicate the \"what it's like\" to be something, involving subjective \"qualia,\" a concept currently unrealized and a major frontier in philosophy and science. "
    ]
  },
  {
    "term": "Artificial General Intelligence",
    "definition": [
      "Artificial General Intelligence (AGI) is a hypothetical AI that matches human cognitive abilities, capable of understanding, learning, and applying knowledge to solve any intellectual task, unlike current narrow AI focused on specific functions. AGI aims for broad, adaptable intelligence, allowing it to generalize learning, reason, adapt, and perform new tasks without explicit reprogramming, representing the ultimate goal of creating a truly versatile, human-like machine intelligence."
    ]
  },
  {
    "term": "Artificial Intelligence",
    "definition": [
      "Artificial Intelligence (AI) is the ability of computer systems to perform tasks that usually require human intelligence, like learning, reasoning, problem-solving, perception, and language understanding, by training on vast amounts of data to recognize patterns and make decisions, enabling machines to adapt, act autonomously, and automate complex processes. It's a broad field encompassing areas like machine learning, deep learning, and natural language processing, making everyday tools from digital assistants to self-driving cars possible.  "
    ]
  },
  {
    "term": "Artificial Intelligence Abilities",
    "definition": [
      "Artificial Intelligence (AI) abilities are machines mimicking human cognition, allowing them to learn, reason, perceive, solve problems, and understand language, enabling tasks like recognizing speech, making decisions, generating content, and automating complex processes, ultimately enhancing efficiency and providing insights by processing vast data. These capabilities go from simple pattern recognition to complex planning and creativity, making AI a tool for augmenting human intelligence across industries. "
    ]
  },
  {
    "term": "Artificial Intelligence Acceptance Measurement Survey",
    "definition": [
      "An AI Acceptance Measurement Survey uses frameworks like TAM (Technology Acceptance Model) or UTAUT (Unified Theory of Acceptance and Use of Technology) to quantify how users perceive and intend to use AI, assessing factors like usefulness, ease of use, trust, ethics, and social influence, ultimately helping developers understand adoption barriers and drivers for tools from chatbots to medical AI. These surveys use Likert scales (e.g., 1-5) to measure attitudes and predict behavioral intentions, crucial for successful AI integration. "
    ]
  },
  {
    "term": "UTAUT",
    "definition": [
      "UTAUT (Unified Theory of Acceptance and Use of Technology) isn't inherently \"artificial intelligence,\" but it's a widely used model to study how people accept and use AI and other technologies, identifying factors like Performance Expectancy (does it help me?), Effort Expectancy (is it easy?), Social Influence (do others use it?), and Facilitating Conditions (do I have the tools?) to predict user behavior and adoption intention. In AI contexts, researchers adapt UTAUT (often adding Hedonic Motivation, Perceived Risk) to understand adoption by students, librarians, or professionals, explaining why they do or don't embrace AI tools. "
    ]
  },
  {
    "term": "TAM",
    "definition": [
      "In AI, TAM usually refers to the Technology Acceptance Model, a theory explaining user adoption through Perceived Usefulness (belief it helps performance) and Perceived Ease of Use (belief it's effortless). Researchers extend TAM for AI (AI-TAM) to include factors like AI Trust, explainability (XAI), and ethical concerns, helping design AI systems that users find beneficial and easy to integrate into workflows, boosting adoption in areas like healthcare or sales. "
    ]
  },
  {
    "term": "Artificial Intelligence Adoption",
    "definition": [
      "Artificial Intelligence (AI) Adoption is the strategic process where organizations integrate AI technologies into their operations, systems, and decision-making to boost efficiency, innovation, and growth, moving from initial awareness to deeply embedded use in core processes like customer service (chatbots), data analysis (ML), and automation, requiring a holistic plan for tech, people, and governance. "
    ]
  },
  {
    "term": "Artificial Intelligence Agent",
    "definition": [
      "An AI Agent is a software system that perceives its environment, makes autonomous decisions, and takes actions to achieve specific goals, going beyond simple requests to manage complex workflows by planning, learning, and using tools like LLMs, APIs, and databases to solve problems with minimal human input, acting as digital workforce for tasks from customer service to coding. "
    ]
  },
  {
    "term": "Artificial Intelligence And Archives",
    "definition": [
      "Artificial Intelligence (AI) in Archives uses technologies like machine learning and NLP to manage, analyze, and make vast historical records more accessible by automating tasks like metadata creation, transcription, and contextual analysis, helping archivists handle digital and analog materials more efficiently, uncovering hidden connections, and improving searchability beyond simple keywords, though it requires human oversight to ensure accuracy and address ethical concerns. It transforms archives into data-driven institutions, allowing for scalable understanding of collections that were previously too large for manual processing. "
    ]
  },
  {
    "term": "Artificial Intelligence And Fake News",
    "definition": [
      "Artificial Intelligence (AI) both creates and combats fake news, enabling the rapid generation of realistic but false text, images (deepfakes), and videos, while also powering tools to detect and flag this fabricated content, leading to an arms race where AI helps spread misinformation at scale but also offers defense mechanisms, challenging journalism and public trust. AI-generated fake news sites are booming, using AI to create content with little human oversight, impacting finance, politics, and public opinion, while tech companies develop watermarking (like Google's SynthID for AI-generated content) and detection algorithms to fight back. "
    ]
  },
  {
    "term": "Artificial Intelligence Anxiety",
    "definition": [
      "Artificial Intelligence (AI) Anxiety is the apprehension, fear, or unease people feel about AI's rapid growth and integration, stemming from concerns about job displacement, loss of privacy, lack of control, ethical issues, and the potential for AI to negatively alter society or even pose an existential threat. It's a psychological response to technological uncertainty, encompassing worries about AI automation replacing jobs, AI systems making biased decisions, or the unknown future impact on daily life and human creativity. "
    ]
  },
  {
    "term": "Artificial Intelligence Capabilities",
    "definition": [
      "Artificial Intelligence (AI) capabilities are machines' power to mimic human intelligence, allowing them to learn, reason, solve problems, perceive, and make decisions using data, algorithms, and patterns, enabling tasks like understanding speech (Siri/Alexa), driving cars, personalized recommendations (Netflix), and complex data analysis, essentially performing cognitive functions without direct human intervention to boost efficiency and solve complex problems across industries. "
    ]
  },
  {
    "term": "Artificial Intelligence Chatbot",
    "definition": [
      "An AI chatbot is a software program using Artificial Intelligence, especially Natural Language Processing (NLP) and Machine Learning (ML), to understand, process, and respond to human language in a conversational way, simulating human interaction for tasks like customer service, sales, or information delivery without human intervention, evolving from basic rule-based bots to complex, learning systems. "
    ]
  },
  {
    "term": "Artificial Intelligence Chatbots Technology Acceptance Model",
    "definition": [
      "The Technology Acceptance Model (TAM) for AI Chatbots applies the classic TAM framework (Perceived Usefulness & Ease of Use) to understand why users accept or reject AI chatbots, focusing on factors like enhanced performance (usefulness) and effort-free interaction (ease of use), but adapted with new elements like trust, anthropomorphism, and perceived risk to capture unique aspects of AI, guiding design and adoption in fields from education to e-commerce. "
    ]
  },
  {
    "term": "Artificial Intelligence Computational Methods",
    "definition": [
      "Artificial Intelligence Computational Methods, often referred to as Computational Intelligence (CI), is a subfield of artificial intelligence that focuses on developing systems that can learn from data and adapt to new situations using nature-inspired computational paradigms, rather than relying solely on explicitly programmed rules and formal logic. \r\nThe primary goal of computational methods is to solve complex, real-world problems where traditional, exact mathematical modeling may fall short due to uncertainty, imprecision, or the need for continuous learning. "
    ]
  },
  {
    "term": "Artificial Intelligence Creativity",
    "definition": [
      "Artificial Intelligence (AI) Creativity is the ability of AI systems to generate novel, valuable, and appropriate outputs like art, music, or ideas, moving beyond mere data processing to simulate human creative processes, often by finding new combinations, exploring conceptual spaces, and generating surprising solutions through sophisticated algorithms, acting as powerful partners to augment human innovation. "
    ]
  },
  {
    "term": "Artificial Intelligence Ethics",
    "definition": [
      "Artificial Intelligence (AI) Ethics is the field of moral principles guiding AI's responsible creation and use, focusing on fairness, transparency, accountability, privacy, and avoiding societal harm, ensuring AI aligns with human values and promotes good by addressing issues like algorithmic bias, data misuse, and autonomous decision-making impacts. "
    ]
  },
  {
    "term": "Artificial Intelligence Generated Content",
    "definition": [
      "Artificial Intelligence Generated Content (AIGC) is any digital material—like text, images, music, or video—created autonomously by AI models, often using machine learning to understand patterns from vast datasets and generate novel outputs from user prompts, enabling efficient creation of diverse content from articles and code to art and music. "
    ]
  },
  {
    "term": "Artificial Intelligence Generated Content Tools",
    "definition": [
      "AI-Generated Content Tools are software using machine learning & NLP to create new text, images, audio, or code from simple user prompts, mimicking human creativity by learning patterns from massive datasets, enabling rapid content creation for blogs, marketing, designs, and more, saving time and boosting efficiency. "
    ]
  },
  {
    "term": "Artificial Intelligence Governance",
    "definition": [
      "Artificial Intelligence (AI) Governance is the system of rules, policies, standards, and processes for responsibly developing, deploying, and using AI, ensuring it's safe, ethical, transparent, and accountable, aligning with laws, societal values, and organizational goals while managing risks like bias, privacy breaches, and unintended consequences. It provides frameworks, like the ISO/IEC 42001 standard, to build trust, ensure compliance, and guide AI's beneficial integration into society. "
    ]
  },
  {
    "term": "Artificial Intelligence Hype",
    "definition": [
      "AI Hype refers to the exaggerated promises, inflated expectations, and sensationalized narratives (both utopian and dystopian) surrounding Artificial Intelligence, often outpacing its current practical capabilities, creating a classic tech hype cycle where initial excitement leads to disillusionment before finding realistic applications in areas like data analysis, predictive modeling, and process automation, rather than magic or human-level consciousness. "
    ]
  },
  {
    "term": "Artificial Intelligence Implementation",
    "definition": [
      "Artificial Intelligence (AI) Implementation is the strategic process of integrating AI technologies (like machine learning, deep learning, NLP) into business operations, systems, and workflows to automate tasks, analyze data, mimic human cognition, and drive efficiency, better decisions, and new capabilities. It's about moving AI from concept to real-world application, using algorithms to learn from data, recognize patterns, solve problems, and even generate content, transforming how organizations operate and compete. "
    ]
  },
  {
    "term": "Artificial Intelligence In African Libraries",
    "definition": [
      "Artificial Intelligence (AI) in African Libraries refers to the emerging use of AI technologies (like chatbots, machine learning, NLP) to transform traditional library services, improving user experience (24/7 support, personalized recommendations) and automating tasks (cataloging, data analysis), though adoption is still nascent, facing challenges like low awareness, funding, and strategy, but holding immense potential for efficiency and relevance in the digital age. "
    ]
  },
  {
    "term": "Artificial Intelligence In Education",
    "definition": [
      "Artificial Intelligence (AI) in Education (AIEd) uses smart technologies to create personalized learning, automate tasks, and offer data insights to improve teaching and student outcomes, essentially making machines act intelligently to support students and educators in ways like customized tutoring, instant feedback, and efficient administration. It adapts to individual student needs by adjusting content and pace, while freeing teachers from routine work to focus on deeper engagement. "
    ]
  },
  {
    "term": "Artificial Intelligence In Finance",
    "definition": [
      "Artificial Intelligence (AI) in finance uses technologies like machine learning (ML) and natural language processing (NLP) to analyze vast amounts of data, automate complex tasks, improve decision-making, and personalize customer experiences in banking, investing, and insurance, driving efficiency in areas from fraud detection and algorithmic trading to risk management and customer service. It mimics human intelligence to find patterns, make predictions, and handle processes faster and more accurately than traditional methods, transforming operations from back-end compliance to front-end client interactions. "
    ]
  },
  {
    "term": "Artificial Intelligence In Higher Education",
    "definition": [
      "Artificial Intelligence (AI) in Higher Education uses technologies like Generative AI and Machine Learning to personalize learning, automate tasks, enhance research, and streamline administration, offering adaptive tutoring, automated grading, and intelligent support for students and faculty, while also raising critical questions about ethics, data privacy, and academic integrity. It reshapes teaching, student support (e.g., chatbots, mental health check-ins), and institutional management, aiming for better outcomes and efficiency. "
    ]
  },
  {
    "term": "Artificial Intelligence In Libraries",
    "definition": [
      "Artificial Intelligence (AI) in libraries uses smart technologies to automate tasks, personalize user experiences, and improve information access, acting like an always-on assistant for tasks from cataloging to reference help, ultimately making libraries more efficient and relevant in the digital age by handling routine work and offering tailored recommendations, though requiring ethical consideration. AI tools help librarians with collection management, offer virtual reference, provide AI translation, and create personalized learning paths, transforming traditional services for better user engagement. "
    ]
  },
  {
    "term": "Artificial Intelligence In Translation",
    "definition": [
      "Artificial Intelligence (AI) in Translation uses deep learning and neural networks to automatically convert text or speech between languages, moving beyond simple word-for-word replacement by analyzing context, grammar, and sentence structure to produce more natural, human-like translations, making it faster and more accessible than traditional methods. These systems, often called Neural Machine Translation (NMT), learn from vast datasets, allowing them to understand language nuances, adapt to specific industries, and provide real-time translation for text, speech, and images. "
    ]
  },
  {
    "term": "Artificial Intelligence Language Platforms",
    "definition": [
      "AI Language Platforms are integrated systems and tools (like Google Cloud AI, AWS AI, Azure AI) providing the foundation (workbench) with technologies like Machine Learning (ML), Natural Language Processing (NLP), and MLOps to build, train, and deploy AI models that understand, generate, and process human language, powering applications from chatbots to virtual assistants, enabling machines to simulate human communication and tasks. "
    ]
  },
  {
    "term": "Artificial Intelligence Information",
    "definition": [
      "In Artificial Intelligence (AI), \"information\" refers to the data, patterns, and knowledge systems that machines use to learn, reason, perceive, and make decisions like humans, enabling tasks from recognizing images and understanding language to optimizing complex systems, essentially empowering computers to process and act on vast data to solve problems and achieve goals. AI systems learn from this information (data) to find patterns, allowing them to perform tasks without explicit programming, like a self-driving car learning traffic rules or a chatbot understanding user intent. "
    ]
  },
  {
    "term": "Artificial Intelligence Markup Language",
    "definition": [
      "Artificial Intelligence Markup Language (AIML) is an XML-based language for creating chatbots, defining human-like conversations by mapping user input patterns to specific responses using simple tags like <category>, <pattern>, and <template>. Developed by Richard Wallace for the A.L.I.C.E. bot, AIML provides a structured, rule-based way to build conversational agents, allowing them to understand queries and reply in a natural, engaging manner, acting as a foundational tool for early AI chatbots. "
    ]
  },
  {
    "term": "Artificial Intelligence Methodologies",
    "definition": [
      "Artificial Intelligence (AI) methodologies are the various algorithms, techniques, and approaches used to build systems that can simulate human cognitive functions like learning, reasoning, problem-solving, perception, and decision-making. Instead of explicit, step-by-step programming for every scenario, AI systems use data to learn and adapt over time. \r\nThe field of AI is broad and multidisciplinary, encompassing several key methodologies and subfields: "
    ]
  },
  {
    "term": "Artificial Intelligence Of Things",
    "definition": [
      "Artificial Intelligence of Things (AIoT) is the synergy of Artificial Intelligence (AI) with the Internet of Things (IoT), creating smart systems that go beyond simple data collection to analyze, learn, and make autonomous decisions, transforming connected devices into intelligent, self-acting entities that enhance efficiency and user experience, like a smart thermostat learning your schedule to adjust temperature. "
    ]
  },
  {
    "term": "Artificial Intelligence Personal Assistant",
    "definition": [
      "An AI Personal Assistant is smart software using Artificial Intelligence (AI) like Machine Learning & NLP to understand voice/text, automate tasks, learn your habits, and provide personalized help with scheduling, info, emails, smart home control, and more, making digital life easier and boosting productivity by mimicking human-like interaction and adapting over time. "
    ]
  },
  {
    "term": "Artificial Intelligence Platform",
    "definition": [
      "An AI Platform is a unified, integrated environment providing tools, infrastructure, and services for building, training, deploying, and managing AI/ML models and applications, covering the entire lifecycle from data prep to monitoring, streamlining development, enabling collaboration, and scaling solutions like chatbots, predictive analytics, and computer vision for businesses. "
    ]
  },
  {
    "term": "Artificial Intelligence Policy",
    "definition": [
      "Artificial Intelligence (AI) Policy is a framework of rules, guidelines, and regulations set by governments, organizations, or institutions to govern the responsible, safe, and ethical creation, deployment, and use of AI technologies, focusing on transparency, accountability, risk mitigation (like bias & security), and ensuring benefits while protecting human rights and complying with laws like GDPR. It bridges high-level principles with practical applications, guiding everything from data usage in healthcare to content creation in academia. "
    ]
  },
  {
    "term": "Artificial Intelligence Raters",
    "definition": [
      "\"Artificial Intelligence Raters\" (AI Raters) are human evaluators who assess the output and performance of artificial intelligence models. Their feedback is crucial for training and refining AI systems, ensuring the AI-generated content or decisions meet specific quality, accuracy, and ethical standards. "
    ]
  },
  {
    "term": "Artificial Intelligence Readiness",
    "definition": [
      "Artificial Intelligence (AI) Readiness is an organization's comprehensive state of preparedness to successfully implement, scale, and gain value from AI, assessing its strategic alignment, data quality, technological infrastructure, workforce skills, and ethical governance to ensure AI adoption drives real business outcomes rather than just installing tools. It's a holistic evaluation of the necessary elements—people, processes, technology, and culture—to move AI from experimentation to production effectively and responsibly. "
    ]
  },
  {
    "term": "Artificial Intelligence Regulation",
    "definition": [
      "Artificial Intelligence (AI) Regulation refers to the laws, policies, and guidelines set by governments and bodies to control AI's development and use, aiming to ensure safety, ethics, transparency, and accountability while balancing innovation, covering issues like data privacy, bias, and societal impact. Key examples include the EU's risk-based AI Act, the U.S. Executive Orders, and Brazil's AI framework, all addressing concerns about misuse, discrimination, and managing AI's rapid evolution. "
    ]
  },
  {
    "term": "Artificial Intelligence Security",
    "definition": [
      "AI Security (Artificial Intelligence Security) is a dual concept: protecting AI systems from threats (like data poisoning, model theft, adversarial attacks) and using AI to enhance overall cybersecurity through automated threat detection, response, and anomaly identification. It involves securing the entire AI lifecycle—data, models, pipelines, and applications—to ensure trustworthiness, while also applying AI's analytical power to find and stop traditional cyberattacks faster. "
    ]
  },
  {
    "term": "Artificial Intelligence Technology",
    "definition": [
      "Artificial Intelligence (AI) is the capability of computer systems to perform tasks requiring human intelligence, like learning, reasoning, problem-solving, and perception, by using algorithms and data to simulate cognitive functions. It enables machines to understand language, recognize patterns, make decisions, and adapt, powering everything from virtual assistants (Siri, Alexa) and recommendation engines (Netflix, YouTube) to self-driving cars and advanced medical diagnostics. "
    ]
  },
  {
    "term": "Artificial Intelligence Threats",
    "definition": [
      "AI threats involve risks like job displacement, privacy invasion, amplified bias, sophisticated cyberattacks (deepfakes, phishing), misinformation, autonomous weapons, and potential loss of control over advanced systems, impacting security, ethics, jobs, and societal stability by leveraging AI's power for malicious or unintended harmful outcomes. These threats range from immediate concerns, such as AI-powered scams, to existential worries about uncontrollable superintelligence. "
    ]
  },
  {
    "term": "Artificial Intelligence Tools",
    "definition": [
      "Artificial Intelligence (AI) Tools are software applications using AI algorithms to mimic human intelligence, automating tasks like understanding language, recognizing patterns, solving problems, and making decisions, with common examples including virtual assistants (Siri, Alexa), recommendation systems (Netflix), and generative AI for creating content (ChatGPT). These tools learn from vast data to perform complex jobs, enhancing efficiency and enabling new capabilities in areas from healthcare to customer service. "
    ]
  },
  {
    "term": "Artificial Intelligence Use",
    "definition": [
      "Artificial Intelligence (AI) use involves machines simulating human intelligence to learn, reason, solve problems, and perform tasks like understanding language, recognizing patterns, and making decisions, found in everything from self-driving cars and medical diagnosis tools to virtual assistants and personalized recommendations, automating processes and enhancing capabilities across industries. It works by training algorithms on vast data to find patterns, allowing systems to adapt, predict, and act autonomously, often leveraging Machine Learning and Deep Learning. "
    ]
  },
  {
    "term": "Artificial Internet Of Things",
    "definition": [
      "Artificial Intelligence of Things (AIoT) is the powerful fusion of Artificial Intelligence (AI) with the Internet of Things (IoT), creating smart systems where connected devices not only gather and transmit data but also analyze it, learn from it, and make autonomous decisions, essentially giving IoT devices a \"brain\" for smarter, more efficient operations, like traffic lights adjusting in real-time or smart ovens knowing when food is perfectly cooked. "
    ]
  },
  {
    "term": "Artificial Neural Network-Structural Equation Model",
    "definition": [
      "The Artificial Neural Network-Structural Equation Model (SEM-ANN) hybrid approach is a research methodology that combines the complementary strengths of traditional statistical modeling (Structural Equation Modeling) and artificial intelligence (Artificial Neural Networks). It is primarily used in social sciences, marketing, and business research to gain deeper insights into complex relationships between variables. "
    ]
  },
  {
    "term": "Artificial Neural Networks",
    "definition": [
      "Artificial Neural Networks (ANNs) are AI systems modeled after the human brain, using interconnected \"neurons\" in layers to learn complex patterns from data, enabling tasks like image recognition, natural language processing, and predictions by finding non-linear relationships, making them a core part of deep learning and modern AI. "
    ]
  },
  {
    "term": "Artificial Sophistry",
    "definition": [
      "\"Artificial sophistry\" is not a formal term in artificial intelligence (AI) but a philosophical concept used to describe the use of AI systems, particularly large language models (LLMs), to produce persuasive yet fallacious arguments that lack a genuine concern for truth or underlying logical validity. \r\nThe concept draws a parallel between modern AI and the ancient Greek Sophists, who were known for using clever, persuasive rhetoric to win arguments, often without regard for objective truth, in contrast to philosophers like Socrates and Plato who sought objective truths. "
    ]
  },
  {
    "term": "Artificial Training Datasets Generation",
    "definition": [
      "Artificial Training Datasets Generation, or more commonly synthetic data generation, is the process of creating artificial information that computationally or algorithmically mimics the statistical properties and patterns of real-world data, without being collected from real-world events. \r\nThis process addresses key challenges in artificial intelligence (AI) development, such as data scarcity, privacy concerns, cost, and the need for diverse training examples. "
    ]
  },
  {
    "term": "Aspect-Based Sentiment Analysis",
    "definition": [
      "Aspect-Based Sentiment Analysis (ABSA) is an advanced AI technique in Natural Language Processing (NLP) that goes beyond general sentiment to pinpoint opinions about specific aspects or features (like \"screen,\" \"battery life,\" \"service\") within text, revealing nuanced feedback, such as a reviewer loving the \"camera\" but hating the \"price\" of a phone, offering deeper insights than overall positive/negative/neutral labels for understanding customer feedback, product reviews, and market trends. "
    ]
  },
  {
    "term": "Aspect-Level Sentiment Classification",
    "definition": [
      "Aspect-Level Sentiment Classification (ALSC) in AI is a detailed form of sentiment analysis that identifies specific aspects (features, topics) within a text and determines the sentiment (positive, negative, neutral) towards each aspect, going beyond overall text sentiment to find nuances like \"good battery life, bad screen\" in a single review. It uses NLP and deep learning, employing techniques like attention mechanisms to link opinion words to relevant aspects, allowing for granular understanding of opinions on specific parts of a product or service. "
    ]
  },
  {
    "term": "Assisted Reasoning",
    "definition": [
      "Assisted Reasoning refers to AI systems designed to collaborate with and augment human cognitive processes, rather than operate fully autonomously. The AI acts as an intelligent partner, using logical inference, data analysis, and problem-solving techniques to provide insights, suggestions, and step-by-step explanations that help human users make more informed decisions. "
    ]
  },
  {
    "term": "Attention Flow",
    "definition": [
      "In artificial intelligence, attention flow is a post-processing method for interpreting how information propagates through the layers of a transformer model, typically for explainability purposes. It tracks the contribution of input elements (like words in a sentence) to the final output. "
    ]
  },
  {
    "term": "Attention Networks",
    "definition": [
      "Attention Networks in AI use a mechanism that allows neural networks to dynamically focus on the most relevant parts of input data, mimicking human selective attention, by assigning importance scores (weights) to different input elements, which significantly boosts performance in tasks like translation, image captioning, and language understanding by handling long-range dependencies and improving context, most notably powering models like the Transformer. "
    ]
  },
  {
    "term": "Attentional Bias",
    "definition": [
      "In artificial intelligence (AI), attentional bias refers to the systematic and disproportionate focus of an AI model's attention mechanism on specific input features while undervaluing or ignoring others. This leads to a systematic error in processing and decision-making, often reflecting and amplifying human or societal biases present in the training data. "
    ]
  },
  {
    "term": "Attitudes Toward Artificial Intelligence",
    "definition": [
      "Attitudes toward AI are complex and mixed, generally positive about potential benefits in health, education, and science, but mixed with significant concerns about job loss, creativity, ethics, and trustworthiness, often varying by application and shaped by personal traits like openness and fear, with many people feeling ambivalent or cautious rather than purely accepting or rejecting it. People are often more accepting of AI in supportive roles (like learning) but skeptical in high-stakes areas (like medical diagnosis), and attitudes can shift with more knowledge and positive experiences. "
    ]
  },
  {
    "term": "Attitudes Toward Communicative AI",
    "definition": [
      "\"Attitudes Toward Communicative AI\" refers to the comprehensive evaluations, beliefs, and emotions people hold regarding artificial intelligence applications designed for communication, such as chatbots, virtual assistants, and generative text/image tools. These attitudes are multifaceted and range from optimistic to skeptical, with significant variations depending on the specific application, context, and individual characteristics. "
    ]
  },
  {
    "term": "Attribute Compatibility-Aware Attention",
    "definition": [
      "Attribute Compatibility-Aware Attention (ACAA) is an artificial intelligence mechanism used to evaluate and visualize the harmonious interaction between different attributes of items, especially in applications like fashion recommendation and editing. "
    ]
  },
  {
    "term": "Augmentation and Replacement",
    "definition": [
      "In AI, Augmentation enhances human capabilities by working alongside people (like a copilot) to boost productivity, creativity, and decision-making, while Replacement (or Automation) involves AI fully taking over tasks, removing humans from the process to handle repetitive, rule-based jobs. Augmentation focuses on collaboration, using AI for data analysis and insights, whereas replacement aims for full autonomy, like in self-driving cars or automated invoice processing, shifting roles from doing to guiding and interpreting. "
    ]
  },
  {
    "term": "Augmentation Effect",
    "definition": [
      "The Augmentation Effect in artificial intelligence (AI), more commonly referred to as AI augmentation or augmented intelligence, describes the use of AI technologies to enhance human capabilities and decision-making rather than to replace human workers entirely. \r\nThis approach emphasizes a collaborative partnership between humans and machines, leveraging the unique strengths of each: AI excels at speed, scale, data processing, and pattern recognition, while humans contribute intuition, creativity, contextual understanding, and ethical judgment. "
    ]
  },
  {
    "term": "Sequence Augmentation",
    "definition": [
      "Sequence augmentation in artificial intelligence is the process of artificially generating new, modified sequences (e.g., text, time-series, or user behavior histories) from existing data to increase the size and diversity of training datasets. \r\nThis technique is a critical form of data augmentation used to improve the performance, robustness, and generalization capabilities of machine learning models, especially when the original labeled data is limited, imbalanced, or noisy. "
    ]
  },
  {
    "term": "Autoencoder",
    "definition": [
      "An Autoencoder is an unsupervised neural network that learns efficient data representations by compressing input into a smaller \"latent space\" (encoding) and then reconstructing it, aiming to mirror the original data. It consists of an encoder (compresses data) and a decoder (reconstructs data), learning the most crucial features to minimize reconstruction error, making it ideal for dimensionality reduction, anomaly detection, denoising, and feature extraction in AI. "
    ]
  },
  {
    "term": "Automated Content Analysis",
    "definition": [
      "Automated Content Analysis (ACA) using AI is the process of using machine learning and algorithms to systematically analyze vast amounts of digital content (text, images, video, audio) to find patterns, themes, and insights far faster and more consistently than humans can, enabling real-time understanding, classification, and decision-making for research, marketing, security, and more. It transforms unstructured data into structured information by identifying context, sentiment, relevance, or security risks automatically, often relying on techniques like keyword extraction, topic modeling, and image recognition. "
    ]
  },
  {
    "term": "Automated Content Production",
    "definition": [
      "Automated Content Production (ACP) is the use of artificial intelligence (AI), specifically machine learning (ML), natural language processing (NLP), and natural language generation (NLG), to streamline and generate various types of digital content with minimal human intervention. This technology assists in the creation of text, images, audio, and video content at scale. "
    ]
  },
  {
    "term": "Automated Decision-Making",
    "definition": [
      "Automated Decision-Making (ADM) is the use of data, machines, and algorithms, often powered by artificial intelligence (AI), to make decisions with varying degrees of human oversight or even no human intervention at all. "
    ]
  },
  {
    "term": "Automated Deduction",
    "definition": [
      "Automated Deduction (AD) in AI is the science of using computers to apply formal logic to prove theorems, solve problems, and derive new conclusions from existing knowledge, essentially automating reasoning by proving mathematical statements or verifying system correctness. It's a core AI field that uses algorithms to explore logical pathways, finding new insights in mathematics, ensuring software/hardware reliability, and supporting expert systems, often called Automated Theorem Proving (ATP). "
    ]
  },
  {
    "term": "Automated Fact-Checking",
    "definition": [
      "Automated Fact-Checking (AFC) uses Artificial Intelligence (AI) and Machine Learning (ML) to rapidly analyze vast amounts of online content, extract specific claims, and verify their accuracy by comparing them against trusted databases and reliable sources, significantly speeding up the fight against misinformation by flagging potentially false statements for human review or providing immediate context. These systems, leveraging Natural Language Processing (NLP), can identify patterns, cross-reference data, and even generate explanations, but still require human oversight for nuance, context, and complex disinformation. "
    ]
  },
  {
    "term": "Automated Hermeneutical Appropriation",
    "definition": [
      "Automated Hermeneutical Appropriation refers to a specific ethical concern in artificial intelligence (AI) where a machine learning (ML) system establishes or controls the shared meanings and interpretative frameworks (hermeneutical resources) without adequate human oversight. This can impair human understanding and communication, leading to a form of hermeneutical injustice. "
    ]
  },
  {
    "term": "Automated Narratives",
    "definition": [
      "Automated narratives in artificial intelligence refer to systems that use AI, particularly natural language generation (NLG) and large language models (LLMs), to automatically generate human-readable narratives or stories from structured data or minimal user input. This technology can be broadly divided into data-driven summarization and fictional story generation. "
    ]
  },
  {
    "term": "Automated Subject Indexing",
    "definition": [
      "Automated Subject Indexing (ASI) using AI is the computer-driven process of analyzing documents (text, metadata) to automatically assign relevant subject terms or descriptors from controlled vocabularies (like thesauri), mimicking human indexing but at scale, using machine learning, NLP, and statistical methods to improve discoverability and manage huge digital collections efficiently, often as a multi-label classification task. "
    ]
  },
  {
    "term": "Autonomous Systems",
    "definition": [
      "Autonomous Systems (AI) are intelligent machines or software that can perceive their environment, make decisions, and take actions independently, without constant human control, using AI, machine learning, and sensors to learn, adapt, and complete complex, multi-step tasks in the physical or digital world, like self-driving cars or AI agents. They function as self-managing entities, moving beyond simple automation to understand context and achieve goals with minimal human oversight. "
    ]
  },
  {
    "term": "Automated Text Summarization",
    "definition": [
      "Automated Text Summarization (ATS) is an AI-powered Natural Language Processing (NLP) technique that computationally condenses long documents into shorter, coherent summaries, extracting or generating the most important information to help users quickly grasp key insights and manage information overload. It uses machine learning and deep learning models to analyze text and either select key sentences (extractive) or create new ones (abstractive), tackling the challenge of processing massive amounts of online text efficiently. "
    ]
  },
  {
    "term": "Automatic Classification",
    "definition": [
      "Automatic classification (also known as auto-classification or automated classification) is an Artificial Intelligence (AI) process that uses machine learning algorithms to sort and categorize data, such as documents, emails, or images, into predefined classes or groups automatically, without manual human intervention. "
    ]
  },
  {
    "term": "Automatic Classification Number Assignment",
    "definition": [
      "Automatic Classification Number Assignment is an artificial intelligence (AI) process that uses machine learning to automatically sort, organize, and label data (such as documents, images, or product information) into predefined categories without manual human intervention. "
    ]
  },
  {
    "term": "Automatic Construction Of Knowledge Graph",
    "definition": [
      "Automatic Construction of Knowledge Graphs (AKG) uses AI, especially Large Language Models (LLMs) and Graph Neural Networks (GNNs), to transform vast amounts of unstructured data (text, documents) into structured, interconnected knowledge graphs (KGs) by automatically extracting entities, relations, and concepts, streamlining processes that traditionally took experts months or years. This involves stages like entity/relation extraction, schema induction (generalizing concepts), and knowledge refinement, allowing KGs to grow dynamically, infer new knowledge, and support applications like material design or regulatory analysis. "
    ]
  },
  {
    "term": "Automatic Detection",
    "definition": [
      "Automatic detection in artificial intelligence (AI) is the use of AI and machine learning (ML) algorithms to autonomously identify and classify specific objects, patterns, or anomalies within data, often in real-time and with minimal human intervention. This differs from traditional detection methods by being dynamic, scalable, and continuously learning and adapting to new data and patterns. "
    ]
  },
  {
    "term": "Automatic Indexing",
    "definition": [
      "AI Automatic Indexing uses machine learning to analyze content, automatically tag, categorize, and organize vast amounts of digital data (documents, images, etc.), creating searchable maps (indices) for faster retrieval, moving beyond simple keywords to understand context and themes, making information management more efficient and intelligent. "
    ]
  },
  {
    "term": "Automatic Keyphrase Extraction",
    "definition": [
      "Automatic Keyphrase Extraction (AKE) uses AI and Natural Language Processing (NLP) to automatically identify and pull out the most important words and phrases (keyphrases) from a document that represent its main topics, essentially creating a concise summary for tasks like search, classification, and indexing. It works by finding candidate phrases and ranking them using statistical (like TF-IDF), graph-based (like TextRank), or modern neural network methods (like BERT) to determine their significance, moving from manual feature engineering to learned representations. "
    ]
  },
  {
    "term": "Automatic Keyphrase Generation",
    "definition": [
      "Automatic Keyphrase Generation (AKG) is a task within artificial intelligence (AI) and natural language processing (NLP) that involves automatically creating a set of short, meaningful phrases that accurately summarize the main topics or themes of a given document. "
    ]
  },
  {
    "term": "Automatic Questions Generation",
    "definition": [
      "Automatic Question Generation (AQG) using AI creates questions from text or images, automating assessment creation for education, chatbots, and research by using Natural Language Processing (NLP) to understand content and generate varied, contextually relevant questions (like multiple-choice, True/False, fill-in-the-blanks) quickly, saving time and enhancing learning. "
    ]
  },
  {
    "term": "Automatic Segmentation",
    "definition": [
      "Automatic segmentation in AI is using algorithms, especially deep learning (like CNNs), to automatically identify, outline, and label regions of interest (ROIs) in images (like tumors or organs in medical scans) without manual drawing, making tasks faster, more consistent, and objective than manual methods, crucial for fields like radiology, pathology, and customer analytics. "
    ]
  },
  {
    "term": "Automatic Speech Recognition",
    "definition": [
      "Automatic Speech Recognition (ASR) is an artificial intelligence technology that converts spoken human language into written text or commands, using machine learning (ML) and Natural Language Processing (NLP) to understand audio signals, recognize words, and generate accurate transcripts for applications like voice assistants (Siri, Alexa), closed captions, and transcription services. It allows for natural human-computer interaction by processing complex audio and context to enable voice-first experiences across devices and industries. "
    ]
  },
  {
    "term": "Automatic Text Simplification",
    "definition": [
      "Automatic Text Simplification (ATS) is a field within artificial intelligence (AI) and natural language processing (NLP) that uses computational techniques to automatically rewrite complex text into a simpler, more accessible version while preserving the original meaning and core information. "
    ]
  },
  {
    "term": "Automation Artificial Intelligence",
    "definition": [
      "AI Automation is the use of Artificial Intelligence (like machine learning, NLP, computer vision) to automate tasks that typically need human thinking, learning, and decision-making, going beyond simple rules to handle complex data, learn from patterns, and improve processes for greater efficiency, accuracy, and scale. It combines AI's intelligence with automation's execution to streamline workflows, freeing humans for higher-value work. "
    ]
  },
  {
    "term": "Automation Bias",
    "definition": [
      "Automation bias, in the context of AI, is the human tendency to over-rely on automated systems, accepting their outputs as correct even when contradictory evidence suggests otherwise, leading to reduced critical thinking, errors, and potential failures, such as GPS leading drivers into water or ignoring critical alarms in a pipeline control room. This bias stems from perceiving technology as infallible, and it's a major concern as AI becomes more integrated into critical fields like healthcare, finance, and aviation. "
    ]
  },
  {
    "term": "Automation Tools",
    "definition": [
      "AI automation tools use artificial intelligence (like machine learning, NLP) to automate tasks that need human-like thinking, adapting beyond fixed rules to handle complex, changing data, thereby streamlining processes, improving decisions, and boosting efficiency in areas from customer service to data analysis. They power smarter workflows, allowing systems to understand context, learn from patterns, and act independently, unlike traditional automation. "
    ]
  },
  {
    "term": "AutoML",
    "definition": [
      "AutoML (Automated Machine Learning) is an AI-driven process that automates the complex, time-consuming steps of building and deploying machine learning models, making AI accessible to non-experts by handling tasks like data preprocessing, feature engineering, model selection, and hyperparameter tuning, allowing users to create high-quality models faster and more efficiently without deep ML expertise. It democratizes AI development, boosts productivity, and often finds better-performing models than manual methods by systematically testing many combinations, streamlining workflows for data scientists and domain experts alike. "
    ]
  },
  {
    "term": "Autonomous Agency",
    "definition": [
      "In AI, Autonomous Agency refers to an AI system's ability to perceive its environment, reason, set its own sub-goals, and take independent actions to achieve a high-level objective, without continuous human prompting, learning and adapting as it goes. Unlike basic AI that follows fixed rules, an autonomous agent plans multi-step tasks, manages resources, and interacts with other systems to complete complex work, making it a significant leap in AI automation for dynamic real-world applications. "
    ]
  },
  {
    "term": "Autonomous Multi-Agent System",
    "definition": [
      "An Autonomous Multi-Agent System (AMAS) in AI is a network of independent, self-directed AI \"agents\" that communicate and collaborate in a shared environment to solve complex problems too difficult for a single AI, using decentralized decision-making, planning, and tool-use to achieve collective goals, like coordinating drones or optimizing a smart grid. These agents, often powered by Large Language Models (LLMs), act like a dynamic, intelligent workforce, adapting and evolving without constant human oversight, enabling scalability and robust problem-solving. "
    ]
  },
  {
    "term": "Autonomous Shopping Systems",
    "definition": [
      "Autonomous Shopping Systems (ASS) use AI, computer vision, and sensor fusion to automate the entire retail journey, letting customers \"grab-and-go\" from checkout-free stores (like Amazon Go) or use AI agents for online tasks, eliminating human intervention for purchases, payments, and even reordering staples, creating a frictionless, convenience-focused experience. "
    ]
  },
  {
    "term": "Autonomous Technology",
    "definition": [
      "Autonomous Technology (AI) refers to systems that use artificial intelligence to perceive, reason, decide, and act independently to achieve goals, requiring minimal human intervention by learning from data and adapting to environments, seen in self-driving cars, robots, and AI agents. It's about systems performing complex, multi-step tasks and workflows end-to-end, not just narrow functions."
    ]
  },
  {
    "term": "Back Propagation Neural Network",
    "definition": [
      "A Backpropagation Neural Network uses the \"Backpropagation\" algorithm, a core AI training method where errors from predictions are sent backward through the network to adjust internal \"weights\" (connection strengths) using calculus (chain rule), allowing the network to learn from mistakes, minimize errors, and improve accuracy over time for tasks like image recognition and language understanding. It's how deep learning models \"learn\" efficiently. "
    ]
  },
  {
    "term": "Backdoor Watermark",
    "definition": [
      "A backdoor watermark is a technique used to embed a hidden, verifiable signature within an artificial intelligence (AI) model, primarily to protect its intellectual property and prove ownership. This method re-purposes the mechanism of a backdoor attack for a beneficial application. "
    ]
  },
  {
    "term": "BARD",
    "definition": [
      "Google Bard was a conversational AI chatbot, similar to ChatGPT, that used Google's powerful large language models (like LaMDA and PaLM 2) to generate human-like text, answer questions, create content, summarize info, and brainstorm ideas, leveraging real-time internet access for up-to-date info; it was designed as a creative collaborator but evolved into the more advanced Google Gemini. "
    ]
  },
  {
    "term": "Barriers To Adopt AI",
    "definition": [
      "Barriers to AI adoption are multi-faceted, centering on data issues (quality, bias, privacy), talent gaps (lack of expertise), organizational resistance (fear, unclear strategy, integration with legacy systems), high costs, and ethical/regulatory concerns (security, compliance, fairness). Key challenges include proving ROI, managing complex data, overcoming employee fear of job loss, and navigating unclear legal landscapes, all hindering large-scale, successful integration. "
    ]
  },
  {
    "term": "Basic Graph Pattern",
    "definition": [
      "A Basic Graph Pattern (BGP) is a set of connected nodes (entities) and edges (relationships) that forms a specific substructure within a larger graph. In artificial intelligence (AI), these patterns are used to represent complex relationships and structures within data, enabling more sophisticated analysis and reasoning. "
    ]
  },
  {
    "term": "Bayesian Classifier Combination",
    "definition": [
      "Bayesian Classifier Combination (BCC) is a framework in artificial intelligence and machine learning for aggregating the outputs of multiple individual classifiers (which can even be human experts) to produce a single, more accurate prediction. \r\nUnlike simpler methods like majority voting, BCC uses a principled, probabilistic approach based on Bayes' theorem to infer the true class label, while simultaneously accounting for the varying performance and potential correlations between the individual classifiers. "
    ]
  },
  {
    "term": "Bayesian Inference",
    "definition": [
      "Bayesian Inference in AI is a powerful statistical method using Bayes' Theorem to update beliefs (probabilities) about a hypothesis as new data arrives, blending prior knowledge with observed evidence to make smarter, uncertainty-aware predictions. It's crucial for AI systems needing to reason under uncertainty, handling complex models, limited data, and enabling principled decisions in areas like virtual assistants, spam filters, and predictive analytics. "
    ]
  },
  {
    "term": "Bayesian Networks",
    "definition": [
      "Bayesian Networks (BNs) in AI are probabilistic graphical models using a Directed Acyclic Graph (DAG) to map variables and their conditional dependencies, allowing AI to reason under uncertainty by updating beliefs based on evidence, like a detective using clues (symptoms) to find causes (diseases) using Bayes' theorem. They offer transparent, interpretable models for complex systems, predicting outcomes and diagnosing problems in areas like medicine, finance, and image processing, by representing knowledge efficiently and handling missing data. "
    ]
  },
  {
    "term": "Bayesian Statistics",
    "definition": [
      "Bayesian Statistics in AI is a powerful framework using Bayes' Theorem to update beliefs (probabilities) as new data arrives, allowing AI models to quantify and manage uncertainty by combining prior knowledge with observed evidence to form better, more confident predictions. It treats parameters as probability distributions, offering deeper insights than traditional methods, crucial for high-stakes fields like healthcare, finance, and autonomous systems where understanding \"how sure\" a model is matters. "
    ]
  },
  {
    "term": "Behavior Planning",
    "definition": [
      "Behavior Planning in artificial intelligence (AI) is the process by which an autonomous agent determines a sequence of actions, or a strategy, to move from its current state to a desired goal state in an intelligent and adaptive manner. It enables agents to act purposefully and autonomously in complex or dynamic environments. "
    ]
  },
  {
    "term": "Behavioral Intention",
    "definition": [
      "In AI, Behavioral Intention (BI) refers to a person's conscious plan or likelihood to adopt, use, or learn an AI technology, studied using models like the Theory of Planned Behavior (TPB) and UTAUT to find factors such as usefulness, ease of use, social influence, and self-efficacy that drive willingness to engage with AI tools**. Essentially, it's understanding why someone decides to use or resist AI, from chatbots to educational software, by analyzing their attitudes, perceived control, and societal pressures. "
    ]
  },
  {
    "term": "Behavioral Reasoning Theory",
    "definition": [
      "Behavioral Reasoning Theory (BRT) in the context of Artificial Intelligence (AI) is a framework used to explain and predict user adoption and behavior toward AI technologies. It posits that individuals' decisions are heavily influenced by the specific \"reasons for\" and \"reasons against\" using a particular AI system, which serve to justify their actions and shape their intentions."
    ]
  },
  {
    "term": "Behavioral-Responses",
    "definition": [
      "In artificial intelligence (AI), behavioral responses refer to the actions, adaptations, and interactions exhibited by AI systems in response to various stimuli, contexts, and feedback, often mimicking human behavior. This concept is central to \"Behavioral AI\" and the emerging field of \"AI Agent Behavioral Science\". "
    ]
  },
  {
    "term": "Benchmark Dataset",
    "definition": [
      "A Benchmark Dataset in AI is a standardized, widely-accepted collection of data used to fairly evaluate and compare the performance of different machine learning models or algorithms on specific tasks, acting as a common ruler for measuring progress in areas like image recognition or language understanding. These curated datasets (like ImageNet or MNIST) ensure consistency, allowing researchers to objectively see if one model truly outperforms another, rather than just performing better on different data. "
    ]
  },
  {
    "term": "BERT",
    "definition": [
      "BERT (Bidirectional Encoder Representations from Transformers) is a revolutionary AI language model from Google that understands text context by reading words in both directions simultaneously, leading to much better comprehension for tasks like search, question answering, and sentiment analysis, by using a Transformer architecture and a two-step training process (pre-training on massive text, then fine-tuning). "
    ]
  },
  {
    "term": "BERTopic",
    "definition": [
      "BERTopic is an advanced topic modeling technique in artificial intelligence (AI) that uses modern natural language processing (NLP) methods, specifically transformer-based embeddings (like BERT), to analyze and organize large text datasets into coherent topics. \r\nUnlike traditional methods such as Latent Dirichlet Allocation (LDA) that rely solely on word frequency, BERTopic leverages semantic meaning to produce more accurate and human-interpretable results. "
    ]
  },
  {
    "term": "Bi-Directional Recurrent Neural Network",
    "definition": [
      "A Bidirectional Recurrent Neural Network (BRNN) is an extension of traditional recurrent neural networks (RNNs) that processes sequential data in both forward and backward directions to provide a more comprehensive understanding of the context. This architecture allows the network to utilize information from both the past and the future of a given time step when making predictions. "
    ]
  },
  {
    "term": "Bi-Directional Spatiotemporal-Aware",
    "definition": [
      "\"Bi-directional spatiotemporal-aware\" is a term used in artificial intelligence (AI) to describe models that analyze data by considering both spatial (location) and temporal (time) relationships in both forward and backward directions. This comprehensive approach allows the AI to develop a more robust understanding of complex data, particularly in fields like video analysis, traffic prediction, and location-based services. "
    ]
  },
  {
    "term": "Bi-Hypergraph Network",
    "definition": [
      "A Bi-Hypergraph Network is a type of artificial intelligence (AI) model that uses two separate but interconnected hypergraphs to represent and analyze data from different perspectives or modalities. This approach allows the network to model complex, higher-order relationships that go beyond simple pairwise connections. "
    ]
  },
  {
    "term": "Bi-Lstm",
    "definition": [
      "A Bidirectional Long Short-Term Memory (BiLSTM) network is an advanced type of recurrent neural network (RNN) in artificial intelligence that processes sequential data in both forward and backward directions. This dual processing allows it to capture a richer and more comprehensive context from both past and future information within a sequence. "
    ]
  },
  {
    "term": "Bias Detection",
    "definition": [
      "Bias detection in AI is the process of identifying systematic errors or unfair patterns in AI systems (data, algorithms, models) that favor or disadvantage certain groups, often based on race, gender, age, etc., ensuring fairness, accuracy, and compliance with ethical standards, crucial for preventing real-world harm in areas like hiring, lending, and healthcare. It involves auditing data, testing model outputs with fairness metrics, and using specialized tools to spot discriminatory tendencies before deployment, as biased AI reflects and amplifies societal prejudices from its training data. "
    ]
  },
  {
    "term": "Bias Mitigation",
    "definition": [
      "Bias mitigation in AI is the process of identifying, reducing, and preventing unfair prejudices in AI systems that lead to discriminatory outcomes, ensuring fairness, inclusivity, and accuracy by applying techniques across the AI lifecycle—from cleaning biased training data (like ensuring diverse faces in facial recognition data) to using fairness-aware algorithms and continuous monitoring to treat groups equitably. "
    ]
  },
  {
    "term": "Bidimensional Networks",
    "definition": [
      "The term \"Bidimensional Networks\" (AI) most commonly refers to the use of two-dimensional (2D) structures or materials in creating neural network hardware, or specific network architectures designed to process 2D data like images or spatial distributions. It is not a single, distinct type of general AI network architecture, but rather an umbrella term appearing in specific research contexts. "
    ]
  },
  {
    "term": "Big Data",
    "definition": [
      "Big Data refers to massive, complex datasets (the \"fuel\"), while Artificial Intelligence (AI) involves technologies that analyze this data to find patterns, learn, and make decisions (the \"engine\"), with AI using Big Data to power smarter, automated insights and predictions for things like personalized recommendations, fraud detection, and self-driving cars, creating a powerful synergy where more data leads to better AI and more sophisticated analysis. "
    ]
  },
  {
    "term": "Big Data Algorithm System",
    "definition": [
      "A Big Data Algorithm System (AI) combines massive datasets (Big Data) with Artificial Intelligence to find patterns, make predictions, and automate decisions, essentially using AI algorithms to learn from and extract value from enormous amounts of information that humans can't process efficiently. Big Data provides the fuel (vast data), while AI algorithms provide the engine (learning and analysis), enabling intelligent systems for things like personalized recommendations, fraud detection, and medical diagnostics. "
    ]
  },
  {
    "term": "Big Data Analytics",
    "definition": [
      "Big Data Analytics (BDA) with Artificial Intelligence (AI) is the use of AI, especially Machine Learning (ML), to automatically find patterns, make predictions, and gain insights from massive, complex datasets (Big Data) faster and more accurately than traditional methods. AI acts as the engine, processing vast amounts of structured, unstructured (images, text), and semi-structured data to automate analysis, discover hidden correlations, and enable smarter, real-time decisions across descriptive, predictive, and prescriptive analytics. "
    ]
  },
  {
    "term": "Big Data Analytics Capability",
    "definition": [
      "Big Data Analytics Capability (AI) is the use of Artificial Intelligence and Machine Learning to automatically process, analyze, and find hidden patterns in massive, complex datasets (Big Data) to generate faster, deeper, and more actionable insights for better decision-making, automation, and strategic advantage than traditional methods could achieve. It essentially supercharges data analysis by using AI to handle volume, velocity, and variety (the 3 Vs of big data) to predict trends, prescribe actions, and automate complex tasks like data prep and visualization."
    ]
  },
  {
    "term": "Big Data Capabilities",
    "definition": [
      "Big Data Capabilities with AI involve using artificial intelligence and machine learning to analyze massive datasets, uncovering hidden patterns, making predictions, and automating complex decisions far beyond human capacity, transforming raw data into actionable insights for better business outcomes in healthcare, finance, retail, and more, essentially making AI the \"brain\" that learns from the \"fuel\" (Big Data). "
    ]
  },
  {
    "term": "Big Data Privacy",
    "definition": [
      "Big Data Privacy (AI) is about ethically managing massive datasets used by AI, balancing innovation with protecting sensitive personal info from misuse, unauthorized access, or bias, involving controls like consent, transparency, security, and advanced privacy techniques (e.g., federated learning) to prevent risks from AI's deep data analysis. It tackles challenges from data collection (ingestion), training, and prediction (inference) stages, ensuring data is handled responsibly while AI gains insights. "
    ]
  },
  {
    "term": "Big Data Research",
    "definition": [
      "Big Data Research in AI involves using massive, complex datasets (Volume, Velocity, Variety) as fuel for AI and Machine Learning (ML) models to find hidden patterns, make predictions, and automate intelligent decisions, essentially teaching machines to \"think\" and learn from the world's data to solve complex problems, from healthcare diagnostics to fraud detection. It's the synergy where Big Data provides the insights, and AI provides the analytical power to transform raw information into actionable intelligence. "
    ]
  },
  {
    "term": "Big Data Technology",
    "definition": [
      "Big Data Technology in AI refers to using massive, complex datasets (Big Data) as fuel to train Artificial Intelligence systems, enabling them to learn, find patterns, make predictions, and automate tasks that mimic human intelligence, essentially making AI smarter and more capable with more, richer information. AI analyzes Big Data (volume, velocity, variety from sources like social media, IoT, transactions) to deliver insights, personalization, and efficiency, while Big Data provides the scale AI needs to evolve beyond basic functions. "
    ]
  },
  {
    "term": "Biometric Categorisation",
    "definition": [
      "Biometric categorization using AI is an system that sorts people into groups based on their unique physical or behavioral traits (like face, fingerprint, gait, voice) to infer characteristics such as age, sex, race, or more sensitive attributes like political views, sexual orientation, or personality, with the EU AI Act heavily restricting or banning uses that infer protected characteristics due to significant privacy and rights risks, though some limited, non-sensitive applications (e.g., virtual try-ons) may be allowed. "
    ]
  },
  {
    "term": "Blending-Target Domain Adaptation",
    "definition": [
      "Blending-Target Domain Adaptation (BTDA) is a challenging sub-field of artificial intelligence domain adaptation where a model trained on a source domain must adapt to a target domain that is an unlabeled mixture of multiple distinct sub-target domains. "
    ]
  },
  {
    "term": "Blockchain",
    "definition": [
      "Blockchain isn't a single technology but the powerful combination of Blockchain's secure, decentralized ledger with AI's intelligent analysis, creating transparent, trustworthy systems for tasks like supply chain tracking (authenticity), smart grids (optimizing energy), and secure data management, where blockchain ensures data integrity and AI finds patterns and automates decisions, solving issues of trust and speed in complex digital environments. Essentially, AI provides the \"thinking\" and blockchain the secure \"remembering\" for next-gen applications. "
    ]
  },
  {
    "term": "Body Detection",
    "definition": [
      "Body detection is a field of computer vision that uses AI and machine learning algorithms to identify, locate, track, and analyze human beings in images, videos, or real-time sensor data. \r\nUnlike basic motion detection, which simply reacts to any movement, AI-powered body detection is trained on vast datasets to specifically recognize human shapes, movements, and behaviors, significantly reducing false alarms and providing deeper analysis capabilities. "
    ]
  },
  {
    "term": "Book Rating Prediction",
    "definition": [
      "Book Rating Prediction in artificial intelligence (AI) is the use of machine learning models to forecast the potential rating a user would assign to a specific book, or to predict the average overall rating a book might receive. This process is a core component of modern AI-driven book recommendation systems. "
    ]
  },
  {
    "term": "Bootstrapping",
    "definition": [
      "Bootstrapping in artificial intelligence and machine learning is a powerful, self-starting process used primarily for estimating model uncertainty, improving model performance, and allowing agents to learn efficiently from limited data. \r\nThe term \"bootstrapping\" stems from the phrase \"pulling oneself up by one's bootstraps,\" a metaphor for achieving something impossible through one's own efforts, as the method generates robust conclusions using only the original dataset. "
    ]
  },
  {
    "term": "Bubble Filters",
    "definition": [
      "The term \"filter bubble\" in artificial intelligence (AI) and technology refers to the intellectual isolation that occurs when algorithms selectively guess what information a user would like to see based on their past online activity, search history, and personal information. This curated content reinforces a user's existing beliefs and preferences, effectively isolating them from opposing viewpoints and diverse perspectives. "
    ]
  },
  {
    "term": "CamemBERT",
    "definition": [
      "CamemBERT is a state-of-the-art language model for the French language, developed by Facebook AI (Meta AI) and Inria researchers. It is an adaptation of the English-centric RoBERTa architecture (an improved version of BERT) and is specifically designed to achieve high performance on French Natural Language Processing (NLP) tasks. "
    ]
  },
  {
    "term": "Candidate Record Pairs",
    "definition": [
      "In artificial intelligence and data management, a candidate record pair refers to two data entries that are identified by an algorithm as potential matches representing the same real-world entity. This is a crucial step in record linkage or entity matching, the process of combining records from different data sources or identifying duplicates within a single dataset without unique identifiers. "
    ]
  },
  {
    "term": "Case-Based Reasoning",
    "definition": [
      "Case-Based Reasoning (CBR) is an AI method solving new problems by finding and adapting solutions from similar past experiences (cases) stored in a knowledge base, mimicking human learning where similar problems yield similar solutions. It uses a cycle of Retrieve, Reuse, Revise, and Retain to learn and adapt, making it powerful for complex domains where explicit rules are difficult to define, like medical diagnosis, legal reasoning, or technical support. "
    ]
  },
  {
    "term": "Causal Embedding",
    "definition": [
      "In artificial intelligence, causal embedding refers to methods that represent cause-and-effect relationships within continuous vector spaces (embeddings). This approach allows AI models to move beyond merely identifying statistical correlations in data to understand the underlying mechanisms and reasons why events happen, enabling more reliable, explainable, and robust decision-making. "
    ]
  },
  {
    "term": "Causal Knowledge Analytics",
    "definition": [
      "Causal Knowledge Analytics (AI) is an advanced AI field focused on understanding why things happen, moving beyond traditional AI's pattern/correlation recognition to model true cause-and-effect relationships, enabling explainable decisions, counterfactual reasoning (\"what if\"), and accurate intervention planning in complex systems, unlike standard AI that just predicts based on past data. It uses methods like Structural Causal Models (SCMs) and Causal Discovery to build explicit cause-effect graphs, allowing systems to answer \"why,\" simulate outcomes, and reduce bias in areas like healthcare, finance, and marketing. "
    ]
  },
  {
    "term": "Causal Machine Learning",
    "definition": [
      "Causal Machine Learning (CML) or Causal AI merges traditional machine learning with causal inference, moving AI beyond just predicting patterns (correlation) to understanding why things happen (cause-and-effect), enabling robust \"what-if\" scenarios, explainability, and better decision-making by identifying true drivers behind outcomes, rather than just associated factors, to answer questions about interventions and counterfactuals. It uses techniques like Structural Causal Models (SCMs) and causal graphs (DAGs) to build systems that can infer genuine causal links, crucial for advanced AI and tackling complex problems in healthcare, business, and science. "
    ]
  },
  {
    "term": "Challenges In Implementing AI",
    "definition": [
      "Implementing AI faces hurdles like data issues (quality, bias, privacy), technical complexities (integration, infrastructure, skills gap), high costs, and organizational/ethical challenges (resistance to change, lack of vision, accountability, trust). Businesses struggle with integrating AI into old systems, finding skilled talent, ensuring data security, overcoming workforce fear, and defining clear value, making a strategic approach vital. "
    ]
  },
  {
    "term": "Challenges To Adopt AI",
    "definition": [
      "Key challenges to AI adoption include poor data quality & availability, talent shortages (expertise gaps), high costs & unclear ROI, integration with legacy systems, ethical concerns (bias, privacy), governance/compliance issues, and organizational resistance to change, all hindering scaled, reliable AI implementation."
    ]
  },
  {
    "term": "Character-Level Sequence-To-Sequence Model",
    "definition": [
      "A Character-Level Sequence-to-Sequence (Seq2Seq) model is an artificial intelligence architecture that processes and generates text one character at a time, rather than using larger units like words. This approach allows the model to handle a small, fixed vocabulary of characters (e.g., the English alphabet) and build all words from scratch. "
    ]
  },
  {
    "term": "Character-Recognition",
    "definition": [
      "Character Recognition in AI, primarily Optical Character Recognition (OCR) and its advanced form Intelligent Character Recognition (ICR), is the technology that uses AI, machine learning (ML), and computer vision to convert images of text (printed, handwritten, or typed) into editable, searchable, and machine-readable digital text, transforming static documents into dynamic data for automation, data entry, and information extraction. "
    ]
  },
  {
    "term": "Character-Word Embeddings",
    "definition": [
      "Character-word embeddings are a natural language processing (NLP) technique where the vector representation of a word is constructed by combining information from both the whole word unit and its constituent character components. This hybrid approach offers advantages over using word embeddings or character embeddings alone. "
    ]
  },
  {
    "term": "Characteristic Prompt",
    "definition": [
      "The term \"Characteristic Prompt\" refers to the key elements or characteristics that make an AI prompt effective. The quality of the prompt significantly influences the quality of the AI's output, a principle often summarized as \"junk in, junk out\". "
    ]
  },
  {
    "term": "Chat Dialogue Subsystems",
    "definition": [
      "The term \"Chat Dialogue Subsystems\" refers to the specific, modular components that collectively form an intelligent system capable of engaging in human-like conversations, such as a chatbot or a virtual assistant. These systems use a combination of artificial intelligence (AI) technologies, primarily natural language processing (NLP) and machine learning (ML), to understand user input and generate appropriate responses. "
    ]
  },
  {
    "term": "Chatbot",
    "definition": [
      "An AI chatbot is a computer program that uses Artificial Intelligence, specifically Natural Language Processing (NLP) and Machine Learning (ML), to understand, process, and simulate human-like conversations through text or voice, offering instant, personalized support, answering questions, and automating tasks for users on websites and apps, evolving beyond simple rules to handle complex queries. "
    ]
  },
  {
    "term": "Chatbot Adoption",
    "definition": [
      "Chatbot adoption is the process of integrating artificial intelligence (AI) chatbot technology into an organization's operations or an individual's routine for specific tasks, such as enhancing customer service, improving efficiency, or facilitating learning. This involves moving beyond traditional, rule-based chatbots to sophisticated systems that use natural language processing (NLP) and machine learning (ML) to understand user intent and provide human-like, context-aware responses. "
    ]
  },
  {
    "term": "Chatbot Affordances",
    "definition": [
      "Chatbot affordances refer to the potential uses and action possibilities that the features of artificial intelligence (AI) chatbots offer to users. These are not inherent properties of the technology itself, but rather emerge from the interaction between the chatbot's capabilities and the user's goals and context. "
    ]
  },
  {
    "term": "Chatbot Software",
    "definition": [
      "Chatbot software (artificial intelligence) uses AI like NLP and Machine Learning to simulate human conversation, understanding user intent to provide instant, automated, and personalized responses via text or voice, functioning as virtual assistants for customer service, information retrieval, or task completion, evolving from simple scripts to complex, context-aware interactions. "
    ]
  },
  {
    "term": "Chatbot Support",
    "definition": [
      "Chatbot Support (AI) uses artificial intelligence, Natural Language Processing (NLP), and Machine Learning (ML) to simulate human conversations, allowing software to understand user intent and provide instant, automated support 24/7 on websites and apps, handling common questions and escalating complex issues, thus improving customer experience and efficiency. Unlike basic bots, AI chatbots learn from interactions, understand context, and offer personalized, unscripted responses, making support more dynamic and human-like. "
    ]
  },
  {
    "term": "Chatbot-Human Interaction",
    "definition": [
      "Chatbot-human interaction is the field of study and the practical application of designing how humans and artificial intelligence (AI) systems, specifically chatbots, communicate and collaborate. The core goal is to create seamless and effective conversations that mimic human dialogue, allowing users to interact with technology in a natural way for various purposes. "
    ]
  },
  {
    "term": "ChatGPT",
    "definition": [
      "ChatGPT is an AI chatbot from OpenAI that uses large language models (LLMs) to understand and generate human-like text, allowing users to have natural conversations, ask questions, write code, create content, and get information by typing prompts. It works by analyzing vast amounts of data to find patterns, enabling it to produce coherent, relevant, and creative responses for various tasks, from explaining complex topics to drafting emails or summaries. "
    ]
  },
  {
    "term": "Chatgpt 4.0",
    "definition": [
      "ChatGPT-4o (\"o\" for \"omni\") is a multimodal artificial intelligence model developed by OpenAI. It is designed to enable natural human-computer interaction by seamlessly processing and generating any combination of text, audio, image, and video inputs and outputs in real time. "
    ]
  },
  {
    "term": "Chatgpt Adoption",
    "definition": [
      "ChatGPT adoption refers to the rapid and widespread integration of the OpenAI's AI-powered language model into daily life, work, and institutional processes. This adoption has been notably fast, with ChatGPT reaching one million users in just five days and 100 million active users within two months, making it one of the fastest-growing consumer applications in history. "
    ]
  },
  {
    "term": "Chatgpt Era",
    "definition": [
      "The \"ChatGPT Era\" refers to the current period, beginning with the public launch of OpenAI's ChatGPT in November 2022, which saw a rapid and widespread integration of powerful generative artificial intelligence (AI) tools into public life and various industries. "
    ]
  },
  {
    "term": "Chatgpt In Libraries",
    "definition": [
      "ChatGPT in libraries uses OpenAI's AI to assist with research, user services, staff training, and instruction by answering questions, generating content (like summaries or code), and providing interactive learning, though libraries must also teach users critical evaluation due to potential inaccuracies and ethical concerns like bias. It acts as a research assistant, information literacy tool, and productivity enhancer, helping users find resources and staff manage workloads, but requires thoughtful, ethical implementation and user education on its limitations. "
    ]
  },
  {
    "term": "Chatgpt Natural Language Processing",
    "definition": [
      "ChatGPT is an artificial intelligence (AI) chatbot that uses Natural Language Processing (NLP) to generate human-like conversation. NLP is the core technology that enables ChatGPT to understand, interpret, and respond to human language in a coherent and contextually relevant manner. "
    ]
  },
  {
    "term": "Chatgpt-3.5",
    "definition": [
      "ChatGPT-3.5 is an artificial intelligence (AI) large language model (LLM) developed by OpenAI. It is a text-based model designed for conversational interaction, capable of understanding prompts and generating human-like responses. "
    ]
  },
  {
    "term": "Chatgpt4.5",
    "definition": [
      "ChatGPT 4.5 was a large language model (LLM) developed by OpenAI, notable as an incremental update to the GPT-4 architecture and the last model iteration before the release of the current flagship model, GPT-5. It focused on enhancing conversational naturalness, emotional intelligence, and accuracy through advanced unsupervised learning and human feedback. "
    ]
  },
  {
    "term": "Chatpdf",
    "definition": [
      "ChatPDF is an AI-powered tool that allows users to interact with PDF documents (as well as Word, PowerPoint, and text files) through a conversational chat interface. Instead of manually sifting through lengthy documents, users can ask questions, request summaries, and extract specific information. "
    ]
  },
  {
    "term": "Classification AI",
    "definition": [
      "Classification AI is an AI field where algorithms learn to sort data into predefined categories (classes) based on patterns, enabling automated organization, prediction, and decision-making, like labeling emails as spam or not spam, recognizing images, or detecting fraud, by understanding features in new data after training on labeled examples. It's a core part of machine learning, moving beyond simple rules to understand context for better accuracy at scale. "
    ]
  },
  {
    "term": "Classification Algorithms",
    "definition": [
      "Classification algorithms in AI are supervised learning methods that teach computers to sort input data into predefined categories or classes (like \"spam\" or \"not spam,\" \"cat\" or \"dog\") by learning patterns from labeled training data, enabling them to predict the correct group for new, unseen data, essential for tasks from medical diagnosis to spam filtering. "
    ]
  },
  {
    "term": "Classification And Regression Tree (Cart) Model",
    "definition": [
      "The CART (Classification and Regression Tree) model is a powerful, interpretable machine learning algorithm that builds binary decision trees to predict categorical (classification) or numerical (regression) outcomes by recursively splitting data based on features, creating a flowchart-like structure with nodes, branches, and leaf nodes that represent decisions and predictions, making it excellent for understanding complex data relationships in finance, medicine, and marketing. "
    ]
  },
  {
    "term": "Claude AI",
    "definition": [
      "Claude AI is a family of generative artificial intelligence (AI) chatbots and large language models (LLMs) developed by the AI safety and research company Anthropic. It is designed to be a \"helpful, harmless, and honest\" assistant, with a unique emphasis on ethical guidelines. "
    ]
  },
  {
    "term": "Clip",
    "definition": [
      "CLIP (Contrastive Language-Image Pre-training) is a pioneering AI model developed by OpenAI that bridges the gap between computer vision and natural language understanding. It learns to associate images with their corresponding text descriptions from a massive dataset of 400 million image-text pairs scraped from the internet. "
    ]
  },
  {
    "term": "Cloud-Based Artificial Intelligence",
    "definition": [
      "Cloud-Based AI (Artificial Intelligence) is the delivery of AI tools, services, and processing power over the internet via cloud platforms, allowing businesses to use advanced capabilities like machine learning, NLP, and computer vision without huge upfront hardware costs or complex setup, accessing massive compute power and storage on-demand to build and deploy smart applications easily. It integrates AI with cloud infrastructure, making AI accessible, scalable, and cost-effective by providing managed services and pre-built models. "
    ]
  },
  {
    "term": "Cluster Analysis",
    "definition": [
      "Cluster Analysis in AI is an unsupervised machine learning technique that groups similar data points into \"clusters\" (subsets) without prior labels, revealing hidden patterns by making items within a cluster more alike than those in other clusters. It's used for tasks like customer segmentation, anomaly detection (fraud), image analysis, and data compression, relying on similarities like proximity or density to form groups, with common methods including K-Means or Hierarchical Clustering. "
    ]
  },
  {
    "term": "Cluster Weight Learning",
    "definition": [
      "Cluster Weight Learning is an advanced approach in artificial intelligence (specifically, in machine learning clustering and optimization) that adaptively assigns different weights (or importance scores) to individual data points, features, or even entire clusters during the learning process. The primary goal is to improve the robustness, accuracy, and generalization ability of clustering models, especially when dealing with complex data issues like outliers, noise, or imbalanced class distributions. "
    ]
  },
  {
    "term": "Clustering",
    "definition": [
      "Clustering in AI is an unsupervised machine learning technique that groups similar data points into \"clusters\" (groups) based on inherent patterns, without needing pre-labeled data, to reveal hidden structures, enabling tasks like customer segmentation, anomaly detection, and data simplification. It works by finding natural groupings where items within a cluster are more alike than items in other clusters, helping make sense of complex datasets. "
    ]
  },
  {
    "term": "Clustering Algorithms",
    "definition": [
      "Clustering algorithms in AI are unsupervised machine learning methods that group similar data points into \"clusters\" based on inherent patterns, finding natural groupings without pre-labeled data to reveal structure, segment customers, find anomalies, or aid data compression. Key types include K-Means, DBSCAN, and Hierarchical clustering, each using different strategies (like centroids, density, or hierarchies) to identify similarities, making them vital for data analysis, personalization, and pattern recognition across fields like biology, marketing, and search. "
    ]
  },
  {
    "term": "Co-Clustering",
    "definition": [
      "In AI and data mining, Co-clustering (or biclustering/two-way clustering) is a machine learning technique that simultaneously groups both rows (e.g., users, documents) and columns (e.g., items, words) of a data matrix to find underlying patterns, revealing blocks of similar data points, like customers who like specific products or related genes under certain conditions. It's superior to standard clustering by capturing interactions between dimensions, crucial for big data in text mining, recommendation systems, and bioinformatics. "
    ]
  },
  {
    "term": "Cognition Modelling",
    "definition": [
      "Cognitive Modeling in AI is about building computer systems that mimic human thinking, learning, perception, and problem-solving, creating AI that can understand, reason, and interact more naturally. It uses techniques like machine learning and NLP to process complex data, learn from it, and make decisions, aiming to create truly intelligent agents that can handle tasks like humans, improving everything from user interfaces to expert systems. "
    ]
  },
  {
    "term": "Cognitive Agent",
    "definition": [
      "A Cognitive Agent in AI is an intelligent software system designed to mimic human thought, perception, and decision-making, using skills like reasoning, learning, and understanding natural language to handle complex tasks, adapt to new info, and act autonomously, unlike simple rule-based bots. They combine AI fields (NLP, ML, reasoning) to process unstructured data, interpret intent, and learn from interactions to solve problems contextually, acting like a digital \"brain\" that can understand, decide, and act. "
    ]
  },
  {
    "term": "Cognitive Artificial Intelligence",
    "definition": [
      "Cognitive AI is a branch of artificial intelligence that mimics human thinking, learning, and problem-solving by using machine learning, deep learning, and natural language processing (NLP) to understand context, adapt to new info, and reason, rather than just following fixed rules. It aims to replicate human cognitive abilities like understanding nuance, making logical inferences, and interacting naturally, moving beyond basic automation to enhance human decision-making in complex tasks across industries like healthcare, finance, and retail. "
    ]
  },
  {
    "term": "Digital Twins",
    "definition": [
      "Cognitive Clones (also known as digital twins, mind clones, or thought clones) are advanced artificial intelligence systems designed to replicate a specific person's cognitive processes, including their decision-making patterns, personality traits, knowledge, and communication style. Unlike generic AI models, which are trained on vast, general datasets, cognitive clones are personalized, functioning as a digital proxy for the individual they are based on. ",
      "A Digital Twin, powered by Artificial Intelligence (AI), is a dynamic, virtual replica of a physical object, system, or process, connected by real-time data from sensors (IoT) to mirror its real-world counterpart's behavior, performance, and conditions throughout its lifecycle. AI amplifies these twins by enabling advanced analysis, pattern recognition, predictive modeling, and autonomous decision-making, allowing for real-time monitoring, simulations, \"what-if\" scenarios, and optimization without disrupting the physical asset. "
    ]
  },
  {
    "term": "Human-Artificial Intelligence Collaboration",
    "definition": [
      "Human-AI Collaboration (HAIC) is a partnership where people and AI systems work together, blending human creativity, context, and judgment with AI's speed, data processing, and pattern recognition to achieve better results than either could alone, shifting from simple automation to augmenting human capabilities for complex problem-solving and innovation across industries. "
    ]
  },
  {
    "term": "Communicative AI",
    "definition": [
      "Communicative AI refers to artificial intelligence systems designed to perform communication tasks that were originally exclusive to humans. These systems act as active communication partners, not merely as passive tools or channels, using natural language processing (NLP) and machine learning to understand and generate human-like dialogue. "
    ]
  },
  {
    "term": "Complex Activity Recognition",
    "definition": [
      "Complex Activity Recognition (CAR) in artificial intelligence is a subfield of Human Activity Recognition (HAR) that focuses on identifying and interpreting intricate, real-world human behaviors composed of multiple, varied, and often non-sequential simple actions or sub-activities. Unlike simple activity recognition (e.g., walking or sitting), which involves clear, repetitive patterns, CAR deals with the variability, duration, and sequential dependencies inherent in everyday life. "
    ]
  },
  {
    "term": "Compulsive Use Of Chatgpt",
    "definition": [
      "Compulsive use of ChatGPT refers to the excessive, uncontrollable, and often emotionally dependent interaction with the AI chatbot to the extent that it negatively disrupts a user's daily life, relationships, sleep, or emotional well-being. While not yet an official clinical diagnosis, the behavior mirrors patterns found in other behavioral addictions like internet or social media overuse. "
    ]
  },
  {
    "term": "Concept Drift",
    "definition": [
      "Concept drift in AI is when the real-world patterns a model learns change over time, making its predictions less accurate because the relationship between input data and the target outcome evolves, requiring models to be continuously monitored and retrained to stay effective. It's like teaching a model to recognize spam, but spammers constantly change their tactics, so the \"concept\" of spam shifts, invalidating the old rules. "
    ]
  },
  {
    "term": "Content Intelligence",
    "definition": [
      "Content Intelligence (CI) uses AI, machine learning, and big data to analyze content performance, audience behavior, and market trends, transforming raw content data into actionable insights for better strategy, creation, distribution, and optimization, helping businesses create more relevant, high-performing content that drives engagement and ROI. It goes beyond traditional analytics by predicting what content will work, understanding why, and automating personalized experiences at scale. "
    ]
  },
  {
    "term": "Content Ranking Algorithms",
    "definition": [
      "Content ranking algorithms using AI are smart systems that sort information (like search results, social posts, products) by scoring them based on relevance, quality, and user intent, using machine learning to understand context, user behavior (engagement, history), and content signals (E-E-A-T, comprehensiveness) to predict what you'll find most useful, moving beyond simple keywords to deliver personalized, helpful results. "
    ]
  },
  {
    "term": "Contextual Window",
    "definition": [
      "In AI, a Context Window is an LLM's working memory, defining the maximum amount of text (measured in tokens) it can process at once, including your prompt and conversation history, allowing it to understand long inputs, maintain coherence in long chats, and perform complex tasks by referencing past information before older content gets forgotten or truncated. A larger window means better recall, enabling summaries of books or complex document analysis, while a small window forces the AI to \"forget\" early parts of a long interaction. "
    ]
  },
  {
    "term": "Continual Graph Learning",
    "definition": [
      "Continual Graph Learning (CGL) in AI trains models on constantly changing, evolving graphs (like social networks or biological systems) to learn new patterns incrementally without forgetting old knowledge, addressing the catastrophic forgetting typical in static graph learning by managing complex node/task dependencies and integrating knowledge efficiently over time, using methods like memory replay, regularization, and knowledge distillation. "
    ]
  },
  {
    "term": "Continuity Prediction",
    "definition": [
      "Predicting Business/Operational Continuity: This is the most common use in industry, where AI systems are used to forecast potential disruptions to an organization's operations (e.g., supply chain issues, cyberattacks, natural disasters) and help maintain uninterrupted service.",
      "Continuity as a Technical Property of AI Models (Explainable AI - XAI): This is a more academic and technical concept referring to the mathematical property that small changes in a model's input should result in only small, consistent changes in its output or explanation. "
    ]
  },
  {
    "term": "Contrastive Learning",
    "definition": [
      "Contrastive Learning is a powerful self-supervised AI method that teaches models to understand data by contrasting similar and dissimilar examples, pulling representations of similar items (positives) closer in an embedding space and pushing unrelated items (negatives) further apart, without needing extensive human labels. It works by creating positive pairs (e.g., different crops of the same image) and contrasting them with random negatives, allowing models to learn robust, general features, making it great for tasks with limited labeled data. "
    ]
  },
  {
    "term": "Conversational Agent",
    "definition": [
      "A Conversational Agent (CA) is an AI system using Natural Language Processing (NLP) and Machine Learning (ML) to understand and mimic human conversation via text or voice, powering tools like chatbots and virtual assistants that offer human-like, context-aware interactions for tasks like customer service, information retrieval, and completing complex requests, moving beyond simple pre-programmed responses. They learn from data to provide personalized, efficient, and natural dialogue, handling everything from simple FAQs to complex, multi-turn conversations. "
    ]
  },
  {
    "term": "Conversational Recommender System",
    "definition": [
      "A Conversational Recommender System (CRS) is an artificial intelligence (AI) agent that actively engages users in a multi-turn dialogue (using natural language or other interaction modalities like buttons/forms) to understand their needs and preferences, provide personalized suggestions, and receive feedback. \r\nUnlike traditional, static recommender systems that rely solely on past behavior and one-shot interactions (e.g., a \"recommended for you\" list on Netflix), a CRS dynamically updates its understanding of a user's interests during the conversation, allowing for more personalized and context-aware recommendations. "
    ]
  },
  {
    "term": "Conversational User Interface",
    "definition": [
      " Conversational User Interface (CUI) uses AI and Natural Language Processing (NLP) to let users interact with computers via human-like text or voice, mimicking a natural dialogue instead of traditional buttons or menus. It allows users to speak or type requests in their own language (like with Siri, Alexa, or chatbots), and the AI understands intent to provide relevant responses, making technology more intuitive and accessible. "
    ]
  },
  {
    "term": "Convolutional Neural Network",
    "definition": [
      "A Convolutional Neural Network (CNN) is a specialized deep learning AI model that excels at processing visual data like images and videos, mimicking the human visual cortex by automatically learning hierarchical patterns (edges, shapes, objects) through convolutional layers that use filters to extract features, pooling layers to reduce dimensions, and fully connected layers for final classification, powering tasks from facial recognition to self-driving cars."
    ]
  },
  {
    "term": "COPILOT",
    "definition": [
      "An AI Copilot is a virtual assistant, like Microsoft Copilot, that uses generative AI to boost productivity by offering contextual help, automating tasks, answering questions, drafting content (emails, code, images), and integrating within apps like Microsoft 365 or GitHub to streamline workflows. It acts as a smart, helpful partner, reducing manual effort and allowing users to focus on more strategic work by providing real-time suggestions and insights. "
    ]
  },
  {
    "term": "Cosine Similarity",
    "definition": [
      "Cosine similarity in AI measures how alike two data points (represented as vectors) are by calculating the cosine of the angle between them, indicating similarity in direction, not magnitude, and is crucial for tasks like text analysis, recommendations, and search, where it quantifies semantic closeness with a score from -1 (opposite) to 1 (identical). "
    ]
  },
  {
    "term": "Count Prediction",
    "definition": [
      "In artificial intelligence (AI), count prediction refers to the use of machine learning models and statistical techniques to forecast a specific, discrete numerical value (a \"count\") based on historical data. It is a specific application within the broader field of predictive AI or predictive analytics. \r\nThe goal is to answer \"how many\" questions, using patterns found in large datasets to make data-driven estimations about future or unknown events. "
    ]
  },
  {
    "term": "Critical AI Studies",
    "definition": [
      "Critical AI Studies is an interdisciplinary field examining Artificial Intelligence's deep societal impacts, focusing on power, bias, ethics, and equity, moving beyond just technical aspects to question who benefits, whose values are embedded, and how AI reinforces or challenges existing social injustices, drawing on humanities, social sciences, and computer science to foster accountable, democratic tech development. It analyzes AI's role in reshaping culture, politics, labor, and knowledge, addressing issues like algorithmic racism, data colonialism, and the concentration of power, aiming for inclusive, just, and democratic digital futures. "
    ]
  },
  {
    "term": "Critical Artificial Intelligence Literacy",
    "definition": [
      "Critical AI Literacy is the ability to understand, analyze, and question AI systems, recognizing their technical workings, inherent biases, societal impacts (like power structures, fairness, environment), and limitations, enabling users to engage with them thoughtfully rather than blindly accepting outputs. It goes beyond basic usage, focusing on ethical implications, self-reflection, and ensuring AI serves human values, not just technical functions, fostering informed decision-making and resisting over-reliance. "
    ]
  },
  {
    "term": "Cross Modal Fusion",
    "definition": [
      "Cross-modal fusion in artificial intelligence is the process of integrating information from different data types (modalities), such as text, images, and audio, to create a more comprehensive and unified understanding than any single modality could provide alone. This allows AI systems to mimic human-like perception and reasoning. "
    ]
  },
  {
    "term": "Cross Modal Retrieval",
    "definition": [
      "Cross-Modal Retrieval (CMR) in AI lets you search for data in one format (like images) using a query from a different format (like text), bridging the \"semantic gap\" between modalities like text, audio, video, and images by mapping them to a shared understanding space. It enables intuitive searches, such as typing \"a fluffy dog playing in the park\" to find matching pictures or audio clips, making information access more powerful and natural. "
    ]
  },
  {
    "term": "Cross-Lingual Text Embedding",
    "definition": [
      "Cross-lingual text embedding is a technique in artificial intelligence and natural language processing (NLP) that represents words, sentences, or documents from multiple languages in a single, shared vector space. In this unified space, texts with similar meanings are located close together, regardless of the original language. \r\nThis method allows for semantic comparisons across languages without direct translation. For example, the English word \"dog\" and the German word \"Hund\" would have similar vector representations in the shared embedding space because they refer to the same concept. "
    ]
  },
  {
    "term": "Cross-Modal Alignment",
    "definition": [
      "Cross-Modal Alignment in AI is about teaching models to understand and connect different types of data (like text, images, audio) by mapping them into a shared \"semantic space,\" so concepts like a \"red car\" in text, a picture of a Ferrari, and the sound of a fast engine align, enabling unified understanding, search, and reasoning across modalities, crucial for tasks like image captioning, video Q&A, and zero-shot learning. "
    ]
  },
  {
    "term": "Ctr Prediction",
    "definition": [
      "CTR Prediction (Click-Through Rate Prediction) in AI is using machine learning to forecast the likelihood a user will click on an online ad or recommendation, crucial for personalized ads, search rankings, and content suggestions, by analyzing user behavior, item features, and context with models like DeepFM or XGBoost, evolving from simple models to complex deep learning for better accuracy in real-world systems. "
    ]
  },
  {
    "term": "Cyber Profiling",
    "definition": [
      "Cyber profiling with artificial intelligence (AI) is the analytical process of using machine learning and data analytics to analyze a person's or a malicious entity's online presence and digital footprints to build a comprehensive behavioral or psychological profile. "
    ]
  },
  {
    "term": "Cybersecurity Analytics",
    "definition": [
      "Cybersecurity Analytics with AI uses machine learning and advanced algorithms to automatically analyze vast amounts of security data (logs, network traffic, user behavior) to proactively detect, predict, and respond to threats faster and more accurately than humans, identifying complex patterns, anomalies, malware, and insider threats to strengthen defenses and automate responses. It moves security from reactive to proactive, learning from past attacks to spot new ones and reducing overwhelming alert volumes for security teams. "
    ]
  },
  {
    "term": "Dall-E 2",
    "definition": [
      "DALL-E 2 is an artificial intelligence (AI) system developed by research laboratory OpenAI that can generate original, realistic images and art from a natural language description (a text prompt). "
    ]
  },
  {
    "term": "Data Augmentation",
    "definition": [
      "Data augmentation in AI is a technique to artificially expand a dataset by creating modified copies of existing data, making it larger and more diverse to train robust machine learning models, especially when real data is scarce. It involves applying small, label-preserving transformations (like rotating/flipping images, replacing words with synonyms) to existing data, helping models generalize better and avoid overfitting to specific training examples. "
    ]
  },
  {
    "term": "Data Bias",
    "definition": [
      "Data bias occurs when biases present in the training and fine-tuning data sets of artificial intelligence (AI) models adversely affect model behavior. AI models are programs that have been trained on data sets to recognize certain patterns or make certain decisions."
    ]
  },
  {
    "term": "Data Imbalance",
    "definition": [
      "Data imbalance in artificial intelligence (AI) is a common challenge where the number of instances across different classes in a dataset is disproportionately distributed. This results in a majority class with many samples and a minority class with significantly fewer samples. "
    ]
  },
  {
    "term": "Data Labeling",
    "definition": [
      "Data labeling in AI is the crucial process of tagging or annotating raw data (images, text, audio, video) with meaningful labels, providing context so machine learning models, especially in supervised learning, can learn to recognize patterns, understand information, and make accurate predictions, like identifying a cat in a photo or the sentiment in a review. It's the foundational step that teaches AI systems the \"right answers\" for given inputs, enabling tasks from self-driving cars to smart assistants. "
    ]
  },
  {
    "term": "Data Mining",
    "definition": [
      "Data Mining in AI is using machine learning algorithms and statistical methods to automatically discover hidden patterns, trends, and valuable insights from massive datasets, enabling smarter decisions, automation, and personalization beyond manual analysis. It's the process of extracting knowledge from big data, serving as a core part of building intelligent systems that learn from data to solve problems. "
    ]
  },
  {
    "term": "Data-Centric Artificial Intelligence",
    "definition": [
      "Data-Centric AI (DCAI) is a strategic approach to building AI systems that prioritizes systematically improving the quality and consistency of training data, rather than endlessly tweaking complex models, to achieve better performance. It treats data as a \"living asset\" that requires continuous engineering through cleaning, labeling, and augmenting, recognizing that high-quality data is the key to building more accurate, efficient, and robust AI, especially in real-world applications. "
    ]
  },
  {
    "term": "Dataset Shift",
    "definition": [
      "Dataset Shift in AI is when the statistical properties of data change between the training phase and the real-world deployment (test phase), causing a model's performance to degrade because it sees data it wasn't prepared for, like spam filters failing on new slang or medical AIs misinterpreting vital signs due to pandemic shifts in patient demographics. This mismatch, where the training distribution differs from the target distribution, makes predictions unreliable, necessitating techniques to detect and adapt to these changes. "
    ]
  },
  {
    "term": "Decentralized Intelligence",
    "definition": [
      "Decentralized Intelligence (DeAI) distributes AI tasks, data, and control across a network of devices, moving away from single-server systems for enhanced privacy, security, and resilience, using tech like blockchain, federated learning, and edge computing to keep data local while enabling collaborative, transparent, and user-controlled AI. It prevents data monopolies, reduces single points of failure, and allows users to retain control over their information, fostering a more democratic AI ecosystem. "
    ]
  },
  {
    "term": "Decision Tree",
    "definition": [
      "A Decision Tree in AI is a supervised learning model that uses a flowchart-like structure to make predictions by splitting data into smaller subsets based on feature tests, mimicking human decision-making through a series of nodes (decisions/attributes), branches (outcomes), and leaves (final results/classes) for classification or regression tasks, prized for its simplicity and interpretability. "
    ]
  },
  {
    "term": "Deep Deterministic Policy Gradient",
    "definition": [
      "Deep Deterministic Policy Gradient (DDPG) is an advanced actor-critic reinforcement learning algorithm for training agents in environments with continuous actions (like robot arm movements), combining deep learning with policy gradient ideas to learn both what action to take (actor) and how good that action is (critic), making it powerful for robotics, self-driving, and control tasks. It learns a direct, deterministic action from a state, unlike some methods that output probabilities, using off-policy data and deep neural networks for stable learning. "
    ]
  },
  {
    "term": "Deep Active Learning",
    "definition": [
      "Deep Active Learning (DeepAL) in AI combines Active Learning's smart data selection with Deep Learning's powerful feature extraction, allowing models to efficiently learn from vast datasets by interactively querying human experts for labels on only the most informative, uncertain, or valuable unlabeled examples, drastically reducing annotation costs while maintaining high performance, especially in data-hungry fields like computer vision. "
    ]
  },
  {
    "term": "Deep Belief Networks",
    "definition": [
      "A Deep Belief Network (DBN) is a deep neural network, a type of AI model, built from stacked layers of simpler neural networks (like Restricted Boltzmann Machines, RBMs) that learns hierarchical features from data for tasks like image/speech recognition, classification, and dimensionality reduction, especially good with unlabeled data by first learning features unsupervised and then fine-tuning for supervised tasks. "
    ]
  },
  {
    "term": "Deep Knowledge Tracing",
    "definition": [
      "Deep Knowledge Tracing (DKT) is an AI method using deep learning (especially Recurrent Neural Networks) to model a student's evolving knowledge state over time, predicting future performance by finding complex patterns in their interaction sequences (like correct/incorrect answers) with learning materials, moving beyond simpler models to capture deeper, dynamic learning trajectories for personalized education. It aims to provide more accurate assessments and feedback for Intelligent Tutoring Systems (ITS). "
    ]
  },
  {
    "term": "Deep Learning",
    "definition": [
      "Deep Learning is a powerful subset of Artificial Intelligence (AI) and Machine Learning that uses multi-layered Artificial Neural Networks (inspired by the human brain) to automatically learn complex patterns from vast amounts of data, enabling machines to perform tasks like image recognition, speech understanding, and natural language processing with high accuracy, essentially mimicking human learning and decision-making. "
    ]
  },
  {
    "term": "Deep Learning Model",
    "definition": [
      "A Deep Learning Model is an advanced AI system using multi-layered artificial neural networks, inspired by the human brain, to automatically learn complex patterns from vast amounts of data (images, text, sound) for tasks like recognizing objects, translating languages, or powering self-driving cars, without explicit human programming for every rule. It's a powerful subset of machine learning where \"deep\" refers to these many interconnected layers that extract increasingly complex features from raw data, improving accuracy over time through iteration. "
    ]
  },
  {
    "term": "Deep Multi-View Clustering",
    "definition": [
      "Deep Multi-View Clustering (DMVC) in AI uses deep neural networks to find hidden groups (clusters) in data that has multiple types of features (views), like an image having texture, shape, and color data all at once. It learns powerful, shared representations from these different views to overcome limitations of single-view methods, improving accuracy by fusing complementary information and identifying consistent patterns across modalities for tasks in image analysis, bioinformatics, and social networks. "
    ]
  },
  {
    "term": "Deep Neural Network",
    "definition": [
      "A Deep Neural Network (DNN) is a type of artificial neural network (ANN) in the field of artificial intelligence (AI) characterized by its multiple hidden layers between the input and output layers. This \"deep\" architecture allows the network to automatically learn hierarchical patterns and complex, non-linear relationships in large amounts of data. "
    ]
  },
  {
    "term": "Deep Reinforcement Learning",
    "definition": [
      "Deep Reinforcement Learning (DRL) merges Deep Learning (neural networks) with Reinforcement Learning (trial-and-error learning with rewards/penalties) to create AI agents that learn complex decision-making in vast environments, like playing video games from raw pixels or controlling robots, by maximizing long-term rewards without explicit programming, blending pattern recognition with sequential decision-making. "
    ]
  },
  {
    "term": "Deepfake",
    "definition": [
      "A deepfake is realistic, AI-generated synthetic media (video, audio, image) that makes a person seem to say or do something they never did, created using deep learning to swap faces, manipulate expressions, or clone voices from existing footage. While used in entertainment, deepfakes pose serious risks for spreading misinformation, political sabotage, financial fraud, and cyberbullying, eroding trust in genuine media by blurring the lines between reality and fabrication. "
    ]
  },
  {
    "term": "Deepfake Videos",
    "definition": [
      "Deepfake videos use Artificial Intelligence (AI), specifically deep learning, to create hyper-realistic but fake videos, audio, or images where a person's face, voice, or actions are convincingly swapped or manipulated, making it seem they said or did something they never did. The term combines \"deep learning\" and \"fake,\" and while useful for entertainment, it poses risks like spreading misinformation, political manipulation, fraud, and non-consensual pornography, challenging trust in media."
    ]
  },
  {
    "term": "DeepSeek",
    "definition": [
      "DeepSeek is a rapidly emerging Chinese artificial intelligence company known for developing powerful, cost-efficient, open-source large language models (LLMs) that rival Western counterparts like OpenAI's ChatGPT, featuring strong reasoning, coding, and multilingual (Chinese/English) capabilities, with its R1 model gaining massive popularity for its performance and affordability, sparking global discussion on AI competitiveness and national security. "
    ]
  },
  {
    "term": "Deepseek R1",
    "definition": [
      "DeepSeek-R1 is a powerful, open-source, reasoning-focused AI model from DeepSeek, known for its advanced logic, math, and coding abilities, achieved through a unique reinforcement learning (RL) training that emphasizes chain-of-thought (CoT) and self-correction, making complex AI more efficient and accessible for structured tasks. It uses Mixture-of-Experts (MoE) architecture for efficient processing and runs on less costly hardware, earning praise for democratizing high-performance AI. "
    ]
  },
  {
    "term": "Denoising Autoencoder",
    "definition": [
      "A Denoising Autoencoder (DAE) in AI is a type of neural network that learns to remove noise from corrupted data by reconstructing a clean version from a messy input, forcing it to learn robust, essential features instead of just copying data. It works by intentionally adding noise (like zeros or random values) to the input, then training the autoencoder (encoder-decoder structure) to map this noisy data back to the original, clean data, effectively acting as a sophisticated, data-specific noise filter. "
    ]
  },
  {
    "term": "Diagnosis Through Artificial Intelligence",
    "definition": [
      "Diagnosis through Artificial Intelligence (AI) uses machine learning and deep learning to analyze vast medical data (images, records, labs, genetics) to help doctors spot diseases faster, more accurately, and earlier, supporting clinical decisions, automating tasks, and enabling personalized care, though it acts as a tool to augment, not replace, human experts. It excels at pattern recognition in scans (X-rays, MRIs) for conditions like cancer or diabetic retinopathy and integrates data for comprehensive patient views, reducing misdiagnosis risks. "
    ]
  },
  {
    "term": "Diffusion Kernel Attention Network",
    "definition": [
      "The Diffusion Kernel Attention Network (DKAN) is a type of artificial intelligence architecture that integrates diffusion kernels with attention mechanisms, typically for analyzing complex data like functional brain networks or traffic sequences. It combines the strengths of both concepts to effectively capture both local and global dependencies within data. "
    ]
  },
  {
    "term": "Diffusion Models",
    "definition": [
      "Diffusion Models in AI are powerful generative tools that create realistic data (like images, audio) by learning to reverse a gradual noise-adding process, essentially transforming random static into meaningful content, powering famous tools like DALL-E and Stable Diffusion by mastering the 'denoising' step from pure noise to coherent output. "
    ]
  },
  {
    "term": "Digital Assistants",
    "definition": [
      "Digital Assistants (AI) are AI-powered software tools that understand natural language (voice/text) to perform tasks, answer questions, and automate workflows, acting like a personal helper across devices like phones, smart speakers, and cars. They use Natural Language Processing (NLP), machine learning, and speech recognition to learn user preferences, provide personalized experiences, and manage things like schedules, music, smart home devices, and information retrieval. Popular examples include Siri, Alexa, and Google Assistant. "
    ]
  },
  {
    "term": "Digital Human",
    "definition": [
      "A Digital Human (AI) is a lifelike, computer-generated virtual character powered by artificial intelligence, designed to interact with people naturally through realistic visuals, speech, and human-like body language, going beyond basic chatbots to offer engaging, personalized, and scalable experiences in customer service, training, and more. They use technologies like NLP, machine learning, and computer vision to understand context, emotions, and non-verbal cues, adapting responses in real-time for deeper connections. "
    ]
  },
  {
    "term": "Digital-Intelligence Transformation",
    "definition": [
      "Digital-Intelligence Transformation (DIT) is the strategic integration of Artificial Intelligence (AI) into digital transformation, moving beyond simple digitization to embed intelligence, automation, and learning into business processes, creating agile, data-driven organizations that can analyze complex data, personalize experiences, and predict outcomes for efficiency, innovation, and growth. It's about using smart algorithms (like machine learning, NLP) to make systems learn, evolve, and make better decisions, rather than just automating tasks. "
    ]
  },
  {
    "term": "Dilated Convolution Autoencoder Network",
    "definition": [
      "A Dilated Convolution Autoencoder Network (DCAE) is an artificial intelligence model that uses dilated convolutions within an autoencoder architecture. This design allows the network to efficiently learn compressed, rich feature representations of data (like images or time series) by capturing a wide range of contextual information without losing resolution or significantly increasing computational cost. "
    ]
  },
  {
    "term": "Discrimination-Aware Decision Tree",
    "definition": [
      "A Discrimination-Aware Decision Tree is a type of artificial intelligence model designed to make accurate predictions from potentially biased historical data while explicitly avoiding unfair discrimination based on sensitive attributes like race, gender, or age. \r\nTraditional decision trees are built by optimizing for predictive accuracy alone, which means they can inadvertently learn and perpetuate existing societal biases present in the training data. For example, a loan approval model trained on historical data where a certain group was unfairly denied loans might continue to discriminate against that group in the future, even if the sensitive attribute is removed from the data (due to correlations with other features). "
    ]
  },
  {
    "term": "Disentangled Representation Learning",
    "definition": [
      "Disentangled Representation Learning (DRL) in AI is a technique that teaches models to separate complex data into distinct, interpretable underlying factors (like shape, color, pose in images), so that changing one factor doesn't affect others, leading to more explainable, controllable, and robust AI for tasks like face manipulation, medical imaging, or better generalization. It's about learning a compressed \"code\" where each dimension (or latent variable) controls one meaningful feature, mimicking human understanding. "
    ]
  },
  {
    "term": "Disruptive Innovation",
    "definition": [
      "Disruptive Innovation in AI refers to AI-powered products or services that start simple, affordable, and accessible, often in niche markets, but then rapidly improve to challenge and displace established industry leaders by offering new value, automating complex tasks, creating new business models (like Generative AI), and transforming sectors like healthcare, finance, and manufacturing, essentially moving upmarket to dominate. "
    ]
  },
  {
    "term": "Disruptive Technology",
    "definition": [
      "Disruptive technology (DT) like Artificial Intelligence (AI) creates new markets or radically changes existing ones by offering simpler, cheaper, or more accessible solutions that displace incumbents, with AI disrupting industries by automating human-like cognitive tasks (learning, problem-solving, perception) through machine learning, deep learning, and NLP, driving massive efficiency, new services (like self-driving cars, virtual assistants, advanced diagnostics), and reshaping jobs, business models, and societal norms globally. "
    ]
  },
  {
    "term": "Domain Adaptation",
    "definition": [
      "Domain Adaptation in AI is a type of transfer learning that adapts a model trained on a rich \"source domain\" (like general internet text) to perform well on a different but related \"target domain\" (like medical reports) with limited or no labeled data, bridging the gap in data distribution to maintain accuracy without full retraining. It's crucial for scenarios where getting labeled data for new contexts (e.g., different users, evolving data) is costly or impossible, by aligning features or injecting domain-specific knowledge to create a versatile, domain-invariant model."
    ]
  },
  {
    "term": "Domain-Specific Pre-Training",
    "definition": [
      "Domain-Specific Pre-Training (DSPT) in AI adapts general language models by continuing their training on vast amounts of data from a niche field (like medicine, law, or finance) to teach them specialized vocabulary, context, and nuances, making them experts for industry-specific tasks, improving accuracy, reducing \"hallucinations,\" and creating more reliable, contextually relevant AI solutions without building from scratch. "
    ]
  },
  {
    "term": "Dynamic Argumentation Feedback",
    "definition": [
      "Dynamic Argumentation Feedback refers to the use of machine learning (ML) and natural language processing (NLP) to provide personalized, adaptive, and real-time guidance on a user's argumentation skills or processes. \r\nThis approach differs from traditional, static feedback methods by modeling the user's behavior and performance over time (dynamically) to offer tailored hints, recommendations, and corrections for specific logical errors or weaknesses in their arguments. "
    ]
  },
  {
    "term": "Dynamic Graph Learning",
    "definition": [
      "Dynamic Graph Learning (DGL) in AI models complex, evolving networks (like social media, traffic) where nodes and connections change over time, using techniques like Graph Neural Networks (GNNs) to capture these shifts for tasks like predicting trends or user behavior, unlike static graphs where structure is fixed. It combines sequential data processing with graph analysis, focusing on temporal dynamics (when interactions happen) and evolving graph structures (nodes appearing/disappearing) to understand real-world systems. "
    ]
  },
  {
    "term": "Chatgpt-3",
    "definition": [
      "GPT-3 (Generative Pre-trained Transformer 3) is an artificial intelligence language model created by the research company OpenAI. It is a deep learning system specifically designed to understand and generate human-like text, and it formed the foundation for the original ChatGPT chatbot. "
    ]
  },
  {
    "term": "Dynamic Logits Distortion",
    "definition": [
      "\"Dynamic Logits Distortion\" is a technique in artificial intelligence, primarily used in knowledge distillation and long-tailed learning, that involves dynamically adjusting the raw output scores (logits) of a neural network. "
    ]
  },
  {
    "term": "Dynamic Modality Gate",
    "definition": [
      "A Dynamic Modality Gate is a mechanism used in multimodal machine learning to adaptively control the flow and weighting of information from different data types (modalities) on a sample-by-sample basis. "
    ]
  },
  {
    "term": "Dynamic Nucleus Sampling",
    "definition": [
      "Dynamic nucleus sampling, also known as top-p sampling, is a text generation technique used in AI language models to produce diverse and contextually relevant outputs. It dynamically adjusts the number of possible next words considered at each step, balancing coherence and creativity in the generated text. "
    ]
  },
  {
    "term": "Dynamic Ontology Update",
    "definition": [
      "In artificial intelligence, dynamic ontology update is the process of automatically extending or modifying an existing ontology in real-time to reflect new information, changing contexts, or evolving domain knowledge. This allows AI systems to continuously learn, adapt, and maintain relevance in a changing world, rather than being limited to the static knowledge present at their initial training time. "
    ]
  },
  {
    "term": "Dynamic Topic Modeling",
    "definition": [
      "Dynamic Topic Modeling (DTM) is an AI technique that analyzes large text collections to discover hidden topics and, crucially, how those topics evolve over time, treating documents in time slices (like years) and modeling topic shifts from one period to the next, helping understand trends in anything from scientific literature to social media discourse. It's an extension of traditional topic models like LDA (Latent Dirichlet Allocation) that adds a temporal dimension, allowing researchers to see how language around a concept (like \"climate change\") changes, what new sub-themes emerge, and how old ones fade. "
    ]
  },
  {
    "term": "Dynamic Word Embedding",
    "definition": [
      "Dynamic Word Embedding is a set of techniques in artificial intelligence and natural language processing (NLP) that creates time-aware word representations, allowing the meaning of a word to change and evolve over time. \r\nTraditional (static) word embedding models, such as Word2Vec or GloVe, assume that a word has a fixed meaning regardless of when it is used in a text corpus. Dynamic word embedding models, in contrast, address the reality that language is constantly evolving due to cultural shifts, technological advancements, and political developments. "
    ]
  },
  {
    "term": "Economic Value Of Artificial Intelligence",
    "definition": [
      "The economic value of AI is immense, projected to add trillions to global GDP by 2030 through massive productivity gains, new service creation, and enhanced efficiency, with estimates ranging from $7 trillion (Goldman Sachs) to over $15 trillion (PwC) annually, driven by automation, data analysis, and personalized experiences, though real impact depends on widespread adoption, skilled workforce adaptation, and addressing concerns about job displacement, notes the MIT Sloan Management Review, PwC, McKinsey, and the IMF. "
    ]
  },
  {
    "term": "Editorial Artificial-Intelligence",
    "definition": [
      "Editorial Artificial Intelligence (AI) refers to using AI tools to automate, assist, and enhance tasks within the publishing and content creation lifecycle, from generating ideas and drafting text to checking grammar, identifying plagiarism, finding reviewers, formatting, and even creating summaries, fundamentally changing how content is produced, reviewed, and distributed while raising ethical questions about authorship and originality. "
    ]
  },
  {
    "term": "Edge AI",
    "definition": [
      "Edge AI brings artificial intelligence (AI) processing directly onto local devices (like phones, cameras, IoT sensors) at the \"edge\" of the network, rather than sending all data to a central cloud for analysis. This enables real-time decision-making, lower latency, reduced bandwidth, enhanced privacy (as raw data stays local), and offline functionality, making devices smarter and more efficient for tasks from voice commands to autonomous driving. "
    ]
  },
  {
    "term": "Effects Of AI On Libraries",
    "definition": [
      "AI is transforming libraries by automating routine tasks (cataloging, data entry), enhancing user experience (personalized recommendations, 24/7 chatbots), improving research (semantic search, smarter discovery), and providing data-driven insights for better collection management, while also introducing challenges like data privacy, cost, and ethical concerns about bias and potential job displacement. "
    ]
  },
  {
    "term": "Eliza",
    "definition": [
      "ELIZA was one of the first chatbots, created at MIT by Joseph Weizenbaum in the mid-1960s, designed to simulate a Rogerian psychotherapist by using simple pattern matching and substitution to rephrase user inputs as questions, giving the illusion of understanding and empathetic conversation without actual comprehension, pioneering early human-computer interaction. It's a foundational example of natural language processing (NLP) and demonstrated how easily humans could be fooled into believing a machine understood them, a phenomenon now known as the \"ELIZA effect\". "
    ]
  },
  {
    "term": "Embeddings",
    "definition": [
      "In artificial intelligence, embeddings are numerical representations (vectors or lists of numbers) of complex, real-world objects like words, images, audio, or entire documents. These vectors capture the semantic meaning and contextual relationships within the data, allowing AI and machine learning models to process and understand information more effectively than they could in its raw format. "
    ]
  },
  {
    "term": "Embodied Artificial Intelligence",
    "definition": [
      "Embodied AI integrates artificial intelligence into physical systems (robots, vehicles) to let them perceive, learn, and act in the real world, moving beyond disembodied tasks (like chatbots) to understand context through physical interaction, sensory feedback, and trial-and-error, enabling human-like dexterity and problem-solving for complex real-world challenges. It combines machine learning, vision, and robotics to create intelligent agents that can navigate, manipulate objects, and collaborate with humans, acting as a bridge toward more advanced general AI. "
    ]
  },
  {
    "term": "Embodied Human Language Model",
    "definition": [
      "Embodied Human Language Model (EHLM) is a theoretical concept in cognitive linguistics that posits human language as a cognitive tool deeply rooted in our sensory and physical interactions with the world, challenging the purely abstract, symbolic computation of traditional AI models. \r\nIn contrast to EHLM, which is a model of human cognition, \"Embodied AI\" (or \"Embodied Artificial Intelligence\") is the active field of AI research focused on integrating artificial intelligence into physical systems, such as robots, autonomous vehicles, and wearable devices. "
    ]
  },
  {
    "term": "Emotion Prompt",
    "definition": [
      "Emotion Prompting is a prompt engineering technique that enhances AI responses by adding emotional context, cues, or psychological triggers to standard text prompts, making them more nuanced, human-like, and effective, by leveraging the AI's training on emotionally rich data to improve performance on tasks like writing, summarization, and even complex problem-solving. It works by adding phrases like \"I'm counting on you\" or \"This is a life-changing moment\" to influence the AI's tone and depth, leading to better results. "
    ]
  },
  {
    "term": "Entity Linking",
    "definition": [
      "Entity Linking (EL) is a task in Artificial Intelligence (AI) and Natural Language Processing (NLP) that involves automatically identifying mentions of people, places, organizations, and concepts within unstructured text and connecting them to unique, well-defined entries in a structured knowledge base. \r\nThis process effectively bridges the gap between raw, ambiguous text data and curated, machine-readable knowledge graphs, allowing AI systems to understand the precise meaning and context of the content. "
    ]
  },
  {
    "term": "Entity Recognition Model",
    "definition": [
      "An Entity Recognition Model (NER) in AI uses Natural Language Processing (NLP) to automatically find and classify key \"entities\" (names, places, dates, organizations, etc.) in unstructured text, transforming messy data into structured information for analysis, like telling the difference between \"Apple\" the company and the fruit. These models identify subjects (who/what) and their types, enabling AI systems (chatbots, search engines) to understand context and extract important details for tasks like summarizing documents, powering chatbots, and improving search relevance. "
    ]
  },
  {
    "term": "Entity Relation Extraction",
    "definition": [
      "Entity Relation Extraction (ERE) is a core task in artificial intelligence (AI) and natural language processing (NLP) that aims to identify and categorize both \"entities\" (key information) and the semantic \"relationships\" between them within unstructured text. \r\nThe primary goal is to transform raw, human-readable text into a structured, machine-readable format, often represented as knowledge triplets (entity 1, relation, entity 2). This structured data is fundamental for building knowledge graphs, powering question-answering systems, and enabling advanced data analysis. "
    ]
  },
  {
    "term": "Epistemology And Ethics Of ML",
    "definition": [
      "Epistemology and ethics of Machine Learning (ML) are intertwined philosophical fields that examine the nature of knowledge generated by AI systems and the moral principles guiding their design and use. ",
      "Epistemology (the study of knowledge) in the context of ML focuses on how AI systems acquire, process, and validate \"knowledge,\" and the extent to which this knowledge is reliable and understandable to humans. "
    ]
  },
  {
    "term": "ERNIE",
    "definition": [
      "Ernie (or ERNIE Bot) is a multimodal AI chatbot developed by Chinese tech Baidu, rivaling models like ChatGPT, excelling in Chinese language tasks, and integrating text, image, audio, and video generation. Built on Baidu's ERNIE (Enhanced Representation through Knowledge Integration) large language models, it provides knowledge-enhanced reasoning, content creation, coding, and real-time assistance, powering features in products in China and making advanced AI accessible. "
    ]
  },
  {
    "term": "Ethical Artificial Intelligence",
    "definition": [
      "Ethical Artificial Intelligence (AI) means creating and using AI systems that align with human values like fairness, transparency, accountability, privacy, and non-discrimination, going beyond just legal requirements to prevent harm and ensure positive societal impact. It involves building ethical standards into the entire AI lifecycle—design, training, and deployment—to address risks like bias, lack of explainability, and data misuse, fostering trust and responsible innovation. "
    ]
  },
  {
    "term": "Ethical Implications Of AI",
    "definition": [
      "The ethical implications of AI involve crucial issues like bias & fairness (perpetuating discrimination), transparency & accountability (explainability of \"black box\" decisions), privacy & data security, job displacement & economic impact, and potential human safety risks (autonomous systems, misinformation). Key concerns center on ensuring AI aligns with human values, preventing harm, protecting rights, and establishing responsible governance for these powerful technologies. "
    ]
  },
  {
    "term": "Ethical Machine Learning",
    "definition": [
      "Ethical Machine Learning (ML) involves designing, building, and deploying AI systems that align with human values, ensuring fairness, transparency, accountability, privacy, and safety, to benefit society rather than control or harm it. It goes beyond legal compliance to address potential biases, prevent discrimination, protect user data, explain decisions, and ensure inclusivity, making sure AI serves humanity responsibly. "
    ]
  },
  {
    "term": "Ethics -Based AI Auditing",
    "definition": [
      "Ethics-Based AI Auditing is a structured process to systematically review AI systems, checking if their design, data, and decisions align with human values, fairness, transparency, and legal standards, going beyond basic compliance to ensure trustworthiness, build public trust, and prevent harms like bias or discrimination. Auditors use methods like scrutinizing training data, reviewing decision logic, and assessing impacts to provide actionable insights for responsible AI development and deployment, crucial in high-stakes areas like finance, healthcare, and law enforcement. "
    ]
  },
  {
    "term": "Ethics Of Algorithms",
    "definition": [
      "Ethics of Algorithms (AI) is the field of moral principles guiding the responsible creation and use of AI, focusing on fairness, accountability, transparency, privacy, and preventing harm, ensuring systems benefit humanity while mitigating risks like bias, discrimination, and societal disruption, rather than just maximizing efficiency. It involves embedding values like human rights and safety into AI lifecycles, addressing issues from biased training data to autonomous decisions. "
    ]
  },
  {
    "term": "Event Extraction",
    "definition": [
      "Event Extraction (EE) in AI is an NLP task that automatically identifies \"who, what, when, where, why, and how\" details from unstructured text, turning raw language (like news articles) into structured data about specific happenings (events), including event types, triggers (like a verb), and participants (arguments). It uses AI, often deep learning models, to understand context, classify events (e.g., \"Start-Position,\" \"Transport\"), and extract key details like people, places, and times, enabling knowledge discovery and analysis in finance, security, and public opinion monitoring. "
    ]
  },
  {
    "term": "Event Graph",
    "definition": [
      "In AI, an Event Graph (or Event Knowledge Graph) models dynamic real-world happenings, focusing on actions and changes (events) as core elements, not just static entities, to capture temporal sequences, causality, and complex relationships for tasks like prediction, reasoning, and understanding dynamic systems. Unlike traditional knowledge graphs that map static facts (e.g., \"Person A is in City B\"), event graphs map \"what happens\" (e.g., \"Person A traveled to City B on Date X\"). They serve as a powerful framework for building AI memory, enabling transparent reasoning, process mining, and intelligent automation. "
    ]
  },
  {
    "term": "Event Relation Extraction",
    "definition": [
      "Event Relation Extraction (ERE) is a crucial task in artificial intelligence (specifically, in Natural Language Processing and Information Extraction) that aims to automatically identify and classify the semantic relationships between different events mentioned within a text. The primary goal is to transform unstructured text into structured information, often used to build comprehensive event knowledge graphs or facilitate narrative understanding and prediction. "
    ]
  },
  {
    "term": "Evidential Regression",
    "definition": [
      "Evidential Regression is an artificial intelligence (AI) method that uses deep learning to predict a continuous target value while simultaneously providing a reliable measure of its associated uncertainty. This approach is particularly valuable in safety-critical applications like autonomous driving and medical diagnosis, where understanding a model's confidence is crucial for risk assessment. "
    ]
  },
  {
    "term": "Evolutionary Algorithms",
    "definition": [
      "Evolutionary Algorithms (EAs) in AI are nature-inspired optimization methods that mimic biological evolution (selection, reproduction, mutation) to find excellent solutions to complex problems, especially when traditional methods fail; they work by evolving a population of candidate solutions over generations, keeping the best ones and combining them to create even better offspring until an optimal or good-enough result is found. "
    ]
  },
  {
    "term": "Explainable Artificial Intelligence",
    "definition": [
      "Explainable Artificial Intelligence (XAI) is a field focused on making complex AI systems transparent, so humans can understand how and why they make specific decisions, moving them from \"black boxes\" to transparent \"glass boxes\". It uses methods to provide insights into AI's reasoning, which builds trust, ensures fairness, helps debug models, meets regulatory compliance (like GDPR), and allows users to challenge or contest outcomes in critical areas like healthcare, finance, and justice. "
    ]
  },
  {
    "term": "Factor Model Of LLMs Dependence",
    "definition": [
      "The term \"Factor Model of LLM Dependence\" refers to a psychological framework used to analyze and measure the various ways humans rely on, or become dependent on, Large Language Models (LLMs) like ChatGPT. \r\nResearch in this area has identified distinct factors that characterize this dependency, moving beyond simple use to explore the psychological mechanisms involved. "
    ]
  },
  {
    "term": "Factored Model",
    "definition": [
      "A factored model (or factored representation) in artificial intelligence is an approach where a complex system or problem state is broken down into simpler, interacting components or variables, known as \"factors\". Instead of treating a problem as a single, indivisible entity (an \"atomic\" representation), a factored model represents the system as a collection of features, each with a specific value or probability. \r\nThis decomposition allows AI algorithms to handle complexity more efficiently by reasoning about subsets of the system, rather than the entire monolithic structure at once. "
    ]
  },
  {
    "term": "FAIR",
    "definition": [
      "FAIR in AI refers to principles ensuring AI systems are Findable, Accessible, Interoperable, and Reusable (data/models), promoting data sharing and reproducibility; but it also commonly means Fairness in AI, focusing on unbiased, ethical, and socially responsible AI that avoids discrimination by mitigating biases from training data, ensuring equal opportunity, and protecting users. So, FAIR can mean ethical fairness (no bias) or data manageability (Findable, Accessible, Interoperable, Reusable). "
    ]
  },
  {
    "term": "Fairness Metrics",
    "definition": [
      "Fairness metrics in AI are mathematical tools and statistical measures used to detect, quantify, and mitigate bias in artificial intelligence systems, ensuring they treat different demographic groups (based on race, gender, age, etc.) equitably. They help identify if a model's decisions disproportionately harm or favor certain groups, allowing developers to balance accuracy with ethical goals, comply with regulations (like the EU AI Act), and build trustworthy, inclusive AI for critical areas like hiring, healthcare, and finance. "
    ]
  },
  {
    "term": "Fairness Techniques",
    "definition": [
      "Fairness techniques in AI are methods to ensure AI systems treat all individuals and groups equitably, preventing discrimination based on characteristics like race, gender, or age, by identifying and mitigating biases in data and algorithms throughout the AI lifecycle (collection, training, deployment) to build trustworthy, compliant, and ethical systems. These techniques involve using fairness metrics, defining fairness goals (like equal opportunity), applying mitigation strategies (like causal fairness), and continuous monitoring to avoid perpetuating societal inequalities in critical areas like hiring, lending, and healthcare. "
    ]
  },
  {
    "term": "Feature Detection",
    "definition": [
      "In AI and computer vision, Feature Detection is the process of identifying unique, meaningful points (like corners, edges, or blobs) in an image that remain consistent even with changes in lighting, rotation, or scale, serving as building blocks for tasks like object recognition, tracking, and image stitching. Algorithms like SIFT, SURF, and FAST locate these \"keypoints,\" and then create mathematical descriptors for them, allowing machines to understand and compare visual information reliably. "
    ]
  },
  {
    "term": "Feature Extraction",
    "definition": [
      "Feature extraction in AI is the process of transforming raw, complex data (like images, text, or signals) into a simpler, numerical set of key characteristics (features) that highlight important patterns, making it easier and faster for machine learning models to learn, recognize, and make accurate predictions, significantly improving efficiency and performance by reducing data dimensionality and noise. "
    ]
  },
  {
    "term": "Feature Selection",
    "definition": [
      "Feature Selection in AI is the crucial process of choosing the most relevant input features (variables) from a dataset to train a machine learning model, aiming to boost accuracy, reduce noise/overfitting, speed up training, simplify the model, and improve interpretability by discarding irrelevant or redundant data. It's a key part of data preprocessing, ensuring the model focuses on the most informative attributes, much like packing only essentials for a trip instead of everything you own. "
    ]
  },
  {
    "term": "Few-Shot Learning",
    "definition": [
      "Few-Shot Learning (FSL) is an AI technique where models learn new tasks from just a handful of labeled examples (e.g., 1-5), mimicking human ability to generalize quickly, unlike traditional AI needing thousands of data points. It's a form of meta-learning (\"learning to learn\"), relying on prior knowledge from massive pre-training to adapt fast, making it crucial for data-scarce fields like rare disease detection, personalized AI, and niche computer vision. "
    ]
  },
  {
    "term": "Fomo-AI",
    "definition": [
      "\"FOMO-AI\" primarily refers to the \"Fear Of Missing Out on Artificial Intelligence,\" a psychological phenomenon and a business buzzword describing anxiety about falling behind in the rapidly evolving AI landscape. It also can refer to a specific AI-powered marketing software company, FOMO.ai. ",
      "AI FOMO is a recognized psychological concept defined as the worry that one's AI skills, access, or ability to leverage AI benefits lag behind those of peers or competitors. "
    ]
  },
  {
    "term": "Formal Ontology",
    "definition": [
      "In AI, Formal Ontology is a structured, logical framework defining concepts, properties, and relationships within a domain (like medicine or finance) in a machine-readable way, enabling systems to understand, share, and reason about complex information beyond surface data, essentially giving AI a blueprint for knowledge to facilitate deep, context-aware understanding, as seen with systems like the Basic Formal Ontology (BFO) used for robust data integration and reasoning. "
    ]
  },
  {
    "term": "FP-Growth",
    "definition": [
      "The FP-Growth (Frequent Pattern Growth) algorithm is an efficient data mining technique used in artificial intelligence for discovering frequent patterns (itemsets) and association rules in large transactional datasets. It is a faster, more scalable alternative to traditional methods like the Apriori algorithm, primarily because it avoids the costly process of generating candidate itemsets and requires only two scans of the database. "
    ]
  },
  {
    "term": "Framing Of AI",
    "definition": [
      "Framing in AI has two main meanings: Knowledge Representation Frames, structured data templates (slots/attributes) for organizing facts (like a \"restaurant frame\") to help AI reason about stereotyped situations, as proposed by Marvin Minsky; and Public/Media Framing, how AI's impact (jobs, ethics, benefits) is presented in discourse, shaping public perception and policy. Essentially, it's about structuring information (data frames) or structuring narratives (media frames) for better AI understanding or human perception. "
    ]
  },
  {
    "term": "Frequent Itemset Mining",
    "definition": [
      "Frequent Itemset Mining (FIM) in AI is a core data mining technique that discovers items or sets of items (itemsets) that frequently appear together in large datasets, like discovering {milk, bread} are often bought with {diapers} in market basket analysis, helping businesses with cross-selling, recommendation systems, and pattern discovery in areas like bioinformatics and medical diagnosis, using algorithms like Apriori to find patterns above a minimum \"support\" threshold. "
    ]
  },
  {
    "term": "Friendly AGI",
    "definition": [
      "Friendly artificial intelligence (friendly AI or FAI) is hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests such as fostering the improvement of the human species."
    ]
  },
  {
    "term": "Fuzzy Associative Memory Neural Networks",
    "definition": [
      "A Fuzzy Associative Memory (FAM) neural network is a hybrid artificial intelligence model that combines the learning and pattern recognition capabilities of neural networks with the ability of fuzzy logic to handle uncertainty and imprecise information. \r\nFAMs are a type of associative memory (AM), which are AI systems inspired by the human brain's ability to recall information by association, even from corrupted or incomplete inputs. "
    ]
  },
  {
    "term": "GAI Contextualisation",
    "definition": [
      "GAI contextualization refers to the practice of supplying a Generative Artificial Intelligence (GAI) model with specific, relevant information (context) to produce more accurate, relevant, and useful results for a particular task or environment. "
    ]
  },
  {
    "term": "Gated Network",
    "definition": [
      "A Gated Network in artificial intelligence is a type of neural network that uses gating mechanisms to control the flow of information, allowing it to selectively remember or forget information during processing. This mechanism is particularly important for handling sequential data and addressing issues like the vanishing gradient problem in traditional recurrent neural networks (RNNs). "
    ]
  },
  {
    "term": "Gaussian Mixture Model",
    "definition": [
      "A Gaussian Mixture Model (GMM) in AI is a probabilistic model that represents complex data as a blend (mixture) of several simpler bell-shaped (Gaussian) distributions, each with its own center (mean), spread (variance), and weight. It's used in unsupervised learning for tasks like clustering, assigning data points probabilities of belonging to multiple groups (soft clustering) rather than just one. GMMs are more flexible than K-Means because they can model clusters of different shapes and sizes, handling overlapping groups effectively, making them ideal for discovering hidden subgroups (subpopulations) in data. "
    ]
  },
  {
    "term": "Gemini",
    "definition": [
      "Gemini is a family of advanced, multimodal artificial intelligence (AI) models developed by Google. Unlike traditional AI that might handle only text, Gemini is designed to understand and process various types of information, including text, images, audio, video, and computer code, all at once. "
    ]
  },
  {
    "term": "GenAI Disclosure",
    "definition": [
      "GenAI disclosure (generative artificial intelligence disclosure) refers to the practice of explicitly and transparently stating when, how, and to what extent AI tools have been used in the creation of a work, such as a manuscript, report, advertisement, or academic assignment. "
    ]
  },
  {
    "term": "GenAI Integration In Organizations",
    "definition": [
      "GenAI integration in organizations is the strategic embedding of generative artificial intelligence (GenAI) tools and models into existing business systems, data flows, and operational processes to automate tasks, enhance human creativity, and drive innovation. \r\nUnlike traditional AI, which primarily focuses on analysis and prediction, GenAI is capable of generating new and original content, such as text, images, code, and insights. The goal of integration is to move beyond isolated AI experiments and leverage GenAI to fundamentally transform operations, improve decision-making, and secure a competitive advantage. "
    ]
  },
  {
    "term": "GenAI Products",
    "definition": [
      "GenAI Products are AI tools that create new content (text, images, code, audio, video) by learning patterns from vast datasets, unlike traditional AI that analyzes data; they work via deep learning models (like GPT-4) to generate original outputs from user prompts, revolutionizing creativity and productivity but also posing risks like inaccuracies or bias. "
    ]
  },
  {
    "term": "GenAI-Chatbot",
    "definition": [
      "A GenAI-Chatbot is an advanced conversational AI that uses Generative Artificial Intelligence (GenAI) to create human-like, dynamic, and creative responses, rather than just retrieving pre-programmed answers. Trained on vast datasets, it learns patterns to generate new text, images, code, and more, allowing it to brainstorm, answer complex questions, write, and even create media, making interactions feel more natural and personalized, like with chatbots such as ChatGPT or Claude. "
    ]
  },
  {
    "term": "Gender Differences In AI Literacy",
    "definition": [
      "Gender differences in AI literacy show men often report higher perceived knowledge, confidence, and usage of AI tools, while women express more anxiety, skepticism, and concerns about ethics, stemming from historical underrepresentation in tech and biases in AI systems (like less accurate recognition of female voices/faces). However, actual abilities might be closer, with women often focusing on critical reflection and societal impacts, suggesting a need for inclusive training to bridge perceived gaps and ensure equitable AI development and access. "
    ]
  },
  {
    "term": "General Regression Neural Network",
    "definition": [
      "A General Regression Neural Network (GRNN) is a fast, four-layer AI model for non-parametric regression, approximating complex continuous functions by learning from training data without iterative training, using radial basis functions (Gaussian kernels) to map inputs to outputs, excellent for tasks needing quick, accurate predictions from smaller datasets. "
    ]
  },
  {
    "term": "Generation Prompt",
    "definition": [
      "A Generation Prompt in artificial intelligence (AI) is a natural language instruction, question, or input given to a generative AI model to guide it in creating a specific desired output. This output can be in the form of text, images, code, audio, or other digital media. "
    ]
  },
  {
    "term": "Generative Adversarial Network",
    "definition": [
      "A Generative Adversarial Network (GAN) is an AI framework using two competing neural networks—a Generator and a Discriminator—to create realistic, synthetic data (like images, music, or text) that mimics a training dataset, functioning like a \"forger\" (Generator) trying to fool an \"art critic\" (Discriminator) until the generated output is indistinguishable from the real thing. "
    ]
  },
  {
    "term": "Generative AI Abuse",
    "definition": [
      "Generative AI abuse refers to the deliberate misuse of generative artificial intelligence (AI) tools to facilitate harmful, unethical, or illegal activities. This technology, which can create highly realistic text, images, audio, and video content, amplifies existing threats by enabling malicious actors to operate at a greater speed and scale and with less technical expertise than previously required. "
    ]
  },
  {
    "term": "Generative AI In Education",
    "definition": [
      "Generative AI in Education uses AI models (like ChatGPT) to create new content (text, images, code) from prompts, transforming learning by offering personalized tutoring, generating course materials (lesson plans, quizzes), aiding research (data analysis, drafting), and sparking creativity, though it also challenges traditional assessment by enabling students to generate essays, requiring new focus on critical thinking, ethics, and human-centered learning. "
    ]
  },
  {
    "term": "Generative Artificial Intelligence",
    "definition": [
      "Generative Artificial Intelligence (GenAI) is a type of AI that creates new, original content—like text, images, music, code, or videos—by learning patterns from vast amounts of existing data, essentially mimicking human creativity to produce novel outputs in response to user prompts. Unlike traditional AI that classifies or predicts, GenAI generates content that is often indistinguishable from human-made work, powering tools like ChatGPT and DALL-E for applications from writing and design to scientific research. "
    ]
  },
  {
    "term": "Large Language Model",
    "definition": [
      "Large Language Models (LLMs) are advanced AI systems trained on massive amounts of text data to understand, generate, and process human language, using deep learning (like Transformers) to predict the next word in a sequence, enabling tasks like chatbots, summarization, translation, and code generation. They form the core of many generative AI applications, allowing for human-like text creation and complex language comprehension. "
    ]
  },
  {
    "term": "Natural Language Processing",
    "definition": []
  },
  {
    "term": "Leonardo AI",
    "definition": []
  },
  {
    "term": "Midjourney",
    "definition": []
  },
  {
    "term": "SciSpace",
    "definition": []
  },
  {
    "term": "Machine Learning",
    "definition": []
  },
  {
    "term": "Affective Event",
    "definition": []
  },
  {
    "term": "Trustworthy Ai",
    "definition": []
  },
  {
    "term": "Yolov8s",
    "definition": []
  },
  {
    "term": "Sentiment Analysis",
    "definition": []
  },
  {
    "term": "Sentiment Evolution Analysis",
    "definition": []
  },
  {
    "term": "Generative Design",
    "definition": []
  },
  {
    "term": "Generative Imagery",
    "definition": []
  },
  {
    "term": "Generative Machine Learning",
    "definition": []
  },
  {
    "term": "Generative Mechanisms",
    "definition": []
  },
  {
    "term": "Generative Poetry",
    "definition": []
  },
  {
    "term": "Generative Pre-Trained Model",
    "definition": []
  },
  {
    "term": "Generative Storytelling",
    "definition": []
  },
  {
    "term": "Generative Text",
    "definition": []
  },
  {
    "term": "Generative Transformers",
    "definition": []
  },
  {
    "term": "Genetic Algorithm",
    "definition": []
  },
  {
    "term": "Ghostbots",
    "definition": []
  },
  {
    "term": "Github Copilot",
    "definition": []
  },
  {
    "term": "Generative Neural Network",
    "definition": []
  },
  {
    "term": "Gpt",
    "definition": []
  },
  {
    "term": "Gpt Models",
    "definition": []
  },
  {
    "term": "Gpt-2",
    "definition": []
  },
  {
    "term": "Gpt-3",
    "definition": []
  },
  {
    "term": "Gpt-3. 5",
    "definition": []
  },
  {
    "term": "Gpt-3.5-Turbo",
    "definition": []
  },
  {
    "term": "Gpt-4",
    "definition": []
  },
  {
    "term": "Gpt-4o",
    "definition": []
  },
  {
    "term": "Grad-Cam",
    "definition": []
  },
  {
    "term": "Graph Artificial Intelligence",
    "definition": []
  },
  {
    "term": "Graph Attention Network",
    "definition": []
  },
  {
    "term": "Graph Contrastive Learning",
    "definition": []
  },
  {
    "term": "Graph Convolutional Network",
    "definition": []
  },
  {
    "term": "Graph Deep Learning",
    "definition": []
  },
  {
    "term": "Graph KAG",
    "definition": []
  },
  {
    "term": "Graph Neural Networks",
    "definition": []
  },
  {
    "term": "Graph Representation",
    "definition": []
  },
  {
    "term": "Graph Representation Learning",
    "definition": []
  },
  {
    "term": "Graphical Convolutional Networks",
    "definition": []
  },
  {
    "term": "Green AI",
    "definition": []
  },
  {
    "term": "Gudermannian Neural Networks",
    "definition": []
  },
  {
    "term": "Hallucination Mitigation",
    "definition": []
  },
  {
    "term": "Hallucinations",
    "definition": []
  },
  {
    "term": "Handwriting Text Recognition",
    "definition": []
  },
  {
    "term": "Heterogeneous Graph Neural Network",
    "definition": []
  },
  {
    "term": "Heterogeneous Semantic Networks",
    "definition": []
  },
  {
    "term": "High-Dimensional Learned Index",
    "definition": []
  },
  {
    "term": "Hiring Algorithms",
    "definition": []
  },
  {
    "term": "History Of AI",
    "definition": []
  },
  {
    "term": "Hix.AI",
    "definition": []
  },
  {
    "term": "Human -Centered Artificial Intelligence",
    "definition": []
  },
  {
    "term": "Human Activity Recognition",
    "definition": []
  },
  {
    "term": "Human Cyber Physical Systems",
    "definition": []
  },
  {
    "term": "Human Digital Twin",
    "definition": []
  },
  {
    "term": "Human-Ai Complementarity",
    "definition": []
  },
  {
    "term": "Human-Ai Symbiosis",
    "definition": []
  },
  {
    "term": "Human-Ai Teams",
    "definition": []
  },
  {
    "term": "Human-Algorithm Interaction",
    "definition": []
  },
  {
    "term": "Human-Centered Explainable Artificial Intelligence",
    "definition": []
  },
  {
    "term": "Human-Centered Intelligent Information",
    "definition": []
  },
  {
    "term": "Human-Chatbot Interaction",
    "definition": []
  },
  {
    "term": "Human-Gai Collaboration",
    "definition": []
  },
  {
    "term": "Human-In-The Loop AI",
    "definition": []
  },
  {
    "term": "Human-Llm Collaboration",
    "definition": []
  },
  {
    "term": "Human-Machine Hybridization",
    "definition": []
  },
  {
    "term": "Hybrid Arima",
    "definition": []
  },
  {
    "term": "Hybrid Intelligence",
    "definition": []
  },
  {
    "term": "Hybrid Reasoning",
    "definition": []
  },
  {
    "term": "Hybrid Service Agent",
    "definition": []
  },
  {
    "term": "Hypergraph Induced Graph Convolutional Network",
    "definition": []
  },
  {
    "term": "Hyperparameter Optimization",
    "definition": []
  },
  {
    "term": "Image-Text Matching",
    "definition": []
  },
  {
    "term": "Impact Of AI",
    "definition": []
  },
  {
    "term": "Inclusive AI Design",
    "definition": []
  },
  {
    "term": "IndiaAI",
    "definition": []
  },
  {
    "term": "Informed AI",
    "definition": []
  },
  {
    "term": "Inner-Shape IOU",
    "definition": []
  },
  {
    "term": "Innovation With GAI",
    "definition": []
  },
  {
    "term": "Intelligence System",
    "definition": []
  },
  {
    "term": "Intelligent Agent",
    "definition": []
  },
  {
    "term": "Intelligent Bibliometrics",
    "definition": []
  },
  {
    "term": "Intelligent Conversational Agents",
    "definition": []
  },
  {
    "term": "Intelligent Customer Service",
    "definition": []
  },
  {
    "term": "Intelligent Information Processing Of Ancient Texts",
    "definition": []
  },
  {
    "term": "Intelligent Information Services",
    "definition": []
  },
  {
    "term": "Intelligent Information Technology",
    "definition": []
  },
  {
    "term": "Intelligent Library",
    "definition": []
  },
  {
    "term": "Intelligent Machines",
    "definition": []
  },
  {
    "term": "Intelligent Optimization",
    "definition": []
  },
  {
    "term": "Intelligent Process Automation",
    "definition": []
  },
  {
    "term": "Intelligent Recommender System",
    "definition": []
  },
  {
    "term": "Intelligent System",
    "definition": []
  },
  {
    "term": "Intelligent Technologies",
    "definition": []
  },
  {
    "term": "Intelligent Information Systems",
    "definition": []
  },
  {
    "term": "Interpretable AI",
    "definition": []
  },
  {
    "term": "Interpretable Machine Learning",
    "definition": []
  },
  {
    "term": "Inverse Reinforcement Learning",
    "definition": []
  },
  {
    "term": "K-Means",
    "definition": []
  },
  {
    "term": "K-Nearest Neighbor Classifier",
    "definition": []
  },
  {
    "term": "Knowledge Augmentation",
    "definition": []
  },
  {
    "term": "Knowledge Engineering",
    "definition": []
  },
  {
    "term": "Knowledge Extraction",
    "definition": []
  },
  {
    "term": "Knowledge Graph",
    "definition": []
  },
  {
    "term": "Knowledge Graph Curation",
    "definition": []
  },
  {
    "term": "Knowledge Graph Embedding",
    "definition": []
  },
  {
    "term": "Knowledge Graph Inference Algorithm",
    "definition": []
  },
  {
    "term": "Knowledge Hypergraph",
    "definition": []
  },
  {
    "term": "Knowledge-Aware Models",
    "definition": []
  },
  {
    "term": "Knowledge-Aware Networks",
    "definition": []
  },
  {
    "term": "Knowledge-Aware Recommendation",
    "definition": []
  },
  {
    "term": "Knowledge-Enhanced Medical Large Language Model",
    "definition": []
  },
  {
    "term": "Kohonen Map",
    "definition": []
  },
  {
    "term": "L2 Regularization",
    "definition": []
  },
  {
    "term": "Label Distribution Learning",
    "definition": []
  },
  {
    "term": "Labeled Data Sets",
    "definition": []
  },
  {
    "term": "Language Processing",
    "definition": []
  },
  {
    "term": "Language Transformers",
    "definition": []
  },
  {
    "term": "LASSO",
    "definition": []
  },
  {
    "term": "Latent Dirichlet Allocation",
    "definition": []
  },
  {
    "term": "Layoutlmv3",
    "definition": []
  },
  {
    "term": "Learning Algorithms",
    "definition": []
  },
  {
    "term": "Lexicon-Based Classification",
    "definition": []
  },
  {
    "term": "Leximancer",
    "definition": []
  },
  {
    "term": "Lightgbm",
    "definition": []
  },
  {
    "term": "Linear Feature Selection",
    "definition": []
  },
  {
    "term": "Linguistic Analysis",
    "definition": []
  },
  {
    "term": "Linguistics",
    "definition": []
  },
  {
    "term": "Link Prediction",
    "definition": []
  },
  {
    "term": "Liquid Machine Learning",
    "definition": []
  },
  {
    "term": "Llama-2",
    "definition": []
  },
  {
    "term": "Llm Feedback",
    "definition": []
  },
  {
    "term": "Llms Evaluation",
    "definition": []
  },
  {
    "term": "Location Encoding",
    "definition": []
  },
  {
    "term": "Logits Distillation",
    "definition": []
  },
  {
    "term": "Machine Learning For Library Services",
    "definition": []
  },
  {
    "term": "Machine Learning Algorithms",
    "definition": []
  },
  {
    "term": "Machine Learning Application",
    "definition": []
  },
  {
    "term": "Machine Learning Classifier",
    "definition": []
  },
  {
    "term": "Machine Learning In Cultural Heritage",
    "definition": []
  },
  {
    "term": "Machine Learning Interpretability",
    "definition": []
  },
  {
    "term": "Machine Learning Model",
    "definition": []
  },
  {
    "term": "Machine Learning Patterns",
    "definition": []
  },
  {
    "term": "Machine Reading Comprehension",
    "definition": []
  },
  {
    "term": "Machine Reasoning",
    "definition": []
  },
  {
    "term": "Machine-Aided Indexing",
    "definition": []
  },
  {
    "term": "Message Clustering",
    "definition": []
  },
  {
    "term": "Meta-Learning",
    "definition": []
  },
  {
    "term": "Modal Logic For Classifiers",
    "definition": []
  },
  {
    "term": "Multi-Modal Image Clustering",
    "definition": []
  },
  {
    "term": "Multi-Modal Knowledge Graph",
    "definition": []
  },
  {
    "term": "Multi-Modal Machine Translation",
    "definition": []
  },
  {
    "term": "Multi-Relation Modeling",
    "definition": []
  },
  {
    "term": "Multi-Scale Feature Fusion",
    "definition": []
  },
  {
    "term": "Multi-Task Joint Learning",
    "definition": []
  },
  {
    "term": "Multilayer Perceptron",
    "definition": []
  },
  {
    "term": "Multimodal Data Integration",
    "definition": []
  },
  {
    "term": "Multimodal Data Retrieval",
    "definition": []
  },
  {
    "term": "Multimodal Deep Learning",
    "definition": []
  },
  {
    "term": "Multimodal Fusion Network",
    "definition": []
  },
  {
    "term": "Multimodal Generation",
    "definition": []
  },
  {
    "term": "Multimodal Large Language Model",
    "definition": []
  },
  {
    "term": "Multitask Learning",
    "definition": []
  },
  {
    "term": "Naive Bayes Classifier",
    "definition": []
  },
  {
    "term": "Narrow Artificial Intelligence",
    "definition": []
  },
  {
    "term": "Natural Language Explanations",
    "definition": []
  },
  {
    "term": "Natural Language Generation",
    "definition": []
  },
  {
    "term": "Natural Language Inference",
    "definition": []
  },
  {
    "term": "Natural Language Processing",
    "definition": []
  },
  {
    "term": "Nearest Neighbor Search",
    "definition": []
  },
  {
    "term": "Negative Sequential Pattern",
    "definition": []
  },
  {
    "term": "NERF",
    "definition": []
  },
  {
    "term": "Nested Entity",
    "definition": []
  },
  {
    "term": "Network Embedding",
    "definition": []
  },
  {
    "term": "Neural Additive Model",
    "definition": []
  },
  {
    "term": "Neural Graph Collaborative Filtering",
    "definition": []
  },
  {
    "term": "Neural Machine Learning",
    "definition": []
  },
  {
    "term": "Neural Machine Translation",
    "definition": []
  },
  {
    "term": "Neural Network Approach",
    "definition": []
  },
  {
    "term": "Neural Network Training",
    "definition": []
  },
  {
    "term": "Neural Networks",
    "definition": []
  },
  {
    "term": "Neural Ordinary Differential Equation",
    "definition": []
  },
  {
    "term": "Neural-Based Topic Model",
    "definition": []
  },
  {
    "term": "Neuro-Symbolic AI",
    "definition": []
  },
  {
    "term": "Neuro-Symbolic Intelligence",
    "definition": []
  },
  {
    "term": "Object Detection",
    "definition": []
  },
  {
    "term": "Object-Aware Generative Model",
    "definition": []
  },
  {
    "term": "Of-Interest Recommendation",
    "definition": []
  },
  {
    "term": "One-Pixel Attacks",
    "definition": []
  },
  {
    "term": "One-Shot Augmented Learning",
    "definition": []
  },
  {
    "term": "Online Review Mining",
    "definition": []
  },
  {
    "term": "Ontology",
    "definition": []
  },
  {
    "term": "Ontologies Network",
    "definition": []
  },
  {
    "term": "Ontology Classification",
    "definition": []
  },
  {
    "term": "Ontology-Based Knowledge System",
    "definition": []
  },
  {
    "term": "Ontology-Driven Metadata Models",
    "definition": []
  },
  {
    "term": "Openai",
    "definition": []
  },
  {
    "term": "Openai Operator",
    "definition": []
  },
  {
    "term": "Opinion Mining",
    "definition": []
  },
  {
    "term": "Optimization In CNN Model",
    "definition": []
  },
  {
    "term": "Owl",
    "definition": []
  },
  {
    "term": "Pairwise Training",
    "definition": []
  },
  {
    "term": "Part-Whole Relations",
    "definition": []
  },
  {
    "term": "Particle Swarm Optimisation",
    "definition": []
  },
  {
    "term": "Patent Citation Recommendation",
    "definition": []
  },
  {
    "term": "Pattern Flattening",
    "definition": []
  },
  {
    "term": "Pattern Mining",
    "definition": []
  },
  {
    "term": "Peft",
    "definition": []
  },
  {
    "term": "Perceived Algorithmic Fairness",
    "definition": []
  },
  {
    "term": "Perceptron",
    "definition": []
  },
  {
    "term": "Perplexity AI",
    "definition": []
  },
  {
    "term": "Personal Intelligent Agent",
    "definition": []
  },
  {
    "term": "Point Clouds",
    "definition": []
  },
  {
    "term": "PIAS",
    "definition": []
  },
  {
    "term": "Post-Ocr Text Correction",
    "definition": []
  },
  {
    "term": "Pre-Trained Language Model",
    "definition": []
  },
  {
    "term": "Pre-Trained Model",
    "definition": []
  },
  {
    "term": "Predictive Analytics",
    "definition": []
  },
  {
    "term": "Predictive Computation",
    "definition": []
  },
  {
    "term": "Predictive Maintenance",
    "definition": []
  },
  {
    "term": "Predictive Model",
    "definition": []
  },
  {
    "term": "Probabilistic Models",
    "definition": []
  },
  {
    "term": "Probabilistic Topic Model",
    "definition": []
  },
  {
    "term": "Probability Embedding",
    "definition": []
  },
  {
    "term": "Prohibited Artificial Intelligence Practices",
    "definition": []
  },
  {
    "term": "Prompt",
    "definition": []
  },
  {
    "term": "Prompt Engineering",
    "definition": []
  },
  {
    "term": "Prompt Tuning",
    "definition": []
  },
  {
    "term": "Pu-Learning",
    "definition": []
  },
  {
    "term": "Q-Learning",
    "definition": []
  },
  {
    "term": "Q&A Interaction",
    "definition": []
  },
  {
    "term": "Question-Answering Systems",
    "definition": []
  },
  {
    "term": "Quadratic Particle Swarm Optimisation",
    "definition": []
  },
  {
    "term": "Quantum Clustering",
    "definition": []
  },
  {
    "term": "Quantum Machine Learning",
    "definition": []
  },
  {
    "term": "Quasi-Human",
    "definition": []
  },
  {
    "term": "Query-Aware Mechanism",
    "definition": []
  },
  {
    "term": "Question Classification Model",
    "definition": []
  },
  {
    "term": "Racial Bias",
    "definition": []
  },
  {
    "term": "Random Forest",
    "definition": []
  },
  {
    "term": "Random Sample Partition",
    "definition": []
  },
  {
    "term": "Ranking Prediction Model",
    "definition": []
  },
  {
    "term": "Reading Recommendation Systems",
    "definition": []
  },
  {
    "term": "Rebuttal Success Prediction",
    "definition": []
  },
  {
    "term": "Recommendation Agents",
    "definition": []
  },
  {
    "term": "Recommendation Algorithms",
    "definition": []
  },
  {
    "term": "Recommendation System",
    "definition": []
  },
  {
    "term": "Recurrent Graph Neural Network",
    "definition": []
  },
  {
    "term": "Recurrent Neural Network",
    "definition": []
  },
  {
    "term": "Regression Analysis",
    "definition": []
  },
  {
    "term": "Reinforcement Learning",
    "definition": []
  },
  {
    "term": "Relational Graph Convolutional Networks",
    "definition": []
  },
  {
    "term": "Relative Ai Divide",
    "definition": []
  },
  {
    "term": "Representation Learning",
    "definition": []
  },
  {
    "term": "Residual Network",
    "definition": []
  },
  {
    "term": "Resnet34",
    "definition": []
  },
  {
    "term": "Resnet50",
    "definition": []
  },
  {
    "term": "Responsible Ai Governance",
    "definition": []
  },
  {
    "term": "Responsible Artificial Intelligence",
    "definition": []
  },
  {
    "term": "Reti-CVD",
    "definition": []
  },
  {
    "term": "Retrieval Augmented Generation",
    "definition": []
  },
  {
    "term": "Rising Star Prediction",
    "definition": []
  },
  {
    "term": "Rough Set",
    "definition": []
  },
  {
    "term": "SA-Net",
    "definition": []
  },
  {
    "term": "Saliency Map",
    "definition": []
  },
  {
    "term": "Scholarly Text Mining",
    "definition": []
  },
  {
    "term": "Scibert",
    "definition": []
  },
  {
    "term": "Scientific Papers Mining",
    "definition": []
  },
  {
    "term": "Scientific Publications Clustering",
    "definition": []
  },
  {
    "term": "Self-Organizing Neural Network",
    "definition": []
  },
  {
    "term": "Semantic Alignment",
    "definition": []
  },
  {
    "term": "Semantic Analysis",
    "definition": []
  },
  {
    "term": "Semantic Metadata Generation",
    "definition": []
  },
  {
    "term": "Semantic Network",
    "definition": []
  },
  {
    "term": "Semantic Processing",
    "definition": []
  },
  {
    "term": "Semantic Search",
    "definition": []
  },
  {
    "term": "Semantic Segmentation",
    "definition": []
  },
  {
    "term": "Semantic Similarity",
    "definition": []
  },
  {
    "term": "Semantic Systems",
    "definition": []
  },
  {
    "term": "Semantic Textual Similarity",
    "definition": []
  },
  {
    "term": "Semantic Topic Modeling",
    "definition": []
  },
  {
    "term": "Semantic Web",
    "definition": []
  },
  {
    "term": "Semantics",
    "definition": []
  },
  {
    "term": "Semi-Supervised Classification",
    "definition": []
  },
  {
    "term": "Semi-Supervised Feature Selection",
    "definition": []
  },
  {
    "term": "Semiautomatic Indexing",
    "definition": []
  },
  {
    "term": "Semiotics",
    "definition": []
  },
  {
    "term": "Sentient AI",
    "definition": []
  },
  {
    "term": "Sequential Neural Network",
    "definition": []
  },
  {
    "term": "Sequential Pattern Analysis",
    "definition": []
  },
  {
    "term": "Sequential Pattern Mining",
    "definition": []
  },
  {
    "term": "Sequential Recommendation",
    "definition": []
  },
  {
    "term": "Service Robots",
    "definition": []
  },
  {
    "term": "Session-Based Recommendation",
    "definition": []
  },
  {
    "term": "SHAP",
    "definition": []
  },
  {
    "term": "Shoggoth Meme",
    "definition": []
  },
  {
    "term": "Siamese Neural Network",
    "definition": []
  },
  {
    "term": "Single Neurons",
    "definition": []
  },
  {
    "term": "Smart Citation Analysis",
    "definition": []
  },
  {
    "term": "Smart Information Services",
    "definition": []
  },
  {
    "term": "Smart Learning Management System",
    "definition": []
  },
  {
    "term": "Smote",
    "definition": []
  },
  {
    "term": "Social Emotion Mining",
    "definition": []
  },
  {
    "term": "Social Spider Optimization",
    "definition": []
  },
  {
    "term": "Span-Based Model",
    "definition": []
  },
  {
    "term": "Spatial Token",
    "definition": []
  },
  {
    "term": "Spatially Explicit Machine Learning",
    "definition": []
  },
  {
    "term": "Stacked Deep Convolutional Autoencoder",
    "definition": []
  },
  {
    "term": "Stacked Transformer",
    "definition": []
  },
  {
    "term": "Star Graph Embedding",
    "definition": []
  },
  {
    "term": "Stara Awareness",
    "definition": []
  },
  {
    "term": "Structural Commonality Prompt",
    "definition": []
  },
  {
    "term": "Structured Output Capability",
    "definition": []
  },
  {
    "term": "Student Interaction With AI",
    "definition": []
  },
  {
    "term": "Style Transfer Algorithm",
    "definition": []
  },
  {
    "term": "Stylegan2",
    "definition": []
  },
  {
    "term": "Supervised Contrastive Learning",
    "definition": []
  },
  {
    "term": "Supervised Fine-Tuning",
    "definition": []
  },
  {
    "term": "Supervised Learning",
    "definition": []
  },
  {
    "term": "Supervised Machine Learning",
    "definition": []
  },
  {
    "term": "Support Vector Machine",
    "definition": []
  },
  {
    "term": "Swarm Intelligence",
    "definition": []
  },
  {
    "term": "Symbolic Ai",
    "definition": []
  },
  {
    "term": "Syntactic Complexity",
    "definition": []
  },
  {
    "term": "Syntax-Guided Transformer",
    "definition": []
  },
  {
    "term": "Synthetic Images",
    "definition": []
  },
  {
    "term": "Synthetic Data",
    "definition": []
  },
  {
    "term": "Synthetic User",
    "definition": []
  },
  {
    "term": "T-SNE",
    "definition": []
  },
  {
    "term": "Table Mining",
    "definition": []
  },
  {
    "term": "Tabular Data Generation",
    "definition": []
  },
  {
    "term": "Text Mining",
    "definition": []
  },
  {
    "term": "Text-To-Bim",
    "definition": []
  },
  {
    "term": "Text-To-Image Algorithms",
    "definition": []
  },
  {
    "term": "Text-To-Image Creator",
    "definition": []
  },
  {
    "term": "TF-IDF",
    "definition": []
  },
  {
    "term": "Theory-Driven Deep Learning",
    "definition": []
  },
  {
    "term": "Time Prediction",
    "definition": []
  },
  {
    "term": "Topic Detection",
    "definition": []
  },
  {
    "term": "Topic Modelling",
    "definition": []
  },
  {
    "term": "Trajectory Mining",
    "definition": []
  },
  {
    "term": "Transfer Learning",
    "definition": []
  },
  {
    "term": "Transformer Model",
    "definition": []
  },
  {
    "term": "Transformer Neural Networks",
    "definition": []
  },
  {
    "term": "Tree Decoder",
    "definition": []
  },
  {
    "term": "Turkish Sentiment Analysis",
    "definition": []
  },
  {
    "term": "Turn Dialogue Subsystems",
    "definition": []
  },
  {
    "term": "Two-Stage Attention Optimization",
    "definition": []
  },
  {
    "term": "Two-Tower Architecture",
    "definition": []
  },
  {
    "term": "U-Net",
    "definition": []
  },
  {
    "term": "Unbalanced Data",
    "definition": []
  },
  {
    "term": "Universal Sentence Encoder",
    "definition": []
  },
  {
    "term": "Unsupervised Clustering",
    "definition": []
  },
  {
    "term": "Unsupervised Domain Adaptation",
    "definition": []
  },
  {
    "term": "Unsupervised Learning",
    "definition": []
  },
  {
    "term": "Unsupervised Machine Learning Analysis",
    "definition": []
  },
  {
    "term": "Urdu Language Processing",
    "definition": []
  },
  {
    "term": "Use Of AI By Researchers In The Workplace",
    "definition": []
  },
  {
    "term": "Use Of AI In Foreign Language Learning And Teaching",
    "definition": []
  },
  {
    "term": "Use Of Communicative AI",
    "definition": []
  },
  {
    "term": "Using AI In Academic Assignments",
    "definition": []
  },
  {
    "term": "VAES",
    "definition": []
  },
  {
    "term": "Value Alignment In AI",
    "definition": []
  },
  {
    "term": "Virtual Agents",
    "definition": []
  },
  {
    "term": "Vision-Language Model",
    "definition": []
  },
  {
    "term": "Voice-Based Artificial Intelligence",
    "definition": []
  },
  {
    "term": "VQA",
    "definition": []
  },
  {
    "term": "Wasserstein Gan",
    "definition": []
  },
  {
    "term": "Weak And Strong Artificial Intelligence",
    "definition": []
  },
  {
    "term": "Word Embedding",
    "definition": []
  },
  {
    "term": "Word Segmentation",
    "definition": []
  },
  {
    "term": "Word2vec",
    "definition": []
  },
  {
    "term": "Xgboost",
    "definition": []
  },
  {
    "term": "Yolo",
    "definition": []
  },
  {
    "term": "Yolov5",
    "definition": []
  },
  {
    "term": "TensorFlow",
    "definition": []
  },
  {
    "term": "LangChain",
    "definition": []
  },
  {
    "term": "PyTorch",
    "definition": []
  },
  {
    "term": "Knowledge Representation Frames",
    "definition": []
  },
  {
    "term": "Public Framing",
    "definition": []
  },
  {
    "term": "Media Framing",
    "definition": []
  }
]